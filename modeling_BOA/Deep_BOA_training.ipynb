{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 14:26:54.778537: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-08 14:26:55.030002: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-08 14:26:55.223015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749385615.437327    6825 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749385615.513993    6825 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749385615.873963    6825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749385615.873997    6825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749385615.874001    6825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749385615.874005    6825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-08 14:26:55.922435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler , OneHotEncoder\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>g1</th>\n",
       "      <th>Cos(SZA)</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984234</td>\n",
       "      <td>0.967931</td>\n",
       "      <td>0.849827</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.402452</td>\n",
       "      <td>0.239730</td>\n",
       "      <td>81.357114</td>\n",
       "      <td>1137.064781</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>0.946187</td>\n",
       "      <td>0.150275</td>\n",
       "      <td>708.015229</td>\n",
       "      <td>3186.068532</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.151289</td>\n",
       "      <td>5.695816</td>\n",
       "      <td>3.743539</td>\n",
       "      <td>429.332315</td>\n",
       "      <td>0.133511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.808703</td>\n",
       "      <td>0.502863</td>\n",
       "      <td>0.971936</td>\n",
       "      <td>0.754977</td>\n",
       "      <td>0.212324</td>\n",
       "      <td>0.116173</td>\n",
       "      <td>0.087708</td>\n",
       "      <td>69.669734</td>\n",
       "      <td>2695.217813</td>\n",
       "      <td>0.364243</td>\n",
       "      <td>0.910936</td>\n",
       "      <td>0.347431</td>\n",
       "      <td>708.188358</td>\n",
       "      <td>3186.847609</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.347828</td>\n",
       "      <td>2.122562</td>\n",
       "      <td>0.747097</td>\n",
       "      <td>194.210750</td>\n",
       "      <td>0.363279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266592</td>\n",
       "      <td>0.973465</td>\n",
       "      <td>0.845395</td>\n",
       "      <td>0.580608</td>\n",
       "      <td>1.322036</td>\n",
       "      <td>0.400463</td>\n",
       "      <td>0.232512</td>\n",
       "      <td>17.550390</td>\n",
       "      <td>1081.897319</td>\n",
       "      <td>0.672173</td>\n",
       "      <td>0.896924</td>\n",
       "      <td>0.953452</td>\n",
       "      <td>708.009100</td>\n",
       "      <td>3186.040949</td>\n",
       "      <td>0.953519</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>0.929959</td>\n",
       "      <td>0.610609</td>\n",
       "      <td>648.404343</td>\n",
       "      <td>0.280041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.830782</td>\n",
       "      <td>0.963485</td>\n",
       "      <td>0.971736</td>\n",
       "      <td>0.270474</td>\n",
       "      <td>0.185388</td>\n",
       "      <td>0.039302</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>44.256621</td>\n",
       "      <td>81.149965</td>\n",
       "      <td>0.775990</td>\n",
       "      <td>0.869940</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>707.897906</td>\n",
       "      <td>3185.540575</td>\n",
       "      <td>0.716701</td>\n",
       "      <td>0.716328</td>\n",
       "      <td>1.382759</td>\n",
       "      <td>1.340499</td>\n",
       "      <td>885.138727</td>\n",
       "      <td>0.125099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600291</td>\n",
       "      <td>0.839312</td>\n",
       "      <td>0.797427</td>\n",
       "      <td>0.652968</td>\n",
       "      <td>0.510340</td>\n",
       "      <td>0.652287</td>\n",
       "      <td>0.425923</td>\n",
       "      <td>24.480144</td>\n",
       "      <td>932.892398</td>\n",
       "      <td>0.777209</td>\n",
       "      <td>0.878946</td>\n",
       "      <td>0.910105</td>\n",
       "      <td>707.992544</td>\n",
       "      <td>3185.966446</td>\n",
       "      <td>0.910238</td>\n",
       "      <td>0.910135</td>\n",
       "      <td>0.990440</td>\n",
       "      <td>0.689159</td>\n",
       "      <td>602.697524</td>\n",
       "      <td>0.320636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362498</th>\n",
       "      <td>0.765135</td>\n",
       "      <td>0.641704</td>\n",
       "      <td>0.963197</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.267702</td>\n",
       "      <td>0.214810</td>\n",
       "      <td>0.177313</td>\n",
       "      <td>22.962506</td>\n",
       "      <td>183.428535</td>\n",
       "      <td>0.740243</td>\n",
       "      <td>0.668902</td>\n",
       "      <td>0.920760</td>\n",
       "      <td>707.909270</td>\n",
       "      <td>3185.591714</td>\n",
       "      <td>0.920877</td>\n",
       "      <td>0.920786</td>\n",
       "      <td>1.064013</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>534.140819</td>\n",
       "      <td>0.364438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362499</th>\n",
       "      <td>0.749872</td>\n",
       "      <td>0.800834</td>\n",
       "      <td>0.842750</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.287853</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.437803</td>\n",
       "      <td>55.826486</td>\n",
       "      <td>452.874619</td>\n",
       "      <td>0.016449</td>\n",
       "      <td>0.818049</td>\n",
       "      <td>0.561701</td>\n",
       "      <td>707.939208</td>\n",
       "      <td>3185.726437</td>\n",
       "      <td>0.562559</td>\n",
       "      <td>0.561892</td>\n",
       "      <td>1.690357</td>\n",
       "      <td>1.419079</td>\n",
       "      <td>387.264753</td>\n",
       "      <td>0.564494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362500</th>\n",
       "      <td>0.538644</td>\n",
       "      <td>0.788402</td>\n",
       "      <td>0.827843</td>\n",
       "      <td>0.727779</td>\n",
       "      <td>0.618701</td>\n",
       "      <td>0.694040</td>\n",
       "      <td>0.505108</td>\n",
       "      <td>6.365350</td>\n",
       "      <td>423.130196</td>\n",
       "      <td>0.050525</td>\n",
       "      <td>0.896188</td>\n",
       "      <td>0.993835</td>\n",
       "      <td>707.935903</td>\n",
       "      <td>3185.711565</td>\n",
       "      <td>0.993844</td>\n",
       "      <td>0.993837</td>\n",
       "      <td>0.959983</td>\n",
       "      <td>0.814335</td>\n",
       "      <td>476.450471</td>\n",
       "      <td>0.591383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362501</th>\n",
       "      <td>0.634018</td>\n",
       "      <td>0.991370</td>\n",
       "      <td>0.965507</td>\n",
       "      <td>0.661306</td>\n",
       "      <td>0.455678</td>\n",
       "      <td>0.103640</td>\n",
       "      <td>0.068538</td>\n",
       "      <td>7.738936</td>\n",
       "      <td>2006.981168</td>\n",
       "      <td>0.832208</td>\n",
       "      <td>0.860886</td>\n",
       "      <td>0.990892</td>\n",
       "      <td>708.111887</td>\n",
       "      <td>3186.503491</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.990895</td>\n",
       "      <td>0.807461</td>\n",
       "      <td>0.369966</td>\n",
       "      <td>1015.811664</td>\n",
       "      <td>-0.093105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362502</th>\n",
       "      <td>0.323002</td>\n",
       "      <td>0.940875</td>\n",
       "      <td>0.665318</td>\n",
       "      <td>0.306548</td>\n",
       "      <td>1.130098</td>\n",
       "      <td>0.587625</td>\n",
       "      <td>0.180135</td>\n",
       "      <td>21.099784</td>\n",
       "      <td>354.152329</td>\n",
       "      <td>0.982156</td>\n",
       "      <td>0.832794</td>\n",
       "      <td>0.932955</td>\n",
       "      <td>707.928239</td>\n",
       "      <td>3185.677076</td>\n",
       "      <td>0.933053</td>\n",
       "      <td>0.932977</td>\n",
       "      <td>1.030396</td>\n",
       "      <td>0.897896</td>\n",
       "      <td>416.207341</td>\n",
       "      <td>0.463743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362503 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0       0.984234  0.967931  0.849827  0.595673  0.015892  0.402452  0.239730   \n",
       "1       0.808703  0.502863  0.971936  0.754977  0.212324  0.116173  0.087708   \n",
       "2       0.266592  0.973465  0.845395  0.580608  1.322036  0.400463  0.232512   \n",
       "3       0.830782  0.963485  0.971736  0.270474  0.185388  0.039302  0.010630   \n",
       "4       0.600291  0.839312  0.797427  0.652968  0.510340  0.652287  0.425923   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "362498  0.765135  0.641704  0.963197  0.825442  0.267702  0.214810  0.177313   \n",
       "362499  0.749872  0.800834  0.842750  0.719020  0.287853  0.608888  0.437803   \n",
       "362500  0.538644  0.788402  0.827843  0.727779  0.618701  0.694040  0.505108   \n",
       "362501  0.634018  0.991370  0.965507  0.661306  0.455678  0.103640  0.068538   \n",
       "362502  0.323002  0.940875  0.665318  0.306548  1.130098  0.587625  0.180135   \n",
       "\n",
       "              SZA            Z  R_scence        g1  Cos(SZA)        mu_g  \\\n",
       "0       81.357114  1137.064781  0.370253  0.946187  0.150275  708.015229   \n",
       "1       69.669734  2695.217813  0.364243  0.910936  0.347431  708.188358   \n",
       "2       17.550390  1081.897319  0.672173  0.896924  0.953452  708.009100   \n",
       "3       44.256621    81.149965  0.775990  0.869940  0.716221  707.897906   \n",
       "4       24.480144   932.892398  0.777209  0.878946  0.910105  707.992544   \n",
       "...           ...          ...       ...       ...       ...         ...   \n",
       "362498  22.962506   183.428535  0.740243  0.668902  0.920760  707.909270   \n",
       "362499  55.826486   452.874619  0.016449  0.818049  0.561701  707.939208   \n",
       "362500   6.365350   423.130196  0.050525  0.896188  0.993835  707.935903   \n",
       "362501   7.738936  2006.981168  0.832208  0.860886  0.990892  708.111887   \n",
       "362502  21.099784   354.152329  0.982156  0.832794  0.932955  707.928239   \n",
       "\n",
       "               mu_a  muprime_g  muprime_a  mprime_g  mprime_a       BOA_RT  \\\n",
       "0       3186.068532   0.154730   0.151289  5.695816  3.743539   429.332315   \n",
       "1       3186.847609   0.349206   0.347828  2.122562  0.747097   194.210750   \n",
       "2       3186.040949   0.953519   0.953467  0.929959  0.610609   648.404343   \n",
       "3       3185.540575   0.716701   0.716328  1.382759  1.340499   885.138727   \n",
       "4       3185.966446   0.910238   0.910135  0.990440  0.689159   602.697524   \n",
       "...             ...        ...        ...       ...       ...          ...   \n",
       "362498  3185.591714   0.920877   0.920786  1.064013  0.990855   534.140819   \n",
       "362499  3185.726437   0.562559   0.561892  1.690357  1.419079   387.264753   \n",
       "362500  3185.711565   0.993844   0.993837  0.959983  0.814335   476.450471   \n",
       "362501  3186.503491   0.990905   0.990895  0.807461  0.369966  1015.811664   \n",
       "362502  3185.677076   0.933053   0.932977  1.030396  0.897896   416.207341   \n",
       "\n",
       "           alpha  \n",
       "0       0.133511  \n",
       "1       0.363279  \n",
       "2       0.280041  \n",
       "3       0.125099  \n",
       "4       0.320636  \n",
       "...          ...  \n",
       "362498  0.364438  \n",
       "362499  0.564494  \n",
       "362500  0.591383  \n",
       "362501 -0.093105  \n",
       "362502  0.463743  \n",
       "\n",
       "[362503 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('no_outliers.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>Cos(SZA)</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984234</td>\n",
       "      <td>0.967931</td>\n",
       "      <td>0.849827</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.402452</td>\n",
       "      <td>0.239730</td>\n",
       "      <td>81.357114</td>\n",
       "      <td>1137.064781</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150275</td>\n",
       "      <td>708.015229</td>\n",
       "      <td>3186.068532</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.151289</td>\n",
       "      <td>5.695816</td>\n",
       "      <td>3.743539</td>\n",
       "      <td>429.332315</td>\n",
       "      <td>0.133511</td>\n",
       "      <td>0.429332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.808703</td>\n",
       "      <td>0.502863</td>\n",
       "      <td>0.971936</td>\n",
       "      <td>0.754977</td>\n",
       "      <td>0.212324</td>\n",
       "      <td>0.116173</td>\n",
       "      <td>0.087708</td>\n",
       "      <td>69.669734</td>\n",
       "      <td>2695.217813</td>\n",
       "      <td>0.364243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347431</td>\n",
       "      <td>708.188358</td>\n",
       "      <td>3186.847609</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.347828</td>\n",
       "      <td>2.122562</td>\n",
       "      <td>0.747097</td>\n",
       "      <td>194.210750</td>\n",
       "      <td>0.363279</td>\n",
       "      <td>0.194211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266592</td>\n",
       "      <td>0.973465</td>\n",
       "      <td>0.845395</td>\n",
       "      <td>0.580608</td>\n",
       "      <td>1.322036</td>\n",
       "      <td>0.400463</td>\n",
       "      <td>0.232512</td>\n",
       "      <td>17.550390</td>\n",
       "      <td>1081.897319</td>\n",
       "      <td>0.672173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953452</td>\n",
       "      <td>708.009100</td>\n",
       "      <td>3186.040949</td>\n",
       "      <td>0.953519</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>0.929959</td>\n",
       "      <td>0.610609</td>\n",
       "      <td>648.404343</td>\n",
       "      <td>0.280041</td>\n",
       "      <td>0.648404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.830782</td>\n",
       "      <td>0.963485</td>\n",
       "      <td>0.971736</td>\n",
       "      <td>0.270474</td>\n",
       "      <td>0.185388</td>\n",
       "      <td>0.039302</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>44.256621</td>\n",
       "      <td>81.149965</td>\n",
       "      <td>0.775990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>707.897906</td>\n",
       "      <td>3185.540575</td>\n",
       "      <td>0.716701</td>\n",
       "      <td>0.716328</td>\n",
       "      <td>1.382759</td>\n",
       "      <td>1.340499</td>\n",
       "      <td>885.138727</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>0.885139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600291</td>\n",
       "      <td>0.839312</td>\n",
       "      <td>0.797427</td>\n",
       "      <td>0.652968</td>\n",
       "      <td>0.510340</td>\n",
       "      <td>0.652287</td>\n",
       "      <td>0.425923</td>\n",
       "      <td>24.480144</td>\n",
       "      <td>932.892398</td>\n",
       "      <td>0.777209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910105</td>\n",
       "      <td>707.992544</td>\n",
       "      <td>3185.966446</td>\n",
       "      <td>0.910238</td>\n",
       "      <td>0.910135</td>\n",
       "      <td>0.990440</td>\n",
       "      <td>0.689159</td>\n",
       "      <td>602.697524</td>\n",
       "      <td>0.320636</td>\n",
       "      <td>0.602698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362498</th>\n",
       "      <td>0.765135</td>\n",
       "      <td>0.641704</td>\n",
       "      <td>0.963197</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.267702</td>\n",
       "      <td>0.214810</td>\n",
       "      <td>0.177313</td>\n",
       "      <td>22.962506</td>\n",
       "      <td>183.428535</td>\n",
       "      <td>0.740243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920760</td>\n",
       "      <td>707.909270</td>\n",
       "      <td>3185.591714</td>\n",
       "      <td>0.920877</td>\n",
       "      <td>0.920786</td>\n",
       "      <td>1.064013</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>534.140819</td>\n",
       "      <td>0.364438</td>\n",
       "      <td>0.534141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362499</th>\n",
       "      <td>0.749872</td>\n",
       "      <td>0.800834</td>\n",
       "      <td>0.842750</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.287853</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.437803</td>\n",
       "      <td>55.826486</td>\n",
       "      <td>452.874619</td>\n",
       "      <td>0.016449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561701</td>\n",
       "      <td>707.939208</td>\n",
       "      <td>3185.726437</td>\n",
       "      <td>0.562559</td>\n",
       "      <td>0.561892</td>\n",
       "      <td>1.690357</td>\n",
       "      <td>1.419079</td>\n",
       "      <td>387.264753</td>\n",
       "      <td>0.564494</td>\n",
       "      <td>0.387265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362500</th>\n",
       "      <td>0.538644</td>\n",
       "      <td>0.788402</td>\n",
       "      <td>0.827843</td>\n",
       "      <td>0.727779</td>\n",
       "      <td>0.618701</td>\n",
       "      <td>0.694040</td>\n",
       "      <td>0.505108</td>\n",
       "      <td>6.365350</td>\n",
       "      <td>423.130196</td>\n",
       "      <td>0.050525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993835</td>\n",
       "      <td>707.935903</td>\n",
       "      <td>3185.711565</td>\n",
       "      <td>0.993844</td>\n",
       "      <td>0.993837</td>\n",
       "      <td>0.959983</td>\n",
       "      <td>0.814335</td>\n",
       "      <td>476.450471</td>\n",
       "      <td>0.591383</td>\n",
       "      <td>0.476450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362501</th>\n",
       "      <td>0.634018</td>\n",
       "      <td>0.991370</td>\n",
       "      <td>0.965507</td>\n",
       "      <td>0.661306</td>\n",
       "      <td>0.455678</td>\n",
       "      <td>0.103640</td>\n",
       "      <td>0.068538</td>\n",
       "      <td>7.738936</td>\n",
       "      <td>2006.981168</td>\n",
       "      <td>0.832208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990892</td>\n",
       "      <td>708.111887</td>\n",
       "      <td>3186.503491</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.990895</td>\n",
       "      <td>0.807461</td>\n",
       "      <td>0.369966</td>\n",
       "      <td>1015.811664</td>\n",
       "      <td>-0.093105</td>\n",
       "      <td>1.015812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362502</th>\n",
       "      <td>0.323002</td>\n",
       "      <td>0.940875</td>\n",
       "      <td>0.665318</td>\n",
       "      <td>0.306548</td>\n",
       "      <td>1.130098</td>\n",
       "      <td>0.587625</td>\n",
       "      <td>0.180135</td>\n",
       "      <td>21.099784</td>\n",
       "      <td>354.152329</td>\n",
       "      <td>0.982156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932955</td>\n",
       "      <td>707.928239</td>\n",
       "      <td>3185.677076</td>\n",
       "      <td>0.933053</td>\n",
       "      <td>0.932977</td>\n",
       "      <td>1.030396</td>\n",
       "      <td>0.897896</td>\n",
       "      <td>416.207341</td>\n",
       "      <td>0.463743</td>\n",
       "      <td>0.416207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362503 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0       0.984234  0.967931  0.849827  0.595673  0.015892  0.402452  0.239730   \n",
       "1       0.808703  0.502863  0.971936  0.754977  0.212324  0.116173  0.087708   \n",
       "2       0.266592  0.973465  0.845395  0.580608  1.322036  0.400463  0.232512   \n",
       "3       0.830782  0.963485  0.971736  0.270474  0.185388  0.039302  0.010630   \n",
       "4       0.600291  0.839312  0.797427  0.652968  0.510340  0.652287  0.425923   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "362498  0.765135  0.641704  0.963197  0.825442  0.267702  0.214810  0.177313   \n",
       "362499  0.749872  0.800834  0.842750  0.719020  0.287853  0.608888  0.437803   \n",
       "362500  0.538644  0.788402  0.827843  0.727779  0.618701  0.694040  0.505108   \n",
       "362501  0.634018  0.991370  0.965507  0.661306  0.455678  0.103640  0.068538   \n",
       "362502  0.323002  0.940875  0.665318  0.306548  1.130098  0.587625  0.180135   \n",
       "\n",
       "              SZA            Z  R_scence  ...  Cos(SZA)        mu_g  \\\n",
       "0       81.357114  1137.064781  0.370253  ...  0.150275  708.015229   \n",
       "1       69.669734  2695.217813  0.364243  ...  0.347431  708.188358   \n",
       "2       17.550390  1081.897319  0.672173  ...  0.953452  708.009100   \n",
       "3       44.256621    81.149965  0.775990  ...  0.716221  707.897906   \n",
       "4       24.480144   932.892398  0.777209  ...  0.910105  707.992544   \n",
       "...           ...          ...       ...  ...       ...         ...   \n",
       "362498  22.962506   183.428535  0.740243  ...  0.920760  707.909270   \n",
       "362499  55.826486   452.874619  0.016449  ...  0.561701  707.939208   \n",
       "362500   6.365350   423.130196  0.050525  ...  0.993835  707.935903   \n",
       "362501   7.738936  2006.981168  0.832208  ...  0.990892  708.111887   \n",
       "362502  21.099784   354.152329  0.982156  ...  0.932955  707.928239   \n",
       "\n",
       "               mu_a  muprime_g  muprime_a  mprime_g  mprime_a       BOA_RT  \\\n",
       "0       3186.068532   0.154730   0.151289  5.695816  3.743539   429.332315   \n",
       "1       3186.847609   0.349206   0.347828  2.122562  0.747097   194.210750   \n",
       "2       3186.040949   0.953519   0.953467  0.929959  0.610609   648.404343   \n",
       "3       3185.540575   0.716701   0.716328  1.382759  1.340499   885.138727   \n",
       "4       3185.966446   0.910238   0.910135  0.990440  0.689159   602.697524   \n",
       "...             ...        ...        ...       ...       ...          ...   \n",
       "362498  3185.591714   0.920877   0.920786  1.064013  0.990855   534.140819   \n",
       "362499  3185.726437   0.562559   0.561892  1.690357  1.419079   387.264753   \n",
       "362500  3185.711565   0.993844   0.993837  0.959983  0.814335   476.450471   \n",
       "362501  3186.503491   0.990905   0.990895  0.807461  0.369966  1015.811664   \n",
       "362502  3185.677076   0.933053   0.932977  1.030396  0.897896   416.207341   \n",
       "\n",
       "           alpha  BOA_fraction  \n",
       "0       0.133511      0.429332  \n",
       "1       0.363279      0.194211  \n",
       "2       0.280041      0.648404  \n",
       "3       0.125099      0.885139  \n",
       "4       0.320636      0.602698  \n",
       "...          ...           ...  \n",
       "362498  0.364438      0.534141  \n",
       "362499  0.564494      0.387265  \n",
       "362500  0.591383      0.476450  \n",
       "362501 -0.093105      1.015812  \n",
       "362502  0.463743      0.416207  \n",
       "\n",
       "[362503 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['BOA_fraction']=data['BOA_RT']/1000\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOA_RT=data['BOA_RT']\n",
    "data.drop(columns=['BOA_RT'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>g1</th>\n",
       "      <th>Cos(SZA)</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984234</td>\n",
       "      <td>0.967931</td>\n",
       "      <td>0.849827</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.402452</td>\n",
       "      <td>0.239730</td>\n",
       "      <td>81.357114</td>\n",
       "      <td>1137.064781</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>0.946187</td>\n",
       "      <td>0.150275</td>\n",
       "      <td>708.015229</td>\n",
       "      <td>3186.068532</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.151289</td>\n",
       "      <td>5.695816</td>\n",
       "      <td>3.743539</td>\n",
       "      <td>0.429332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.808703</td>\n",
       "      <td>0.502863</td>\n",
       "      <td>0.971936</td>\n",
       "      <td>0.754977</td>\n",
       "      <td>0.212324</td>\n",
       "      <td>0.116173</td>\n",
       "      <td>0.087708</td>\n",
       "      <td>69.669734</td>\n",
       "      <td>2695.217813</td>\n",
       "      <td>0.364243</td>\n",
       "      <td>0.910936</td>\n",
       "      <td>0.347431</td>\n",
       "      <td>708.188358</td>\n",
       "      <td>3186.847609</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.347828</td>\n",
       "      <td>2.122562</td>\n",
       "      <td>0.747097</td>\n",
       "      <td>0.194211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266592</td>\n",
       "      <td>0.973465</td>\n",
       "      <td>0.845395</td>\n",
       "      <td>0.580608</td>\n",
       "      <td>1.322036</td>\n",
       "      <td>0.400463</td>\n",
       "      <td>0.232512</td>\n",
       "      <td>17.550390</td>\n",
       "      <td>1081.897319</td>\n",
       "      <td>0.672173</td>\n",
       "      <td>0.896924</td>\n",
       "      <td>0.953452</td>\n",
       "      <td>708.009100</td>\n",
       "      <td>3186.040949</td>\n",
       "      <td>0.953519</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>0.929959</td>\n",
       "      <td>0.610609</td>\n",
       "      <td>0.648404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.830782</td>\n",
       "      <td>0.963485</td>\n",
       "      <td>0.971736</td>\n",
       "      <td>0.270474</td>\n",
       "      <td>0.185388</td>\n",
       "      <td>0.039302</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>44.256621</td>\n",
       "      <td>81.149965</td>\n",
       "      <td>0.775990</td>\n",
       "      <td>0.869940</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>707.897906</td>\n",
       "      <td>3185.540575</td>\n",
       "      <td>0.716701</td>\n",
       "      <td>0.716328</td>\n",
       "      <td>1.382759</td>\n",
       "      <td>1.340499</td>\n",
       "      <td>0.885139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600291</td>\n",
       "      <td>0.839312</td>\n",
       "      <td>0.797427</td>\n",
       "      <td>0.652968</td>\n",
       "      <td>0.510340</td>\n",
       "      <td>0.652287</td>\n",
       "      <td>0.425923</td>\n",
       "      <td>24.480144</td>\n",
       "      <td>932.892398</td>\n",
       "      <td>0.777209</td>\n",
       "      <td>0.878946</td>\n",
       "      <td>0.910105</td>\n",
       "      <td>707.992544</td>\n",
       "      <td>3185.966446</td>\n",
       "      <td>0.910238</td>\n",
       "      <td>0.910135</td>\n",
       "      <td>0.990440</td>\n",
       "      <td>0.689159</td>\n",
       "      <td>0.602698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362498</th>\n",
       "      <td>0.765135</td>\n",
       "      <td>0.641704</td>\n",
       "      <td>0.963197</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.267702</td>\n",
       "      <td>0.214810</td>\n",
       "      <td>0.177313</td>\n",
       "      <td>22.962506</td>\n",
       "      <td>183.428535</td>\n",
       "      <td>0.740243</td>\n",
       "      <td>0.668902</td>\n",
       "      <td>0.920760</td>\n",
       "      <td>707.909270</td>\n",
       "      <td>3185.591714</td>\n",
       "      <td>0.920877</td>\n",
       "      <td>0.920786</td>\n",
       "      <td>1.064013</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>0.534141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362499</th>\n",
       "      <td>0.749872</td>\n",
       "      <td>0.800834</td>\n",
       "      <td>0.842750</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.287853</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.437803</td>\n",
       "      <td>55.826486</td>\n",
       "      <td>452.874619</td>\n",
       "      <td>0.016449</td>\n",
       "      <td>0.818049</td>\n",
       "      <td>0.561701</td>\n",
       "      <td>707.939208</td>\n",
       "      <td>3185.726437</td>\n",
       "      <td>0.562559</td>\n",
       "      <td>0.561892</td>\n",
       "      <td>1.690357</td>\n",
       "      <td>1.419079</td>\n",
       "      <td>0.387265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362500</th>\n",
       "      <td>0.538644</td>\n",
       "      <td>0.788402</td>\n",
       "      <td>0.827843</td>\n",
       "      <td>0.727779</td>\n",
       "      <td>0.618701</td>\n",
       "      <td>0.694040</td>\n",
       "      <td>0.505108</td>\n",
       "      <td>6.365350</td>\n",
       "      <td>423.130196</td>\n",
       "      <td>0.050525</td>\n",
       "      <td>0.896188</td>\n",
       "      <td>0.993835</td>\n",
       "      <td>707.935903</td>\n",
       "      <td>3185.711565</td>\n",
       "      <td>0.993844</td>\n",
       "      <td>0.993837</td>\n",
       "      <td>0.959983</td>\n",
       "      <td>0.814335</td>\n",
       "      <td>0.476450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362501</th>\n",
       "      <td>0.634018</td>\n",
       "      <td>0.991370</td>\n",
       "      <td>0.965507</td>\n",
       "      <td>0.661306</td>\n",
       "      <td>0.455678</td>\n",
       "      <td>0.103640</td>\n",
       "      <td>0.068538</td>\n",
       "      <td>7.738936</td>\n",
       "      <td>2006.981168</td>\n",
       "      <td>0.832208</td>\n",
       "      <td>0.860886</td>\n",
       "      <td>0.990892</td>\n",
       "      <td>708.111887</td>\n",
       "      <td>3186.503491</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.990895</td>\n",
       "      <td>0.807461</td>\n",
       "      <td>0.369966</td>\n",
       "      <td>1.015812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362502</th>\n",
       "      <td>0.323002</td>\n",
       "      <td>0.940875</td>\n",
       "      <td>0.665318</td>\n",
       "      <td>0.306548</td>\n",
       "      <td>1.130098</td>\n",
       "      <td>0.587625</td>\n",
       "      <td>0.180135</td>\n",
       "      <td>21.099784</td>\n",
       "      <td>354.152329</td>\n",
       "      <td>0.982156</td>\n",
       "      <td>0.832794</td>\n",
       "      <td>0.932955</td>\n",
       "      <td>707.928239</td>\n",
       "      <td>3185.677076</td>\n",
       "      <td>0.933053</td>\n",
       "      <td>0.932977</td>\n",
       "      <td>1.030396</td>\n",
       "      <td>0.897896</td>\n",
       "      <td>0.416207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362503 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0       0.984234  0.967931  0.849827  0.595673  0.015892  0.402452  0.239730   \n",
       "1       0.808703  0.502863  0.971936  0.754977  0.212324  0.116173  0.087708   \n",
       "2       0.266592  0.973465  0.845395  0.580608  1.322036  0.400463  0.232512   \n",
       "3       0.830782  0.963485  0.971736  0.270474  0.185388  0.039302  0.010630   \n",
       "4       0.600291  0.839312  0.797427  0.652968  0.510340  0.652287  0.425923   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "362498  0.765135  0.641704  0.963197  0.825442  0.267702  0.214810  0.177313   \n",
       "362499  0.749872  0.800834  0.842750  0.719020  0.287853  0.608888  0.437803   \n",
       "362500  0.538644  0.788402  0.827843  0.727779  0.618701  0.694040  0.505108   \n",
       "362501  0.634018  0.991370  0.965507  0.661306  0.455678  0.103640  0.068538   \n",
       "362502  0.323002  0.940875  0.665318  0.306548  1.130098  0.587625  0.180135   \n",
       "\n",
       "              SZA            Z  R_scence        g1  Cos(SZA)        mu_g  \\\n",
       "0       81.357114  1137.064781  0.370253  0.946187  0.150275  708.015229   \n",
       "1       69.669734  2695.217813  0.364243  0.910936  0.347431  708.188358   \n",
       "2       17.550390  1081.897319  0.672173  0.896924  0.953452  708.009100   \n",
       "3       44.256621    81.149965  0.775990  0.869940  0.716221  707.897906   \n",
       "4       24.480144   932.892398  0.777209  0.878946  0.910105  707.992544   \n",
       "...           ...          ...       ...       ...       ...         ...   \n",
       "362498  22.962506   183.428535  0.740243  0.668902  0.920760  707.909270   \n",
       "362499  55.826486   452.874619  0.016449  0.818049  0.561701  707.939208   \n",
       "362500   6.365350   423.130196  0.050525  0.896188  0.993835  707.935903   \n",
       "362501   7.738936  2006.981168  0.832208  0.860886  0.990892  708.111887   \n",
       "362502  21.099784   354.152329  0.982156  0.832794  0.932955  707.928239   \n",
       "\n",
       "               mu_a  muprime_g  muprime_a  mprime_g  mprime_a  BOA_fraction  \n",
       "0       3186.068532   0.154730   0.151289  5.695816  3.743539      0.429332  \n",
       "1       3186.847609   0.349206   0.347828  2.122562  0.747097      0.194211  \n",
       "2       3186.040949   0.953519   0.953467  0.929959  0.610609      0.648404  \n",
       "3       3185.540575   0.716701   0.716328  1.382759  1.340499      0.885139  \n",
       "4       3185.966446   0.910238   0.910135  0.990440  0.689159      0.602698  \n",
       "...             ...        ...        ...       ...       ...           ...  \n",
       "362498  3185.591714   0.920877   0.920786  1.064013  0.990855      0.534141  \n",
       "362499  3185.726437   0.562559   0.561892  1.690357  1.419079      0.387265  \n",
       "362500  3185.711565   0.993844   0.993837  0.959983  0.814335      0.476450  \n",
       "362501  3186.503491   0.990905   0.990895  0.807461  0.369966      1.015812  \n",
       "362502  3185.677076   0.933053   0.932977  1.030396  0.897896      0.416207  \n",
       "\n",
       "[362503 rows x 19 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = data['alpha']\n",
    "data.drop(columns=['alpha'],inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "cols_to_scale = data.drop(columns=['BOA_fraction']).columns\n",
    "X_scaled = scaler.fit_transform(data[cols_to_scale])\n",
    "y_scaled=scaler_y.fit_transform(data['BOA_fraction'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled,y_scaled,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_with_negative_penalty(y_true, y_pred):\n",
    "    # MAE = moyenne des erreurs absolues\n",
    "    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "    \n",
    "    # Pénalité pour les valeurs négatives\n",
    "    penalty = tf.reduce_mean(tf.maximum(-y_pred, 0.0)) * 5  # ajuste le facteur selon tes besoins\n",
    "\n",
    "    return mae + penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,017</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">43,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">175</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)            │         \u001b[38;5;34m4,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)            │        \u001b[38;5;34m58,017\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)            │        \u001b[38;5;34m43,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m175\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,100</span> (414.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,100\u001b[0m (414.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,100</span> (414.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,100\u001b[0m (414.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"best_params.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Dense(params[\"units1\"], activation=params[\"activation\"], input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(params[\"dropout1\"]))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Dense(params[\"units2\"], activation=params[\"activation\"]))\n",
    "model.add(Dropout(params[\"dropout2\"]))\n",
    "\n",
    "# Layer 3 (if applicable)\n",
    "if params[\"n_layers\"] >= 3:\n",
    "    model.add(Dense(params[\"units3\"], activation=params[\"activation\"]))\n",
    "    model.add(Dropout(params[\"dropout3\"]))\n",
    "\n",
    "# Layer 4 (if applicable)\n",
    "if params[\"n_layers\"] == 4:\n",
    "    model.add(Dense(params[\"units4\"], activation=params[\"activation\"]))\n",
    "    model.add(Dropout(params[\"dropout4\"]))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "model.compile(optimizer=optimizer, loss=mae_with_negative_penalty, metrics=['mae'])\n",
    "\n",
    "# Affichage du résumé du modèle\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1187 - mae: 0.1045\n",
      "Epoch 1: val_mae improved from inf to 0.02471, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 0.1187 - mae: 0.1044 - val_loss: 0.0247 - val_mae: 0.0247 - learning_rate: 3.4066e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0331 - mae: 0.0324\n",
      "Epoch 2: val_mae improved from 0.02471 to 0.01450, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0331 - mae: 0.0324 - val_loss: 0.0146 - val_mae: 0.0145 - learning_rate: 3.4066e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0222 - mae: 0.0220\n",
      "Epoch 3: val_mae improved from 0.01450 to 0.01069, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0222 - mae: 0.0220 - val_loss: 0.0107 - val_mae: 0.0107 - learning_rate: 3.4066e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m3390/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0176 - mae: 0.0174\n",
      "Epoch 4: val_mae improved from 0.01069 to 0.00829, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0176 - mae: 0.0174 - val_loss: 0.0083 - val_mae: 0.0083 - learning_rate: 3.4066e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0159 - mae: 0.0157\n",
      "Epoch 5: val_mae did not improve from 0.00829\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0159 - mae: 0.0157 - val_loss: 0.0100 - val_mae: 0.0099 - learning_rate: 3.4066e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0150 - mae: 0.0148\n",
      "Epoch 6: val_mae improved from 0.00829 to 0.00676, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0150 - mae: 0.0148 - val_loss: 0.0068 - val_mae: 0.0068 - learning_rate: 3.4066e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0143 - mae: 0.0142\n",
      "Epoch 7: val_mae improved from 0.00676 to 0.00600, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0143 - mae: 0.0142 - val_loss: 0.0060 - val_mae: 0.0060 - learning_rate: 3.4066e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0141 - mae: 0.0140\n",
      "Epoch 8: val_mae did not improve from 0.00600\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0141 - mae: 0.0140 - val_loss: 0.0067 - val_mae: 0.0067 - learning_rate: 3.4066e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0138 - mae: 0.0137\n",
      "Epoch 9: val_mae did not improve from 0.00600\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0138 - mae: 0.0137 - val_loss: 0.0085 - val_mae: 0.0085 - learning_rate: 3.4066e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0136 - mae: 0.0135\n",
      "Epoch 10: val_mae did not improve from 0.00600\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0136 - mae: 0.0135 - val_loss: 0.0061 - val_mae: 0.0061 - learning_rate: 3.4066e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0133 - mae: 0.0132\n",
      "Epoch 11: val_mae did not improve from 0.00600\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0133 - mae: 0.0132 - val_loss: 0.0131 - val_mae: 0.0130 - learning_rate: 3.4066e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0132 - mae: 0.0131\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001703323214314878.\n",
      "\n",
      "Epoch 12: val_mae did not improve from 0.00600\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0132 - mae: 0.0131 - val_loss: 0.0077 - val_mae: 0.0077 - learning_rate: 3.4066e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0120 - mae: 0.0119\n",
      "Epoch 13: val_mae did not improve from 0.00600\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0120 - mae: 0.0119 - val_loss: 0.0092 - val_mae: 0.0091 - learning_rate: 1.7033e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0119 - mae: 0.0118\n",
      "Epoch 14: val_mae did not improve from 0.00600\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0119 - mae: 0.0118 - val_loss: 0.0078 - val_mae: 0.0078 - learning_rate: 1.7033e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0118 - mae: 0.0117\n",
      "Epoch 15: val_mae did not improve from 0.00600\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0118 - mae: 0.0117 - val_loss: 0.0064 - val_mae: 0.0064 - learning_rate: 1.7033e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0117 - mae: 0.0116\n",
      "Epoch 16: val_mae improved from 0.00600 to 0.00459, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0117 - mae: 0.0116 - val_loss: 0.0046 - val_mae: 0.0046 - learning_rate: 1.7033e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0117 - mae: 0.0116\n",
      "Epoch 17: val_mae did not improve from 0.00459\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0117 - mae: 0.0116 - val_loss: 0.0075 - val_mae: 0.0075 - learning_rate: 1.7033e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0117 - mae: 0.0116\n",
      "Epoch 18: val_mae did not improve from 0.00459\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0117 - mae: 0.0116 - val_loss: 0.0080 - val_mae: 0.0080 - learning_rate: 1.7033e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0116 - mae: 0.0115\n",
      "Epoch 19: val_mae did not improve from 0.00459\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0116 - mae: 0.0115 - val_loss: 0.0047 - val_mae: 0.0047 - learning_rate: 1.7033e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0115 - mae: 0.0114\n",
      "Epoch 20: val_mae improved from 0.00459 to 0.00392, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0115 - mae: 0.0114 - val_loss: 0.0039 - val_mae: 0.0039 - learning_rate: 1.7033e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0114 - mae: 0.0113\n",
      "Epoch 21: val_mae did not improve from 0.00392\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0114 - mae: 0.0113 - val_loss: 0.0064 - val_mae: 0.0064 - learning_rate: 1.7033e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0115 - mae: 0.0114\n",
      "Epoch 22: val_mae did not improve from 0.00392\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0115 - mae: 0.0114 - val_loss: 0.0047 - val_mae: 0.0047 - learning_rate: 1.7033e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0113 - mae: 0.0112\n",
      "Epoch 23: val_mae did not improve from 0.00392\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0113 - mae: 0.0112 - val_loss: 0.0045 - val_mae: 0.0045 - learning_rate: 1.7033e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0114 - mae: 0.0113\n",
      "Epoch 24: val_mae did not improve from 0.00392\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0114 - mae: 0.0113 - val_loss: 0.0049 - val_mae: 0.0049 - learning_rate: 1.7033e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0114 - mae: 0.0113\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.51661607157439e-05.\n",
      "\n",
      "Epoch 25: val_mae did not improve from 0.00392\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0114 - mae: 0.0113 - val_loss: 0.0070 - val_mae: 0.0070 - learning_rate: 1.7033e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0107 - mae: 0.0107\n",
      "Epoch 26: val_mae did not improve from 0.00392\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0040 - val_mae: 0.0040 - learning_rate: 8.5166e-05\n",
      "Epoch 27/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0107 - mae: 0.0107\n",
      "Epoch 27: val_mae did not improve from 0.00392\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0107 - mae: 0.0107 - val_loss: 0.0052 - val_mae: 0.0052 - learning_rate: 8.5166e-05\n",
      "Epoch 28/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105\n",
      "Epoch 28: val_mae improved from 0.00392 to 0.00356, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105 - val_loss: 0.0036 - val_mae: 0.0036 - learning_rate: 8.5166e-05\n",
      "Epoch 29/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0107 - mae: 0.0106\n",
      "Epoch 29: val_mae improved from 0.00356 to 0.00342, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0107 - mae: 0.0106 - val_loss: 0.0034 - val_mae: 0.0034 - learning_rate: 8.5166e-05\n",
      "Epoch 30/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105\n",
      "Epoch 30: val_mae did not improve from 0.00342\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105 - val_loss: 0.0047 - val_mae: 0.0047 - learning_rate: 8.5166e-05\n",
      "Epoch 31/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105\n",
      "Epoch 31: val_mae did not improve from 0.00342\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105 - val_loss: 0.0054 - val_mae: 0.0054 - learning_rate: 8.5166e-05\n",
      "Epoch 32/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105\n",
      "Epoch 32: val_mae did not improve from 0.00342\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105 - val_loss: 0.0042 - val_mae: 0.0042 - learning_rate: 8.5166e-05\n",
      "Epoch 33/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0105 - mae: 0.0104\n",
      "Epoch 33: val_mae did not improve from 0.00342\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0105 - mae: 0.0104 - val_loss: 0.0038 - val_mae: 0.0038 - learning_rate: 8.5166e-05\n",
      "Epoch 34/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 4.258308035787195e-05.\n",
      "\n",
      "Epoch 34: val_mae did not improve from 0.00342\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0105 - val_loss: 0.0039 - val_mae: 0.0039 - learning_rate: 8.5166e-05\n",
      "Epoch 35/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0102 - mae: 0.0102\n",
      "Epoch 35: val_mae did not improve from 0.00342\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0102 - mae: 0.0102 - val_loss: 0.0044 - val_mae: 0.0044 - learning_rate: 4.2583e-05\n",
      "Epoch 36/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0102 - mae: 0.0101\n",
      "Epoch 36: val_mae improved from 0.00342 to 0.00341, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0102 - mae: 0.0101 - val_loss: 0.0034 - val_mae: 0.0034 - learning_rate: 4.2583e-05\n",
      "Epoch 37/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100\n",
      "Epoch 37: val_mae improved from 0.00341 to 0.00328, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100 - val_loss: 0.0033 - val_mae: 0.0033 - learning_rate: 4.2583e-05\n",
      "Epoch 38/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0102 - mae: 0.0101\n",
      "Epoch 38: val_mae did not improve from 0.00328\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0102 - mae: 0.0101 - val_loss: 0.0033 - val_mae: 0.0033 - learning_rate: 4.2583e-05\n",
      "Epoch 39/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0101\n",
      "Epoch 39: val_mae did not improve from 0.00328\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0046 - val_mae: 0.0046 - learning_rate: 4.2583e-05\n",
      "Epoch 40/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0101\n",
      "Epoch 40: val_mae did not improve from 0.00328\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0041 - val_mae: 0.0041 - learning_rate: 4.2583e-05\n",
      "Epoch 41/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100\n",
      "Epoch 41: val_mae did not improve from 0.00328\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100 - val_loss: 0.0039 - val_mae: 0.0039 - learning_rate: 4.2583e-05\n",
      "Epoch 42/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0101\n",
      "Epoch 42: val_mae improved from 0.00328 to 0.00300, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 0.0101 - mae: 0.0101 - val_loss: 0.0030 - val_mae: 0.0030 - learning_rate: 4.2583e-05\n",
      "Epoch 43/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100\n",
      "Epoch 43: val_mae did not improve from 0.00300\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100 - val_loss: 0.0035 - val_mae: 0.0035 - learning_rate: 4.2583e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100\n",
      "Epoch 44: val_mae did not improve from 0.00300\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100 - val_loss: 0.0043 - val_mae: 0.0043 - learning_rate: 4.2583e-05\n",
      "Epoch 45/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100\n",
      "Epoch 45: val_mae did not improve from 0.00300\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100 - val_loss: 0.0039 - val_mae: 0.0039 - learning_rate: 4.2583e-05\n",
      "Epoch 46/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100\n",
      "Epoch 46: val_mae did not improve from 0.00300\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0100 - val_loss: 0.0040 - val_mae: 0.0040 - learning_rate: 4.2583e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0100 - mae: 0.0099\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 2.1291540178935975e-05.\n",
      "\n",
      "Epoch 47: val_mae did not improve from 0.00300\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0100 - mae: 0.0099 - val_loss: 0.0032 - val_mae: 0.0032 - learning_rate: 4.2583e-05\n",
      "Epoch 48/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0098 - mae: 0.0098\n",
      "Epoch 48: val_mae improved from 0.00300 to 0.00273, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0098 - mae: 0.0098 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 2.1292e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0099 - mae: 0.0098\n",
      "Epoch 49: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0099 - mae: 0.0098 - val_loss: 0.0029 - val_mae: 0.0029 - learning_rate: 2.1292e-05\n",
      "Epoch 50/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0099 - mae: 0.0098\n",
      "Epoch 50: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0099 - mae: 0.0098 - val_loss: 0.0035 - val_mae: 0.0035 - learning_rate: 2.1292e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0099 - mae: 0.0098\n",
      "Epoch 51: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0099 - mae: 0.0098 - val_loss: 0.0036 - val_mae: 0.0036 - learning_rate: 2.1292e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0099 - mae: 0.0098\n",
      "Epoch 52: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0099 - mae: 0.0098 - val_loss: 0.0036 - val_mae: 0.0036 - learning_rate: 2.1292e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0098 - mae: 0.0097\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 1.0645770089467987e-05.\n",
      "\n",
      "Epoch 53: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0098 - mae: 0.0097 - val_loss: 0.0029 - val_mae: 0.0029 - learning_rate: 2.1292e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0098 - mae: 0.0097\n",
      "Epoch 54: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0098 - mae: 0.0097 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 1.0646e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0097\n",
      "Epoch 55: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0030 - val_mae: 0.0030 - learning_rate: 1.0646e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0098 - mae: 0.0097\n",
      "Epoch 56: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0098 - mae: 0.0097 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 1.0646e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096\n",
      "Epoch 57: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096 - val_loss: 0.0029 - val_mae: 0.0029 - learning_rate: 1.0646e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0097\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 5.322885044733994e-06.\n",
      "\n",
      "Epoch 58: val_mae did not improve from 0.00273\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 1.0646e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096\n",
      "Epoch 59: val_mae improved from 0.00273 to 0.00272, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 5.3229e-06\n",
      "Epoch 60/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0097 - mae: 0.0096\n",
      "Epoch 60: val_mae did not improve from 0.00272\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 0.0097 - mae: 0.0096 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 5.3229e-06\n",
      "Epoch 61/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0097 - mae: 0.0097\n",
      "Epoch 61: val_mae did not improve from 0.00272\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0031 - val_mae: 0.0031 - learning_rate: 5.3229e-06\n",
      "Epoch 62/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096\n",
      "Epoch 62: val_mae did not improve from 0.00272\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 5.3229e-06\n",
      "Epoch 63/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 2.661442522366997e-06.\n",
      "\n",
      "Epoch 63: val_mae did not improve from 0.00272\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 5.3229e-06\n",
      "Epoch 64/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096\n",
      "Epoch 64: val_mae improved from 0.00272 to 0.00267, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 2.6614e-06\n",
      "Epoch 65/200\n",
      "\u001b[1m3390/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 65: val_mae did not improve from 0.00267\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 2.6614e-06\n",
      "Epoch 66/200\n",
      "\u001b[1m3390/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 66: val_mae did not improve from 0.00267\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0030 - val_mae: 0.0030 - learning_rate: 2.6614e-06\n",
      "Epoch 67/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 67: val_mae improved from 0.00267 to 0.00263, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0026 - val_mae: 0.0026 - learning_rate: 2.6614e-06\n",
      "Epoch 68/200\n",
      "\u001b[1m3387/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096\n",
      "Epoch 68: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0096 - val_loss: 0.0029 - val_mae: 0.0029 - learning_rate: 2.6614e-06\n",
      "Epoch 69/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 69: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 2.6614e-06\n",
      "Epoch 70/200\n",
      "\u001b[1m3384/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 70: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0030 - val_mae: 0.0030 - learning_rate: 2.6614e-06\n",
      "Epoch 71/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 71: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 2.6614e-06\n",
      "Epoch 72/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.3307212611834984e-06.\n",
      "\n",
      "Epoch 72: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 2.6614e-06\n",
      "Epoch 73/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0097 - mae: 0.0096\n",
      "Epoch 73: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0097 - mae: 0.0096 - val_loss: 0.0028 - val_mae: 0.0028 - learning_rate: 1.3307e-06\n",
      "Epoch 74/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 74: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 1.3307e-06\n",
      "Epoch 75/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0095\n",
      "Epoch 75: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0095 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 1.3307e-06\n",
      "Epoch 76/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 76: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 1.3307e-06\n",
      "Epoch 77/200\n",
      "\u001b[1m3384/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0095\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\n",
      "Epoch 77: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0095 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 1.3307e-06\n",
      "Epoch 78/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0096 - mae: 0.0095\n",
      "Epoch 78: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0095 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 1.0000e-06\n",
      "Epoch 79/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 79: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 1.0000e-06\n",
      "Epoch 80/200\n",
      "\u001b[1m3385/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0095\n",
      "Epoch 80: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0095 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 1.0000e-06\n",
      "Epoch 81/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096\n",
      "Epoch 81: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 1.0000e-06\n",
      "Epoch 82/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0095\n",
      "Epoch 82: val_mae did not improve from 0.00263\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - mae: 0.0095 - val_loss: 0.0027 - val_mae: 0.0027 - learning_rate: 1.0000e-06\n",
      "Epoch 82: early stopping\n",
      "Restoring model weights from the end of the best epoch: 67.\n"
     ]
    }
   ],
   "source": [
    "# 1. Early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_mae',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Model checkpoint (save best model)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='deep_model.keras',\n",
    "    monitor='val_mae',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. CSV logger (log training history)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_logger = CSVLogger(f'training_log_{timestamp}.csv')\n",
    "\n",
    "# Train phase :\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_cb, csv_logger],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2833/2833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scaling(X,scaler,y,scaler_y):\n",
    "    '''cols_to_scale_inv = X.drop(columns=['mprime_a_bin_0', 'mprime_a_bin_1',\n",
    "       'mprime_a_bin_2', 'mprime_a_bin_3', 'mprime_a_bin_4', 'mprime_g_bin_0',\n",
    "       'mprime_g_bin_1', 'mprime_g_bin_2', 'mprime_g_bin_3', 'mprime_g_bin_4'\n",
    "       ]).columns\n",
    "    cols_to_encode_inv = ['mprime_a_bin_0', 'mprime_a_bin_1',\n",
    "       'mprime_a_bin_2', 'mprime_a_bin_3', 'mprime_a_bin_4', 'mprime_g_bin_0',\n",
    "       'mprime_g_bin_1', 'mprime_g_bin_2', 'mprime_g_bin_3', 'mprime_g_bin_4',\n",
    "       ]'''\n",
    "    #X_scaled_part = X_test[cols_to_scale_inv].values\n",
    "    #X_encoded_part = X_test[cols_to_encode_inv].values\n",
    "    # 2. Inverser la standardisation\n",
    "    X_original_scaled = scaler.inverse_transform(X)\n",
    "\n",
    "    # 3. Inverser l'encodage\n",
    "    #X_original_categoricals = encoder.inverse_transform(X_encoded_part)\n",
    "    X_real= pd.DataFrame(\n",
    "        X_original_scaled,\n",
    "        columns=['Tg_scat', 'Tg_abs', 'Ta_abs', 'SSA', 'GOD', 'AOD', 'AODS', 'SZA', 'Z',\n",
    "        'R_scence', 'g1', 'Cos(SZA)', 'mu_g', 'mu_a', 'muprime_g', 'muprime_a',\n",
    "        'mprime_g', 'mprime_a'],\n",
    "        index=X_scaled.index if isinstance(X_scaled, pd.DataFrame) else None\n",
    "    )\n",
    "    '''df_categoricals = pd.DataFrame(\n",
    "        X_original_categoricals,\n",
    "        columns=['mprime_a_bin', 'mprime_g_bin'],\n",
    "        index=X_test.index\n",
    "    )'''\n",
    "    y_real=scaler_y.inverse_transform(y)\n",
    "    return X_real , y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real,y_test_real=inverse_scaling(X_test,scaler,y_test,scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gaz_TROPICAL():\n",
    "    x = np.linspace(0, 2.0, 1000)  \n",
    "\n",
    "    weights = np.piecewise(\n",
    "        x,\n",
    "        [x < 1.37, \n",
    "         (x >= 1.38) & (x < 1.5), \n",
    "         (x >= 1.5) & (x <= 1.75),\n",
    "         x > 1.75],\n",
    "        [0,  # Avant 1.38, poids nul\n",
    "         1,  # Plateau constant\n",
    "         lambda x: (1.75 - x) / (1.75 - 1.5),  # Descente linéaire\n",
    "         0]  # Après 1.75, poids nul\n",
    "    )\n",
    "\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    return np.random.choice(x, size=1, p=weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_aerosol():\n",
    "    x = np.linspace(0, 7, 1000)\n",
    "    \n",
    "    # Accentuer la montée vers 1 et la descente après 1\n",
    "    weights = np.piecewise(\n",
    "        x,\n",
    "        [x <= 1, x > 1],\n",
    "        [\n",
    "            lambda x: (x / 1) ** 2,        # Monte plus vite vers 1\n",
    "            lambda x: ((7 - x) / (7 - 1)) ** 2  # Descend plus vite après 1\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    weights = np.maximum(weights, 0)  # sécurité\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    return np.random.choice(x, size=1, p=weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.107107107107107\n",
      "2.5855855855855854\n",
      "1.8078078078078077\n",
      "0.8758758758758759\n",
      "0.924924924924925\n",
      "2.6486486486486487\n",
      "2.5435435435435436\n",
      "1.8638638638638638\n",
      "1.4854854854854855\n",
      "1.6956956956956957\n",
      "4.414414414414415\n",
      "1.2822822822822824\n",
      "1.4504504504504505\n",
      "1.3943943943943944\n",
      "2.3473473473473474\n",
      "0.8758758758758759\n",
      "3.965965965965966\n",
      "1.7797797797797799\n",
      "1.4504504504504505\n",
      "2.774774774774775\n",
      "4.7857857857857855\n",
      "3.4754754754754753\n",
      "0.5045045045045045\n",
      "3.2862862862862863\n",
      "2.4804804804804803\n",
      "2.011011011011011\n",
      "0.5885885885885885\n",
      "2.067067067067067\n",
      "1.5905905905905906\n",
      "1.5135135135135136\n",
      "0.924924924924925\n",
      "4.4774774774774775\n",
      "1.1561561561561562\n",
      "2.2982982982982985\n",
      "0.4904904904904905\n",
      "2.991991991991992\n",
      "1.212212212212212\n",
      "5.675675675675675\n",
      "4.344344344344345\n",
      "1.093093093093093\n",
      "4.078078078078078\n",
      "2.2072072072072073\n",
      "2.3753753753753752\n",
      "3.5105105105105103\n",
      "1.2472472472472473\n",
      "4.155155155155155\n",
      "5.017017017017017\n",
      "0.8338338338338338\n",
      "0.6446446446446447\n",
      "0.5325325325325325\n",
      "2.158158158158158\n",
      "1.5345345345345345\n",
      "4.183183183183183\n",
      "1.212212212212212\n",
      "2.823823823823824\n",
      "1.5625625625625625\n",
      "2.109109109109109\n",
      "1.2052052052052051\n",
      "3.818818818818819\n",
      "0.8408408408408409\n",
      "1.982982982982983\n",
      "1.8148148148148149\n",
      "0.938938938938939\n",
      "0.973973973973974\n",
      "4.281281281281281\n",
      "1.3733733733733733\n",
      "5.101101101101101\n",
      "1.2542542542542543\n",
      "1.212212212212212\n",
      "1.5485485485485486\n",
      "2.3683683683683685\n",
      "4.86986986986987\n",
      "0.7497497497497497\n",
      "2.977977977977978\n",
      "2.011011011011011\n",
      "2.046046046046046\n",
      "1.0860860860860861\n",
      "2.109109109109109\n",
      "1.9409409409409408\n",
      "0.7707707707707707\n",
      "1.3663663663663663\n",
      "0.9459459459459459\n",
      "1.5065065065065064\n",
      "0.9179179179179179\n",
      "4.86986986986987\n",
      "4.106106106106106\n",
      "3.146146146146146\n",
      "1.044044044044044\n",
      "3.055055055055055\n",
      "2.6346346346346348\n",
      "3.279279279279279\n",
      "3.041041041041041\n",
      "0.9459459459459459\n",
      "4.848848848848848\n",
      "1.5205205205205206\n",
      "5.647647647647648\n",
      "1.9409409409409408\n",
      "3.1671671671671673\n",
      "1.8708708708708708\n",
      "2.4944944944944947\n",
      "5.843843843843843\n",
      "3.041041041041041\n",
      "3.9029029029029028\n",
      "1.044044044044044\n",
      "2.5855855855855854\n",
      "2.4314314314314314\n",
      "1.5485485485485486\n",
      "2.7187187187187187\n",
      "0.48348348348348347\n",
      "1.1141141141141142\n",
      "2.6906906906906904\n",
      "1.7237237237237237\n",
      "4.456456456456457\n",
      "6.2642642642642645\n",
      "0.973973973973974\n",
      "3.6996996996997\n",
      "2.760760760760761\n",
      "2.9219219219219217\n",
      "3.069069069069069\n",
      "1.4784784784784786\n",
      "3.4684684684684686\n",
      "1.3523523523523524\n",
      "1.7587587587587588\n",
      "1.3383383383383383\n",
      "2.5645645645645647\n",
      "1.7517517517517518\n",
      "1.2962962962962963\n",
      "3.4754754754754753\n",
      "5.01001001001001\n",
      "0.6586586586586587\n",
      "0.994994994994995\n",
      "1.5905905905905906\n",
      "2.893893893893894\n",
      "2.025025025025025\n",
      "2.7187187187187187\n",
      "3.174174174174174\n",
      "1.4154154154154155\n",
      "0.7637637637637638\n",
      "2.6276276276276276\n",
      "1.1001001001001\n",
      "0.8898898898898899\n",
      "3.055055055055055\n",
      "1.7307307307307307\n",
      "1.03003003003003\n",
      "2.6276276276276276\n",
      "1.4924924924924925\n",
      "1.947947947947948\n",
      "1.3593593593593594\n",
      "2.865865865865866\n",
      "2.8728728728728727\n",
      "1.037037037037037\n",
      "1.044044044044044\n",
      "1.7797797797797799\n",
      "0.9669669669669669\n",
      "3.6156156156156154\n",
      "3.181181181181181\n",
      "1.135135135135135\n",
      "0.0980980980980981\n",
      "1.03003003003003\n",
      "2.5015015015015014\n",
      "1.6396396396396395\n",
      "2.088088088088088\n",
      "2.5925925925925926\n",
      "1.5905905905905906\n",
      "3.5175175175175175\n",
      "3.23023023023023\n",
      "2.4874874874874875\n",
      "2.9219219219219217\n",
      "3.0970970970970972\n",
      "1.7797797797797799\n",
      "3.419419419419419\n",
      "6.46046046046046\n",
      "2.3963963963963963\n",
      "0.8408408408408409\n",
      "0.95995995995996\n",
      "2.8028028028028027\n",
      "0.1961961961961962\n",
      "5.024024024024024\n",
      "0.5325325325325325\n",
      "2.109109109109109\n",
      "1.3173173173173174\n",
      "2.1371371371371373\n",
      "1.0510510510510511\n",
      "2.27027027027027\n",
      "0.8338338338338338\n",
      "1.9339339339339339\n",
      "3.888888888888889\n",
      "1.5205205205205206\n",
      "1.1911911911911912\n",
      "2.3123123123123124\n",
      "1.7027027027027026\n",
      "1.4154154154154155\n",
      "4.652652652652653\n",
      "0.6936936936936937\n",
      "2.4174174174174174\n",
      "5.535535535535535\n",
      "0.6516516516516516\n",
      "5.08008008008008\n",
      "2.5645645645645647\n",
      "2.4524524524524525\n",
      "3.09009009009009\n",
      "2.970970970970971\n",
      "3.027027027027027\n",
      "5.052052052052052\n",
      "2.053053053053053\n",
      "0.938938938938939\n",
      "5.08008008008008\n",
      "3.335335335335335\n",
      "2.4034034034034035\n",
      "2.4804804804804803\n",
      "1.5555555555555556\n",
      "1.8988988988988988\n",
      "1.7937937937937938\n",
      "0.9109109109109109\n",
      "2.053053053053053\n",
      "2.1021021021021022\n",
      "3.034034034034034\n",
      "3.825825825825826\n",
      "1.5205205205205206\n",
      "1.6606606606606606\n",
      "1.5485485485485486\n",
      "4.736736736736737\n",
      "1.03003003003003\n",
      "2.025025025025025\n",
      "1.009009009009009\n",
      "0.6376376376376376\n",
      "0.9529529529529529\n",
      "1.135135135135135\n",
      "4.61061061061061\n",
      "3.860860860860861\n",
      "3.6506506506506504\n",
      "0.5255255255255256\n",
      "1.3103103103103102\n",
      "1.3943943943943944\n",
      "0.938938938938939\n",
      "1.5065065065065064\n",
      "4.141141141141141\n",
      "0.6866866866866866\n",
      "0.9669669669669669\n",
      "2.893893893893894\n",
      "1.1001001001001\n",
      "1.4854854854854855\n",
      "1.3033033033033032\n",
      "3.930930930930931\n",
      "2.9079079079079078\n",
      "1.5345345345345345\n",
      "2.5925925925925926\n",
      "2.7887887887887888\n",
      "1.2542542542542543\n",
      "0.8408408408408409\n",
      "0.9319319319319319\n",
      "1.1211211211211212\n",
      "5.045045045045045\n",
      "4.113113113113113\n",
      "0.7357357357357357\n",
      "1.2262262262262262\n",
      "4.883883883883884\n",
      "2.5925925925925926\n",
      "1.6396396396396395\n",
      "2.025025025025025\n",
      "0.6796796796796797\n",
      "1.5625625625625625\n",
      "0.6796796796796797\n",
      "4.652652652652653\n",
      "0.5255255255255256\n",
      "5.864864864864865\n",
      "1.079079079079079\n",
      "1.3873873873873874\n",
      "1.5485485485485486\n",
      "4.54054054054054\n",
      "1.5695695695695695\n",
      "0.9319319319319319\n",
      "4.001001001001001\n",
      "0.7567567567567568\n",
      "1.212212212212212\n",
      "1.5625625625625625\n",
      "1.982982982982983\n",
      "1.2192192192192193\n",
      "3.4614614614614614\n",
      "3.7277277277277276\n",
      "4.72972972972973\n",
      "2.3123123123123124\n",
      "2.8028028028028027\n",
      "2.06006006006006\n",
      "1.4714714714714714\n",
      "3.7977977977977977\n",
      "2.116116116116116\n",
      "3.5385385385385386\n",
      "1.1001001001001\n",
      "1.5975975975975976\n",
      "1.4714714714714714\n",
      "1.4784784784784786\n",
      "2.9219219219219217\n",
      "0.5115115115115115\n",
      "1.7377377377377377\n",
      "3.09009009009009\n",
      "0.7497497497497497\n",
      "2.858858858858859\n",
      "3.4124124124124124\n",
      "0.3993993993993994\n",
      "3.972972972972973\n",
      "0.5675675675675675\n",
      "1.1141141141141142\n",
      "4.2392392392392395\n",
      "1.7237237237237237\n",
      "0.5395395395395395\n",
      "3.076076076076076\n",
      "1.023023023023023\n",
      "0.7637637637637638\n",
      "0.8058058058058059\n",
      "2.2632632632632634\n",
      "3.6156156156156154\n",
      "1.0860860860860861\n",
      "1.7587587587587588\n",
      "1.954954954954955\n",
      "1.7307307307307307\n",
      "0.7357357357357357\n",
      "1.3313313313313313\n",
      "2.73973973973974\n",
      "0.29429429429429427\n",
      "1.044044044044044\n",
      "1.1841841841841843\n",
      "2.858858858858859\n",
      "0.9179179179179179\n",
      "4.302302302302302\n",
      "2.893893893893894\n",
      "0.9459459459459459\n",
      "1.4434434434434433\n",
      "2.186186186186186\n",
      "0.8198198198198198\n",
      "2.088088088088088\n",
      "1.17017017017017\n",
      "1.037037037037037\n",
      "1.9409409409409408\n",
      "2.116116116116116\n",
      "1.4294294294294294\n",
      "1.8638638638638638\n",
      "2.424424424424424\n",
      "0.8688688688688688\n",
      "3.944944944944945\n",
      "2.116116116116116\n",
      "2.032032032032032\n",
      "1.2472472472472473\n",
      "1.5275275275275275\n",
      "1.7027027027027026\n",
      "2.032032032032032\n",
      "1.044044044044044\n",
      "2.4594594594594597\n",
      "2.7677677677677677\n",
      "3.6016016016016015\n",
      "5.416416416416417\n",
      "0.9529529529529529\n",
      "1.5205205205205206\n",
      "1.8078078078078077\n",
      "1.5835835835835836\n",
      "5.206206206206206\n",
      "1.982982982982983\n",
      "1.6326326326326326\n",
      "1.7517517517517518\n",
      "2.3123123123123124\n",
      "2.4104104104104103\n",
      "0.4694694694694695\n",
      "0.7357357357357357\n",
      "1.4574574574574575\n",
      "2.984984984984985\n",
      "1.9339339339339339\n",
      "0.9529529529529529\n",
      "0.7077077077077077\n",
      "3.1321321321321323\n",
      "2.7187187187187187\n",
      "1.17017017017017\n",
      "0.8618618618618619\n",
      "1.2612612612612613\n",
      "3.006006006006006\n",
      "1.8288288288288288\n",
      "2.284284284284284\n",
      "1.8148148148148149\n",
      "1.954954954954955\n",
      "4.008008008008008\n",
      "1.7867867867867868\n",
      "4.218218218218218\n",
      "2.6136136136136137\n",
      "1.4364364364364364\n",
      "1.9339339339339339\n",
      "2.06006006006006\n",
      "2.214214214214214\n",
      "1.5415415415415414\n",
      "3.825825825825826\n",
      "5.36036036036036\n",
      "3.188188188188188\n",
      "0.6376376376376376\n",
      "1.6676676676676676\n",
      "0.7147147147147147\n",
      "3.734734734734735\n",
      "3.4894894894894897\n",
      "3.5315315315315314\n",
      "4.722722722722723\n",
      "0.938938938938939\n",
      "1.4994994994994995\n",
      "1.3313313313313313\n",
      "0.9039039039039038\n",
      "4.246246246246246\n",
      "4.106106106106106\n",
      "1.3313313313313313\n",
      "3.7067067067067065\n",
      "3.111111111111111\n",
      "0.1891891891891892\n",
      "0.924924924924925\n",
      "1.3943943943943944\n",
      "1.8708708708708708\n",
      "5.01001001001001\n",
      "0.2802802802802803\n",
      "2.5225225225225225\n",
      "1.3593593593593594\n",
      "2.214214214214214\n",
      "1.2892892892892893\n",
      "0.7357357357357357\n",
      "1.8638638638638638\n",
      "2.27027027027027\n",
      "0.7987987987987988\n",
      "2.5785785785785786\n",
      "1.1281281281281281\n",
      "3.4474474474474475\n",
      "1.079079079079079\n",
      "1.6256256256256256\n",
      "0.7357357357357357\n",
      "2.4734734734734736\n",
      "2.4664664664664664\n",
      "1.5625625625625625\n",
      "1.5205205205205206\n",
      "2.9079079079079078\n",
      "6.201201201201201\n",
      "3.048048048048048\n",
      "1.079079079079079\n",
      "1.0510510510510511\n",
      "1.1001001001001\n",
      "2.13013013013013\n",
      "1.079079079079079\n",
      "3.958958958958959\n",
      "3.853853853853854\n",
      "5.626626626626627\n",
      "2.081081081081081\n",
      "3.3633633633633635\n",
      "2.3613613613613613\n",
      "1.8358358358358358\n",
      "2.7467467467467466\n",
      "5.276276276276276\n",
      "1.072072072072072\n",
      "1.7727727727727727\n",
      "2.5365365365365364\n",
      "2.3333333333333335\n",
      "1.6186186186186187\n",
      "1.4504504504504505\n",
      "5.297297297297297\n",
      "0.7707707707707707\n",
      "0.7287287287287287\n",
      "2.2772772772772774\n",
      "2.4314314314314314\n",
      "1.4994994994994995\n",
      "2.3123123123123124\n",
      "1.7587587587587588\n",
      "2.5575575575575575\n",
      "3.209209209209209\n",
      "5.787787787787788\n",
      "4.456456456456457\n",
      "0.8548548548548548\n",
      "1.5765765765765765\n",
      "1.093093093093093\n",
      "1.7027027027027026\n",
      "4.694694694694695\n",
      "3.6996996996997\n",
      "0.45545545545545546\n",
      "1.4854854854854855\n",
      "1.4504504504504505\n",
      "1.5345345345345345\n",
      "1.4154154154154155\n",
      "0.9459459459459459\n",
      "5.976976976976977\n",
      "1.84984984984985\n",
      "2.7117117117117115\n",
      "2.039039039039039\n",
      "1.5275275275275275\n",
      "0.9039039039039038\n",
      "0.9669669669669669\n",
      "1.037037037037037\n",
      "3.5875875875875876\n",
      "2.4104104104104103\n",
      "3.6506506506506504\n",
      "4.834834834834835\n",
      "1.6606606606606606\n",
      "1.4084084084084083\n",
      "0.7147147147147147\n",
      "3.6926926926926926\n",
      "2.858858858858859\n",
      "2.053053053053053\n",
      "3.2932932932932935\n",
      "1.7937937937937938\n",
      "1.079079079079079\n",
      "3.4754754754754753\n",
      "1.968968968968969\n",
      "4.86986986986987\n",
      "0.95995995995996\n",
      "1.3593593593593594\n",
      "3.139139139139139\n",
      "1.6326326326326326\n",
      "3.153153153153153\n",
      "0.6306306306306306\n",
      "1.7167167167167168\n",
      "4.61061061061061\n",
      "3.3983983983983985\n",
      "0.8618618618618619\n",
      "3.3563563563563563\n",
      "3.209209209209209\n",
      "3.188188188188188\n",
      "1.7307307307307307\n",
      "3.7137137137137137\n",
      "3.4824824824824825\n",
      "2.305305305305305\n",
      "2.34034034034034\n",
      "1.107107107107107\n",
      "4.708708708708708\n",
      "3.076076076076076\n",
      "2.13013013013013\n",
      "1.1981981981981982\n",
      "4.218218218218218\n",
      "2.088088088088088\n",
      "1.5765765765765765\n",
      "1.8008008008008007\n",
      "3.3283283283283285\n",
      "1.2612612612612613\n",
      "1.3943943943943944\n",
      "3.2862862862862863\n",
      "3.111111111111111\n",
      "2.8378378378378377\n",
      "4.925925925925926\n",
      "1.7937937937937938\n",
      "1.1001001001001\n",
      "0.9879879879879879\n",
      "0.95995995995996\n",
      "2.053053053053053\n",
      "1.5905905905905906\n",
      "1.3733733733733733\n",
      "1.3803803803803805\n",
      "2.3123123123123124\n",
      "0.924924924924925\n",
      "2.06006006006006\n",
      "0.7007007007007007\n",
      "1.3453453453453452\n",
      "1.884884884884885\n",
      "1.2682682682682682\n",
      "1.8428428428428427\n",
      "3.930930930930931\n",
      "1.7587587587587588\n",
      "1.5205205205205206\n",
      "3.944944944944945\n",
      "0.4624624624624625\n",
      "3.2162162162162162\n",
      "2.011011011011011\n",
      "1.7797797797797799\n",
      "4.323323323323323\n",
      "2.4944944944944947\n",
      "2.018018018018018\n",
      "1.2822822822822824\n",
      "2.7187187187187187\n",
      "1.142142142142142\n",
      "1.7307307307307307\n",
      "4.37937937937938\n",
      "1.7657657657657657\n",
      "1.8988988988988988\n",
      "1.2052052052052051\n",
      "3.90990990990991\n",
      "2.5435435435435436\n",
      "3.6716716716716715\n",
      "2.4454454454454453\n",
      "1.3453453453453452\n",
      "4.5825825825825826\n",
      "1.5415415415415414\n",
      "1.6116116116116117\n",
      "3.7627627627627627\n",
      "1.5905905905905906\n",
      "4.197197197197197\n",
      "1.009009009009009\n",
      "4.463463463463463\n",
      "1.3313313313313313\n",
      "1.212212212212212\n",
      "2.6276276276276276\n",
      "2.4524524524524525\n",
      "4.043043043043043\n",
      "3.174174174174174\n",
      "1.4714714714714714\n",
      "1.2402402402402402\n",
      "3.076076076076076\n",
      "0.3013013013013013\n",
      "2.06006006006006\n",
      "1.9059059059059058\n",
      "1.9759759759759759\n",
      "1.177177177177177\n",
      "0.7147147147147147\n",
      "2.032032032032032\n",
      "1.002002002002002\n",
      "5.64064064064064\n",
      "0.8618618618618619\n",
      "0.4134134134134134\n",
      "2.5855855855855854\n",
      "2.3333333333333335\n",
      "1.7377377377377377\n",
      "0.5325325325325325\n",
      "0.924924924924925\n",
      "3.9519519519519517\n",
      "0.9109109109109109\n",
      "3.6366366366366365\n",
      "0.5815815815815816\n",
      "2.5365365365365364\n",
      "3.4964964964964964\n",
      "1.212212212212212\n",
      "0.8968968968968969\n",
      "3.279279279279279\n",
      "1.2052052052052051\n",
      "3.181181181181181\n",
      "1.7307307307307307\n",
      "4.33033033033033\n",
      "2.5085085085085086\n",
      "3.5525525525525525\n",
      "0.32932932932932935\n",
      "1.5695695695695695\n",
      "3.125125125125125\n",
      "2.4174174174174174\n",
      "3.4474474474474475\n",
      "1.884884884884885\n",
      "1.7657657657657657\n",
      "2.081081081081081\n",
      "4.295295295295295\n",
      "0.7917917917917918\n",
      "1.5975975975975976\n",
      "2.13013013013013\n",
      "2.2982982982982985\n",
      "3.7767767767767766\n",
      "1.7867867867867868\n",
      "1.0860860860860861\n",
      "1.1981981981981982\n",
      "2.963963963963964\n",
      "1.3733733733733733\n",
      "2.095095095095095\n",
      "1.3593593593593594\n",
      "2.4034034034034035\n",
      "1.8008008008008007\n",
      "1.3873873873873874\n",
      "1.3383383383383383\n",
      "1.5835835835835836\n",
      "1.1561561561561562\n",
      "0.5045045045045045\n",
      "0.7567567567567568\n",
      "3.4964964964964964\n",
      "1.3453453453453452\n",
      "2.6276276276276276\n",
      "1.4504504504504505\n",
      "2.9079079079079078\n",
      "0.36436436436436437\n",
      "2.5645645645645647\n",
      "2.4734734734734736\n",
      "4.064064064064064\n",
      "2.7327327327327327\n",
      "2.3683683683683685\n",
      "1.3663663663663663\n",
      "3.279279279279279\n",
      "2.1021021021021022\n",
      "1.5695695695695695\n",
      "2.6486486486486487\n",
      "4.701701701701702\n",
      "1.3173173173173174\n",
      "3.384384384384384\n",
      "2.5505505505505504\n",
      "4.505505505505505\n",
      "3.5735735735735736\n",
      "0.3083083083083083\n",
      "3.2582582582582584\n",
      "3.195195195195195\n",
      "1.2682682682682682\n",
      "2.4664664664664664\n",
      "3.986986986986987\n",
      "1.3733733733733733\n",
      "1.7727727727727727\n",
      "4.3093093093093096\n",
      "1.5275275275275275\n",
      "1.982982982982983\n",
      "1.884884884884885\n",
      "1.2892892892892893\n",
      "0.980980980980981\n",
      "4.085085085085085\n",
      "1.4084084084084083\n",
      "0.9039039039039038\n",
      "2.94994994994995\n",
      "5.346346346346346\n",
      "1.5695695695695695\n",
      "1.5275275275275275\n",
      "2.2002002002002\n",
      "1.5905905905905906\n",
      "2.06006006006006\n",
      "2.5505505505505504\n",
      "0.4624624624624625\n",
      "1.072072072072072\n",
      "3.118118118118118\n",
      "4.925925925925926\n",
      "2.8168168168168166\n",
      "1.2892892892892893\n",
      "3.6576576576576576\n",
      "2.725725725725726\n",
      "1.5065065065065064\n",
      "4.603603603603603\n",
      "1.954954954954955\n",
      "1.009009009009009\n",
      "3.174174174174174\n",
      "2.2212212212212212\n",
      "1.3453453453453452\n",
      "3.76976976976977\n",
      "5.108108108108108\n",
      "2.5575575575575575\n",
      "3.23023023023023\n",
      "1.2752752752752752\n",
      "0.8828828828828829\n",
      "1.7727727727727727\n",
      "1.5835835835835836\n",
      "1.6746746746746746\n",
      "1.5065065065065064\n",
      "2.4174174174174174\n",
      "2.088088088088088\n",
      "1.7587587587587588\n",
      "0.5325325325325325\n",
      "1.91991991991992\n",
      "3.454454454454454\n",
      "1.2402402402402402\n",
      "4.631631631631632\n",
      "2.5015015015015014\n",
      "5.003003003003003\n",
      "3.6226226226226226\n",
      "2.5925925925925926\n",
      "1.5485485485485486\n",
      "2.725725725725726\n",
      "2.4944944944944947\n",
      "0.5955955955955956\n",
      "1.6676676676676676\n",
      "1.1001001001001\n",
      "2.5155155155155153\n",
      "1.058058058058058\n",
      "0.6236236236236237\n",
      "0.994994994994995\n",
      "2.3963963963963963\n",
      "1.212212212212212\n",
      "1.9059059059059058\n",
      "0.5745745745745746\n",
      "1.954954954954955\n",
      "3.3073073073073074\n",
      "3.111111111111111\n",
      "1.9339339339339339\n",
      "1.4854854854854855\n",
      "0.7007007007007007\n",
      "1.3803803803803805\n",
      "4.771771771771772\n",
      "1.0860860860860861\n",
      "4.057057057057057\n",
      "1.3103103103103102\n",
      "1.4574574574574575\n",
      "4.974974974974975\n",
      "3.188188188188188\n",
      "1.6816816816816818\n",
      "2.074074074074074\n",
      "0.980980980980981\n",
      "1.3173173173173174\n",
      "1.5835835835835836\n",
      "4.106106106106106\n",
      "2.6416416416416415\n",
      "2.6766766766766765\n",
      "0.7217217217217217\n",
      "3.2162162162162162\n",
      "0.8338338338338338\n",
      "1.1981981981981982\n",
      "3.965965965965966\n",
      "3.2722722722722724\n",
      "3.153153153153153\n",
      "1.7657657657657657\n",
      "1.4224224224224224\n",
      "1.065065065065065\n",
      "2.5645645645645647\n",
      "1.2262262262262262\n",
      "3.6296296296296298\n",
      "4.022022022022022\n",
      "2.7467467467467466\n",
      "1.2892892892892893\n",
      "2.8168168168168166\n",
      "2.858858858858859\n",
      "4.155155155155155\n",
      "1.5135135135135136\n",
      "2.2632632632632634\n",
      "2.893893893893894\n",
      "1.3663663663663663\n",
      "0.8898898898898899\n",
      "2.7887887887887888\n",
      "2.5365365365365364\n",
      "1.926926926926927\n",
      "1.1561561561561562\n",
      "3.181181181181181\n",
      "1.6536536536536537\n",
      "2.095095095095095\n",
      "0.9179179179179179\n",
      "0.8688688688688688\n",
      "2.011011011011011\n",
      "1.002002002002002\n",
      "1.3033033033033032\n",
      "2.1931931931931934\n",
      "1.093093093093093\n",
      "1.7167167167167168\n",
      "2.3473473473473474\n",
      "1.072072072072072\n",
      "0.9879879879879879\n",
      "1.5415415415415414\n",
      "1.5065065065065064\n",
      "2.80980980980981\n",
      "3.4474474474474475\n",
      "4.638638638638638\n",
      "1.8638638638638638\n",
      "1.044044044044044\n",
      "0.8478478478478478\n",
      "1.3173173173173174\n",
      "5.843843843843843\n",
      "2.4734734734734736\n",
      "3.083083083083083\n",
      "3.4964964964964964\n",
      "1.926926926926927\n",
      "2.4454454454454453\n",
      "1.8568568568568569\n",
      "1.5555555555555556\n",
      "4.2392392392392395\n",
      "2.3823823823823824\n",
      "3.3283283283283285\n",
      "3.5595595595595597\n",
      "3.4824824824824825\n",
      "1.058058058058058\n",
      "1.6956956956956957\n",
      "0.4274274274274274\n",
      "1.5135135135135136\n",
      "0.5185185185185185\n",
      "1.4224224224224224\n",
      "0.08408408408408408\n",
      "1.1911911911911912\n",
      "5.598598598598598\n",
      "0.7287287287287287\n",
      "2.039039039039039\n",
      "1.17017017017017\n",
      "3.048048048048048\n",
      "1.3593593593593594\n",
      "1.8988988988988988\n",
      "2.179179179179179\n",
      "1.8988988988988988\n",
      "3.188188188188188\n",
      "0.9669669669669669\n",
      "2.2982982982982985\n",
      "2.6836836836836837\n",
      "1.5695695695695695\n",
      "2.13013013013013\n",
      "1.6256256256256256\n",
      "3.09009009009009\n",
      "1.3663663663663663\n",
      "3.265265265265265\n",
      "4.435435435435435\n",
      "0.7917917917917918\n",
      "1.3173173173173174\n",
      "4.33033033033033\n",
      "0.6936936936936937\n",
      "1.1631631631631631\n",
      "4.8558558558558556\n",
      "1.5415415415415414\n",
      "1.7517517517517518\n",
      "1.065065065065065\n",
      "4.33033033033033\n",
      "2.2002002002002\n",
      "3.3283283283283285\n",
      "1.3103103103103102\n",
      "3.4754754754754753\n",
      "1.3243243243243243\n",
      "1.3523523523523524\n",
      "0.95995995995996\n",
      "2.4384384384384385\n",
      "0.9529529529529529\n",
      "1.212212212212212\n",
      "1.2682682682682682\n",
      "0.9879879879879879\n",
      "5.297297297297297\n",
      "3.1671671671671673\n",
      "5.227227227227227\n",
      "1.6886886886886887\n",
      "1.5485485485485486\n",
      "1.5275275275275275\n",
      "3.3983983983983985\n",
      "1.009009009009009\n",
      "1.2892892892892893\n",
      "2.6206206206206204\n",
      "3.4264264264264264\n",
      "3.937937937937938\n",
      "1.8288288288288288\n",
      "1.2472472472472473\n",
      "1.5485485485485486\n",
      "2.3613613613613613\n",
      "0.36436436436436437\n",
      "1.212212212212212\n",
      "1.2752752752752752\n",
      "1.7867867867867868\n",
      "4.05005005005005\n",
      "5.031031031031031\n",
      "1.135135135135135\n",
      "2.87987987987988\n",
      "0.4694694694694695\n",
      "4.141141141141141\n",
      "5.605605605605605\n",
      "4.813813813813813\n",
      "1.2402402402402402\n",
      "0.7007007007007007\n",
      "1.5835835835835836\n",
      "0.9529529529529529\n",
      "4.421421421421422\n",
      "2.4874874874874875\n",
      "0.5255255255255256\n",
      "1.3943943943943944\n",
      "3.3423423423423424\n",
      "1.3663663663663663\n",
      "0.8268268268268268\n",
      "4.043043043043043\n",
      "1.98998998998999\n",
      "4.8558558558558556\n",
      "2.6206206206206204\n",
      "2.795795795795796\n",
      "1.5205205205205206\n",
      "5.5075075075075075\n",
      "0.7917917917917918\n",
      "1.2612612612612613\n",
      "1.6046046046046045\n",
      "2.032032032032032\n",
      "1.3243243243243243\n",
      "4.974974974974975\n",
      "2.725725725725726\n",
      "4.183183183183183\n",
      "2.5365365365365364\n",
      "0.95995995995996\n",
      "0.9039039039039038\n",
      "1.884884884884885\n",
      "0.9109109109109109\n",
      "1.7867867867867868\n",
      "1.037037037037037\n",
      "1.3173173173173174\n",
      "4.7157157157157155\n",
      "4.099099099099099\n",
      "2.067067067067067\n",
      "1.212212212212212\n",
      "1.968968968968969\n",
      "1.3593593593593594\n",
      "3.755755755755756\n",
      "2.179179179179179\n",
      "1.7447447447447448\n",
      "2.4734734734734736\n",
      "1.3103103103103102\n",
      "1.8918918918918919\n",
      "3.825825825825826\n",
      "1.8708708708708708\n",
      "1.093093093093093\n",
      "2.725725725725726\n",
      "1.4574574574574575\n",
      "2.8168168168168166\n",
      "1.6606606606606606\n",
      "2.6276276276276276\n",
      "3.5035035035035036\n",
      "1.5065065065065064\n",
      "3.181181181181181\n",
      "3.8818818818818817\n",
      "3.6226226226226226\n",
      "0.24524524524524524\n",
      "3.895895895895896\n",
      "1.5275275275275275\n",
      "3.3773773773773774\n",
      "4.834834834834835\n",
      "1.3943943943943944\n",
      "1.072072072072072\n",
      "1.3313313313313313\n",
      "1.2752752752752752\n",
      "1.6326326326326326\n",
      "2.942942942942943\n",
      "1.6816816816816818\n",
      "1.947947947947948\n",
      "1.1281281281281281\n",
      "4.365365365365365\n",
      "4.827827827827828\n",
      "3.16016016016016\n",
      "2.2282282282282284\n",
      "0.8408408408408409\n",
      "1.2612612612612613\n",
      "2.053053053053053\n",
      "3.23023023023023\n",
      "1.5835835835835836\n",
      "0.42042042042042044\n",
      "2.3963963963963963\n",
      "3.853853853853854\n",
      "0.9319319319319319\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(1000):\n",
    "    print(K_aerosol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>g1</th>\n",
       "      <th>Cos(SZA)</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>0.849374</td>\n",
       "      <td>0.951706</td>\n",
       "      <td>708.317583</td>\n",
       "      <td>3187.429121</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>0.814209</td>\n",
       "      <td>0.470531</td>\n",
       "      <td>708.012549</td>\n",
       "      <td>3186.056472</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.240763</td>\n",
       "      <td>707.890641</td>\n",
       "      <td>3185.507886</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>0.852120</td>\n",
       "      <td>0.453122</td>\n",
       "      <td>708.341159</td>\n",
       "      <td>3187.535216</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>0.869006</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>707.958273</td>\n",
       "      <td>3185.812230</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>0.959487</td>\n",
       "      <td>0.962254</td>\n",
       "      <td>708.064025</td>\n",
       "      <td>3186.288112</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>0.812618</td>\n",
       "      <td>0.025776</td>\n",
       "      <td>708.122465</td>\n",
       "      <td>3186.551091</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.909007</td>\n",
       "      <td>707.894025</td>\n",
       "      <td>3185.523112</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>0.976101</td>\n",
       "      <td>0.859409</td>\n",
       "      <td>708.028352</td>\n",
       "      <td>3186.127582</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>0.843193</td>\n",
       "      <td>0.993832</td>\n",
       "      <td>707.922330</td>\n",
       "      <td>3185.650486</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence        g1  Cos(SZA)        mu_g  \\\n",
       "0      17.879232  3858.242750  0.894296  0.849374  0.951706  708.317583   \n",
       "1      61.931220  1112.943981  0.419185  0.814209  0.470531  708.012549   \n",
       "2      76.068422    15.772549  0.729906  0.915442  0.240763  707.890641   \n",
       "3      63.055806  4070.432067  0.251342  0.852120  0.453122  708.341159   \n",
       "4       2.351437   624.459254  0.881197  0.869006  0.999158  707.958273   \n",
       "...          ...          ...       ...       ...       ...         ...   \n",
       "90621  15.792506  1576.224940  0.696585  0.959487  0.962254  708.064025   \n",
       "90622  88.522985  2102.182329  0.968884  0.812618  0.025776  708.122465   \n",
       "90623  24.631558    46.223183  0.761757  0.789790  0.909007  707.894025   \n",
       "90624  30.749744  1255.164902  0.559097  0.976101  0.859409  708.028352   \n",
       "90625   6.366867   300.971201  0.195906  0.843193  0.993832  707.922330   \n",
       "\n",
       "              mu_a  muprime_g  muprime_a   mprime_g   mprime_a  \n",
       "0      3187.429121   0.951776   0.951721   0.684362   0.152645  \n",
       "1      3186.056472   0.471695   0.470791   1.873413   1.217586  \n",
       "2      3185.507886   0.243492   0.241375   4.099723   4.110381  \n",
       "3      3187.535216   0.454355   0.453397   1.400187   0.288163  \n",
       "4      3185.812230   0.999159   0.999158   0.933753   0.732430  \n",
       "...            ...        ...        ...        ...        ...  \n",
       "90621  3186.288112   0.962308   0.962266   0.872219   0.472533  \n",
       "90622  3186.551091   0.042399   0.030856  18.672395  11.328534  \n",
       "90623  3185.523112   0.909141   0.909037   1.094304   1.074933  \n",
       "90624  3186.127582   0.859623   0.859456   1.011868   0.621184  \n",
       "90625  3185.650486   0.993841   0.993834   0.973105   0.865627  \n",
       "\n",
       "[90626 rows x 18 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91444475],\n",
       "       [0.5945755 ],\n",
       "       [0.28459194],\n",
       "       ...,\n",
       "       [0.73562703],\n",
       "       [0.92437937],\n",
       "       [0.73044082]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.915453  ],\n",
       "       [0.59725976],\n",
       "       [0.28065583],\n",
       "       ...,\n",
       "       [0.73353857],\n",
       "       [0.92147213],\n",
       "       [0.73259103]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_real=scaler_y.inverse_transform(y_pred)\n",
    "y_pred_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91444475],\n",
       "       [0.5945755 ],\n",
       "       [0.28459194],\n",
       "       ...,\n",
       "       [0.73562703],\n",
       "       [0.92437937],\n",
       "       [0.73044082]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real['BOA_fraction']=y_test_real\n",
    "X_test_real['BOA_fraction_pred']=y_pred_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>708.317583</td>\n",
       "      <td>3187.429121</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.915453</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>915.453003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>708.012549</td>\n",
       "      <td>3186.056472</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.597260</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>597.259766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>707.890641</td>\n",
       "      <td>3185.507886</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>280.655823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>708.341159</td>\n",
       "      <td>3187.535216</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.373551</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.550720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>707.958273</td>\n",
       "      <td>3185.812230</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.833302</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>833.301880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>708.064025</td>\n",
       "      <td>3186.288112</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.801431</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>801.431396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>708.122465</td>\n",
       "      <td>3186.551091</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.045344</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>45.344391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>707.894025</td>\n",
       "      <td>3185.523112</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.733539</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>733.538574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>708.028352</td>\n",
       "      <td>3186.127582</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.921472</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>921.472107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>707.922330</td>\n",
       "      <td>3185.650486</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>732.591003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...        mu_g         mu_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...  708.317583  3187.429121   \n",
       "1      61.931220  1112.943981  0.419185  ...  708.012549  3186.056472   \n",
       "2      76.068422    15.772549  0.729906  ...  707.890641  3185.507886   \n",
       "3      63.055806  4070.432067  0.251342  ...  708.341159  3187.535216   \n",
       "4       2.351437   624.459254  0.881197  ...  707.958273  3185.812230   \n",
       "...          ...          ...       ...  ...         ...          ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...  708.064025  3186.288112   \n",
       "90622  88.522985  2102.182329  0.968884  ...  708.122465  3186.551091   \n",
       "90623  24.631558    46.223183  0.761757  ...  707.894025  3185.523112   \n",
       "90624  30.749744  1255.164902  0.559097  ...  708.028352  3186.127582   \n",
       "90625   6.366867   300.971201  0.195906  ...  707.922330  3185.650486   \n",
       "\n",
       "       muprime_g  muprime_a   mprime_g   mprime_a  BOA_fraction  \\\n",
       "0       0.951776   0.951721   0.684362   0.152645      0.914445   \n",
       "1       0.471695   0.470791   1.873413   1.217586      0.594575   \n",
       "2       0.243492   0.241375   4.099723   4.110381      0.284592   \n",
       "3       0.454355   0.453397   1.400187   0.288163      0.373719   \n",
       "4       0.999159   0.999158   0.933753   0.732430      0.832138   \n",
       "...          ...        ...        ...        ...           ...   \n",
       "90621   0.962308   0.962266   0.872219   0.472533      0.801588   \n",
       "90622   0.042399   0.030856  18.672395  11.328534      0.032089   \n",
       "90623   0.909141   0.909037   1.094304   1.074933      0.735627   \n",
       "90624   0.859623   0.859456   1.011868   0.621184      0.924379   \n",
       "90625   0.993841   0.993834   0.973105   0.865627      0.730441   \n",
       "\n",
       "       BOA_fraction_pred      BOA_RT    BOA_pred  \n",
       "0               0.915453  914.444755  915.453003  \n",
       "1               0.597260  594.575497  597.259766  \n",
       "2               0.280656  284.591943  280.655823  \n",
       "3               0.373551  373.718733  373.550720  \n",
       "4               0.833302  832.137702  833.301880  \n",
       "...                  ...         ...         ...  \n",
       "90621           0.801431  801.588284  801.431396  \n",
       "90622           0.045344   32.089238   45.344391  \n",
       "90623           0.733539  735.627029  733.538574  \n",
       "90624           0.921472  924.379368  921.472107  \n",
       "90625           0.732591  730.440816  732.591003  \n",
       "\n",
       "[90626 rows x 22 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real['BOA_RT']=X_test_real['BOA_fraction']*1000\n",
    "X_test_real['BOA_pred']=X_test_real['BOA_fraction_pred']*1000\n",
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>0.948453</td>\n",
       "      <td>0.081226</td>\n",
       "      <td>0.979127</td>\n",
       "      <td>0.989486</td>\n",
       "      <td>0.052923</td>\n",
       "      <td>2.006293</td>\n",
       "      <td>1.985198</td>\n",
       "      <td>85.79666</td>\n",
       "      <td>339.707196</td>\n",
       "      <td>0.475261</td>\n",
       "      <td>...</td>\n",
       "      <td>707.926634</td>\n",
       "      <td>3185.669854</td>\n",
       "      <td>0.081866</td>\n",
       "      <td>0.075367</td>\n",
       "      <td>11.762631</td>\n",
       "      <td>11.195724</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.002975</td>\n",
       "      <td>0.15682</td>\n",
       "      <td>-2.974701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "6066  0.948453  0.081226  0.979127  0.989486  0.052923  2.006293  1.985198   \n",
       "\n",
       "           SZA           Z  R_scence  ...        mu_g         mu_a  muprime_g  \\\n",
       "6066  85.79666  339.707196  0.475261  ...  707.926634  3185.669854   0.081866   \n",
       "\n",
       "      muprime_a   mprime_g   mprime_a  BOA_fraction  BOA_fraction_pred  \\\n",
       "6066   0.075367  11.762631  11.195724      0.000157          -0.002975   \n",
       "\n",
       "       BOA_RT  BOA_pred  \n",
       "6066  0.15682 -2.974701  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real[X_test_real['BOA_pred']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>708.317583</td>\n",
       "      <td>3187.429121</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.917256</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>917.255554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>708.012549</td>\n",
       "      <td>3186.056472</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.595136</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>595.135681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>707.890641</td>\n",
       "      <td>3185.507886</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280470</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>280.470062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>708.341159</td>\n",
       "      <td>3187.535216</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.373296</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.295898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>707.958273</td>\n",
       "      <td>3185.812230</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.832918</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>832.917542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>708.064025</td>\n",
       "      <td>3186.288112</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800043</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>800.042725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>708.122465</td>\n",
       "      <td>3186.551091</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.048756</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>48.756180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>707.894025</td>\n",
       "      <td>3185.523112</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.733867</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>733.867493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>708.028352</td>\n",
       "      <td>3186.127582</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.919594</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>919.593994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>707.922330</td>\n",
       "      <td>3185.650486</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.732572</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>732.571533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...        mu_g         mu_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...  708.317583  3187.429121   \n",
       "1      61.931220  1112.943981  0.419185  ...  708.012549  3186.056472   \n",
       "2      76.068422    15.772549  0.729906  ...  707.890641  3185.507886   \n",
       "3      63.055806  4070.432067  0.251342  ...  708.341159  3187.535216   \n",
       "4       2.351437   624.459254  0.881197  ...  707.958273  3185.812230   \n",
       "...          ...          ...       ...  ...         ...          ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...  708.064025  3186.288112   \n",
       "90622  88.522985  2102.182329  0.968884  ...  708.122465  3186.551091   \n",
       "90623  24.631558    46.223183  0.761757  ...  707.894025  3185.523112   \n",
       "90624  30.749744  1255.164902  0.559097  ...  708.028352  3186.127582   \n",
       "90625   6.366867   300.971201  0.195906  ...  707.922330  3185.650486   \n",
       "\n",
       "       muprime_g  muprime_a   mprime_g   mprime_a  BOA_fraction  \\\n",
       "0       0.951776   0.951721   0.684362   0.152645      0.914445   \n",
       "1       0.471695   0.470791   1.873413   1.217586      0.594575   \n",
       "2       0.243492   0.241375   4.099723   4.110381      0.284592   \n",
       "3       0.454355   0.453397   1.400187   0.288163      0.373719   \n",
       "4       0.999159   0.999158   0.933753   0.732430      0.832138   \n",
       "...          ...        ...        ...        ...           ...   \n",
       "90621   0.962308   0.962266   0.872219   0.472533      0.801588   \n",
       "90622   0.042399   0.030856  18.672395  11.328534      0.032089   \n",
       "90623   0.909141   0.909037   1.094304   1.074933      0.735627   \n",
       "90624   0.859623   0.859456   1.011868   0.621184      0.924379   \n",
       "90625   0.993841   0.993834   0.973105   0.865627      0.730441   \n",
       "\n",
       "       BOA_fraction_pred      BOA_RT    BOA_pred  \n",
       "0               0.917256  914.444755  917.255554  \n",
       "1               0.595136  594.575497  595.135681  \n",
       "2               0.280470  284.591943  280.470062  \n",
       "3               0.373296  373.718733  373.295898  \n",
       "4               0.832918  832.137702  832.917542  \n",
       "...                  ...         ...         ...  \n",
       "90621           0.800043  801.588284  800.042725  \n",
       "90622           0.048756   32.089238   48.756180  \n",
       "90623           0.733867  735.627029  733.867493  \n",
       "90624           0.919594  924.379368  919.593994  \n",
       "90625           0.732572  730.440816  732.571533  \n",
       "\n",
       "[90626 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calculate_BOA(thetha,z,Tg_abs,Tg_scat,optical_depth,albedo,alpha):   \n",
    "    def muprime(z,h,µ):\n",
    "        RAYON_TERRESTRE=6371\n",
    "        eta = (RAYON_TERRESTRE*1000 + z) / h\n",
    "        root = (eta*µ)**2  + 2 * eta + 1\n",
    "        sum = (root)**0.5 - eta * µ\n",
    "        if sum > 0 :\n",
    "            return 1/sum\n",
    "        return 1 \n",
    "    Ha=2000\n",
    "    Hg=9000\n",
    "    angle_rad = math.radians(thetha)\n",
    "    µ=math.cos(angle_rad)  \n",
    "    Y_a=muprime(z,Ha,µ)\n",
    "    Y_g=muprime(z,Hg,µ)\n",
    "    Ma=math.exp(-z/Ha)/Y_a\n",
    "    Mg=math.exp(-z/Hg)/Y_g\n",
    "    delta_a_scat=optical_depth*albedo\n",
    "    delta_g_scat=-math.log(Tg_scat)\n",
    "    Ta_abs=math.exp(-optical_depth*(1-albedo))\n",
    "    numerator= 1000*(Tg_abs**Mg)*(Ta_abs**Ma)\n",
    "    denominator=1+alpha*delta_g_scat*Mg+(alpha*(1/3)*delta_a_scat)*Ma\n",
    "    BOA_ia= numerator / denominator\n",
    "    return BOA_ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alpha(BOA_fraction,Ta_abs,Tg_abs,mprime_g,mprime_a,GOD,AODS):   \n",
    "    numerator= (1/BOA_fraction)*(Tg_abs**(mprime_g))*(Ta_abs**(mprime_a))-1\n",
    "    denominator=(GOD * mprime_g) + (AODS * mprime_a) / 3\n",
    "    alpha= numerator / denominator\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.917256</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>917.255554</td>\n",
       "      <td>-0.417386</td>\n",
       "      <td>-0.224256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.595136</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>595.135681</td>\n",
       "      <td>0.375515</td>\n",
       "      <td>0.376680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280470</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>280.470062</td>\n",
       "      <td>0.257075</td>\n",
       "      <td>0.238902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.373296</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.295898</td>\n",
       "      <td>0.475234</td>\n",
       "      <td>0.471005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.832918</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>832.917542</td>\n",
       "      <td>0.125017</td>\n",
       "      <td>0.128247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800043</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>800.042725</td>\n",
       "      <td>0.215932</td>\n",
       "      <td>0.194530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.048756</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>48.756180</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>-0.142424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.733867</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>733.867493</td>\n",
       "      <td>0.168416</td>\n",
       "      <td>0.149952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.919594</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>919.593994</td>\n",
       "      <td>0.166861</td>\n",
       "      <td>0.143431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.732572</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>732.571533</td>\n",
       "      <td>0.222330</td>\n",
       "      <td>0.229081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_g  muprime_a   mprime_g  \\\n",
       "0      17.879232  3858.242750  0.894296  ...   0.951776   0.951721   0.684362   \n",
       "1      61.931220  1112.943981  0.419185  ...   0.471695   0.470791   1.873413   \n",
       "2      76.068422    15.772549  0.729906  ...   0.243492   0.241375   4.099723   \n",
       "3      63.055806  4070.432067  0.251342  ...   0.454355   0.453397   1.400187   \n",
       "4       2.351437   624.459254  0.881197  ...   0.999159   0.999158   0.933753   \n",
       "...          ...          ...       ...  ...        ...        ...        ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...   0.962308   0.962266   0.872219   \n",
       "90622  88.522985  2102.182329  0.968884  ...   0.042399   0.030856  18.672395   \n",
       "90623  24.631558    46.223183  0.761757  ...   0.909141   0.909037   1.094304   \n",
       "90624  30.749744  1255.164902  0.559097  ...   0.859623   0.859456   1.011868   \n",
       "90625   6.366867   300.971201  0.195906  ...   0.993841   0.993834   0.973105   \n",
       "\n",
       "        mprime_a  BOA_fraction  BOA_fraction_pred      BOA_RT    BOA_pred  \\\n",
       "0       0.152645      0.914445           0.917256  914.444755  917.255554   \n",
       "1       1.217586      0.594575           0.595136  594.575497  595.135681   \n",
       "2       4.110381      0.284592           0.280470  284.591943  280.470062   \n",
       "3       0.288163      0.373719           0.373296  373.718733  373.295898   \n",
       "4       0.732430      0.832138           0.832918  832.137702  832.917542   \n",
       "...          ...           ...                ...         ...         ...   \n",
       "90621   0.472533      0.801588           0.800043  801.588284  800.042725   \n",
       "90622  11.328534      0.032089           0.048756   32.089238   48.756180   \n",
       "90623   1.074933      0.735627           0.733867  735.627029  733.867493   \n",
       "90624   0.621184      0.924379           0.919594  924.379368  919.593994   \n",
       "90625   0.865627      0.730441           0.732572  730.440816  732.571533   \n",
       "\n",
       "       alpha_pred     alpha  \n",
       "0       -0.417386 -0.224256  \n",
       "1        0.375515  0.376680  \n",
       "2        0.257075  0.238902  \n",
       "3        0.475234  0.471005  \n",
       "4        0.125017  0.128247  \n",
       "...           ...       ...  \n",
       "90621    0.215932  0.194530  \n",
       "90622   -0.142424 -0.142424  \n",
       "90623    0.168416  0.149952  \n",
       "90624    0.166861  0.143431  \n",
       "90625    0.222330  0.229081  \n",
       "\n",
       "[90626 rows x 24 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real['alpha_pred']=X_test_real.apply(\n",
    "    lambda row: calculate_alpha(\n",
    "        row['BOA_fraction_pred'],\n",
    "        row['Ta_abs'],\n",
    "        row['Tg_abs'],\n",
    "        row['mprime_g'],\n",
    "        row['mprime_a'],\n",
    "        row['GOD'],\n",
    "        row['AODS'],\n",
    "    ), axis=1\n",
    ")\n",
    "X_test_real['alpha']=X_test_real.apply(\n",
    "    lambda row: calculate_alpha(\n",
    "        row['BOA_fraction'],\n",
    "        row['Ta_abs'],\n",
    "        row['Tg_abs'],\n",
    "        row['mprime_g'],\n",
    "        row['mprime_a'],\n",
    "        row['GOD'],\n",
    "        row['AODS'],\n",
    "    ), axis=1\n",
    ")\n",
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_RT_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.915612</td>\n",
       "      <td>-0.304591</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>915.611863</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>914.444755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.594615</td>\n",
       "      <td>0.376599</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>594.614685</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>594.575497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280167</td>\n",
       "      <td>0.258431</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>280.167222</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>284.591943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>372.900099</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.718733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.834354</td>\n",
       "      <td>0.119085</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>834.353626</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>832.137702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800986</td>\n",
       "      <td>0.202855</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>800.986350</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>801.588284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>36.498807</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>32.089238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.734697</td>\n",
       "      <td>0.159697</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>734.697282</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>735.627029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.922319</td>\n",
       "      <td>0.153491</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>922.318637</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>924.379368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>0.228026</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>730.773032</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>730.440816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_a   mprime_g   mprime_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...   0.951721   0.684362   0.152645   \n",
       "1      61.931220  1112.943981  0.419185  ...   0.470791   1.873413   1.217586   \n",
       "2      76.068422    15.772549  0.729906  ...   0.241375   4.099723   4.110381   \n",
       "3      63.055806  4070.432067  0.251342  ...   0.453397   1.400187   0.288163   \n",
       "4       2.351437   624.459254  0.881197  ...   0.999158   0.933753   0.732430   \n",
       "...          ...          ...       ...  ...        ...        ...        ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...   0.962266   0.872219   0.472533   \n",
       "90622  88.522985  2102.182329  0.968884  ...   0.030856  18.672395  11.328534   \n",
       "90623  24.631558    46.223183  0.761757  ...   0.909037   1.094304   1.074933   \n",
       "90624  30.749744  1255.164902  0.559097  ...   0.859456   1.011868   0.621184   \n",
       "90625   6.366867   300.971201  0.195906  ...   0.993834   0.973105   0.865627   \n",
       "\n",
       "       BOA_fraction  BOA_fraction_pred  alpha_pred     alpha    BOA_pred  \\\n",
       "0          0.914445           0.915612   -0.304591 -0.224256  915.611863   \n",
       "1          0.594575           0.594615    0.376599  0.376680  594.614685   \n",
       "2          0.284592           0.280167    0.258431  0.238902  280.167222   \n",
       "3          0.373719           0.372900    0.479201  0.471005  372.900099   \n",
       "4          0.832138           0.834354    0.119085  0.128247  834.353626   \n",
       "...             ...                ...         ...       ...         ...   \n",
       "90621      0.801588           0.800986    0.202855  0.194530  800.986350   \n",
       "90622      0.032089           0.036499   -0.142424 -0.142424   36.498807   \n",
       "90623      0.735627           0.734697    0.159697  0.149952  734.697282   \n",
       "90624      0.924379           0.922319    0.153491  0.143431  922.318637   \n",
       "90625      0.730441           0.730773    0.228026  0.229081  730.773032   \n",
       "\n",
       "           BOA_RT  BOA_RT_test  \n",
       "0      914.444755   914.444755  \n",
       "1      594.575497   594.575497  \n",
       "2      284.591943   284.591943  \n",
       "3      373.718733   373.718733  \n",
       "4      832.137702   832.137702  \n",
       "...           ...          ...  \n",
       "90621  801.588284   801.588284  \n",
       "90622   32.089238    32.089238  \n",
       "90623  735.627029   735.627029  \n",
       "90624  924.379368   924.379368  \n",
       "90625  730.440816   730.440816  \n",
       "\n",
       "[90626 rows x 25 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real['BOA_pred'] = X_test_real.apply(lambda row: calculate_BOA(row['SZA'], row['Z'],row['Tg_abs'],row['Tg_scat'],row['AOD'],row['SSA'],row['alpha_pred']), axis=1)\n",
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_BOA_IA(Tg_abs,Mg,Ta_abs,Ma,alpha,delta_g_scat,AODS):\n",
    "    numerator= 1000*(Tg_abs**Mg)*(Ta_abs**Ma)\n",
    "    denominator=1+alpha*delta_g_scat*Mg+(alpha*(1/3)*AODS)*Ma\n",
    "    \n",
    "    BOA_ia= numerator / denominator\n",
    "    return BOA_ia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real['BOA_pred'] = X_test_real.apply(lambda row: calculate_BOA_IA(row['Tg_abs'], row['mprime_g'],row['Ta_abs'],row['mprime_a'],row['alpha_pred'],row['GOD'],row['AODS']), axis=1)\n",
    "X_test_real['BOA_RT']=1000*X_test_real['BOA_fraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real['BOA_RT_test'] = X_test_real.apply(lambda row: calculate_BOA_IA(row['Tg_abs'], row['mprime_g'],row['Ta_abs'],row['mprime_a'],row['alpha'],row['GOD'],row['AODS']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_RT_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.915612</td>\n",
       "      <td>-0.304591</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>915.611863</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>914.444755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.594615</td>\n",
       "      <td>0.376599</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>594.614685</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>594.575497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280167</td>\n",
       "      <td>0.258431</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>280.167222</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>284.591943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>372.900099</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.718733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.834354</td>\n",
       "      <td>0.119085</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>834.353626</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>832.137702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800986</td>\n",
       "      <td>0.202855</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>800.986350</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>801.588284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>36.498807</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>32.089238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.734697</td>\n",
       "      <td>0.159697</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>734.697282</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>735.627029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.922319</td>\n",
       "      <td>0.153491</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>922.318637</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>924.379368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>0.228026</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>730.773032</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>730.440816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_a   mprime_g   mprime_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...   0.951721   0.684362   0.152645   \n",
       "1      61.931220  1112.943981  0.419185  ...   0.470791   1.873413   1.217586   \n",
       "2      76.068422    15.772549  0.729906  ...   0.241375   4.099723   4.110381   \n",
       "3      63.055806  4070.432067  0.251342  ...   0.453397   1.400187   0.288163   \n",
       "4       2.351437   624.459254  0.881197  ...   0.999158   0.933753   0.732430   \n",
       "...          ...          ...       ...  ...        ...        ...        ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...   0.962266   0.872219   0.472533   \n",
       "90622  88.522985  2102.182329  0.968884  ...   0.030856  18.672395  11.328534   \n",
       "90623  24.631558    46.223183  0.761757  ...   0.909037   1.094304   1.074933   \n",
       "90624  30.749744  1255.164902  0.559097  ...   0.859456   1.011868   0.621184   \n",
       "90625   6.366867   300.971201  0.195906  ...   0.993834   0.973105   0.865627   \n",
       "\n",
       "       BOA_fraction  BOA_fraction_pred  alpha_pred     alpha    BOA_pred  \\\n",
       "0          0.914445           0.915612   -0.304591 -0.224256  915.611863   \n",
       "1          0.594575           0.594615    0.376599  0.376680  594.614685   \n",
       "2          0.284592           0.280167    0.258431  0.238902  280.167222   \n",
       "3          0.373719           0.372900    0.479201  0.471005  372.900099   \n",
       "4          0.832138           0.834354    0.119085  0.128247  834.353626   \n",
       "...             ...                ...         ...       ...         ...   \n",
       "90621      0.801588           0.800986    0.202855  0.194530  800.986350   \n",
       "90622      0.032089           0.036499   -0.142424 -0.142424   36.498807   \n",
       "90623      0.735627           0.734697    0.159697  0.149952  734.697282   \n",
       "90624      0.924379           0.922319    0.153491  0.143431  922.318637   \n",
       "90625      0.730441           0.730773    0.228026  0.229081  730.773032   \n",
       "\n",
       "           BOA_RT  BOA_RT_test  \n",
       "0      914.444755   914.444755  \n",
       "1      594.575497   594.575497  \n",
       "2      284.591943   284.591943  \n",
       "3      373.718733   373.718733  \n",
       "4      832.137702   832.137702  \n",
       "...           ...          ...  \n",
       "90621  801.588284   801.588284  \n",
       "90622   32.089238    32.089238  \n",
       "90623  735.627029   735.627029  \n",
       "90624  924.379368   924.379368  \n",
       "90625  730.440816   730.440816  \n",
       "\n",
       "[90626 rows x 25 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Erreur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056358177696628375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae=mean_absolute_error(X_test_real['alpha'],X_test_real['alpha_pred'])\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E_BOA Erreur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3361722759313124"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae=mean_absolute_error(X_test_real['BOA_RT'],X_test_real['BOA_pred'])\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>Erreur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.917256</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>917.255554</td>\n",
       "      <td>-0.417386</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>2.810799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.595136</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>595.135681</td>\n",
       "      <td>0.375515</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>0.560184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280470</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>280.470062</td>\n",
       "      <td>0.257075</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>4.121881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.373296</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.295898</td>\n",
       "      <td>0.475234</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>0.422835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.832918</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>832.917542</td>\n",
       "      <td>0.125017</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>0.779840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800043</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>800.042725</td>\n",
       "      <td>0.215932</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>1.545559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.048756</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>48.756180</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>16.666942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.733867</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>733.867493</td>\n",
       "      <td>0.168416</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>1.759536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.919594</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>919.593994</td>\n",
       "      <td>0.166861</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>4.785374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.732572</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>732.571533</td>\n",
       "      <td>0.222330</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>2.130717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_a   mprime_g   mprime_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...   0.951721   0.684362   0.152645   \n",
       "1      61.931220  1112.943981  0.419185  ...   0.470791   1.873413   1.217586   \n",
       "2      76.068422    15.772549  0.729906  ...   0.241375   4.099723   4.110381   \n",
       "3      63.055806  4070.432067  0.251342  ...   0.453397   1.400187   0.288163   \n",
       "4       2.351437   624.459254  0.881197  ...   0.999158   0.933753   0.732430   \n",
       "...          ...          ...       ...  ...        ...        ...        ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...   0.962266   0.872219   0.472533   \n",
       "90622  88.522985  2102.182329  0.968884  ...   0.030856  18.672395  11.328534   \n",
       "90623  24.631558    46.223183  0.761757  ...   0.909037   1.094304   1.074933   \n",
       "90624  30.749744  1255.164902  0.559097  ...   0.859456   1.011868   0.621184   \n",
       "90625   6.366867   300.971201  0.195906  ...   0.993834   0.973105   0.865627   \n",
       "\n",
       "       BOA_fraction  BOA_fraction_pred      BOA_RT    BOA_pred  alpha_pred  \\\n",
       "0          0.914445           0.917256  914.444755  917.255554   -0.417386   \n",
       "1          0.594575           0.595136  594.575497  595.135681    0.375515   \n",
       "2          0.284592           0.280470  284.591943  280.470062    0.257075   \n",
       "3          0.373719           0.373296  373.718733  373.295898    0.475234   \n",
       "4          0.832138           0.832918  832.137702  832.917542    0.125017   \n",
       "...             ...                ...         ...         ...         ...   \n",
       "90621      0.801588           0.800043  801.588284  800.042725    0.215932   \n",
       "90622      0.032089           0.048756   32.089238   48.756180   -0.142424   \n",
       "90623      0.735627           0.733867  735.627029  733.867493    0.168416   \n",
       "90624      0.924379           0.919594  924.379368  919.593994    0.166861   \n",
       "90625      0.730441           0.732572  730.440816  732.571533    0.222330   \n",
       "\n",
       "          alpha     Erreur  \n",
       "0     -0.224256   2.810799  \n",
       "1      0.376680   0.560184  \n",
       "2      0.238902   4.121881  \n",
       "3      0.471005   0.422835  \n",
       "4      0.128247   0.779840  \n",
       "...         ...        ...  \n",
       "90621  0.194530   1.545559  \n",
       "90622 -0.142424  16.666942  \n",
       "90623  0.149952   1.759536  \n",
       "90624  0.143431   4.785374  \n",
       "90625  0.229081   2.130717  \n",
       "\n",
       "[90626 rows x 25 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real['Erreur']=np.abs(X_test_real['BOA_RT']-X_test_real['BOA_pred'])\n",
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>Erreur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.339585</td>\n",
       "      <td>0.069493</td>\n",
       "      <td>0.814930</td>\n",
       "      <td>0.215956</td>\n",
       "      <td>1.080031</td>\n",
       "      <td>0.261022</td>\n",
       "      <td>0.056369</td>\n",
       "      <td>38.372890</td>\n",
       "      <td>3885.035877</td>\n",
       "      <td>0.223047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784064</td>\n",
       "      <td>0.827994</td>\n",
       "      <td>0.182820</td>\n",
       "      <td>0.060016</td>\n",
       "      <td>0.094407</td>\n",
       "      <td>60.015525</td>\n",
       "      <td>94.407310</td>\n",
       "      <td>0.135579</td>\n",
       "      <td>0.851628</td>\n",
       "      <td>34.391785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.972872</td>\n",
       "      <td>0.993285</td>\n",
       "      <td>0.677149</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.020870</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>14.520411</td>\n",
       "      <td>6968.795702</td>\n",
       "      <td>0.306753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968069</td>\n",
       "      <td>0.476211</td>\n",
       "      <td>0.031684</td>\n",
       "      <td>0.985817</td>\n",
       "      <td>1.008013</td>\n",
       "      <td>985.817193</td>\n",
       "      <td>1008.012878</td>\n",
       "      <td>-4.675118</td>\n",
       "      <td>0.216218</td>\n",
       "      <td>22.195685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.788627</td>\n",
       "      <td>0.115739</td>\n",
       "      <td>0.969286</td>\n",
       "      <td>0.349916</td>\n",
       "      <td>0.237462</td>\n",
       "      <td>0.047987</td>\n",
       "      <td>0.016792</td>\n",
       "      <td>13.828972</td>\n",
       "      <td>113.301969</td>\n",
       "      <td>0.921797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971023</td>\n",
       "      <td>1.016925</td>\n",
       "      <td>0.973122</td>\n",
       "      <td>0.092290</td>\n",
       "      <td>0.113015</td>\n",
       "      <td>92.289962</td>\n",
       "      <td>113.015335</td>\n",
       "      <td>-0.170595</td>\n",
       "      <td>0.700542</td>\n",
       "      <td>20.725373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.954422</td>\n",
       "      <td>0.955509</td>\n",
       "      <td>0.987617</td>\n",
       "      <td>0.052284</td>\n",
       "      <td>0.046650</td>\n",
       "      <td>0.013148</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>86.528727</td>\n",
       "      <td>449.272884</td>\n",
       "      <td>0.508727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063028</td>\n",
       "      <td>13.490962</td>\n",
       "      <td>12.673768</td>\n",
       "      <td>0.435286</td>\n",
       "      <td>0.408940</td>\n",
       "      <td>435.286133</td>\n",
       "      <td>408.939880</td>\n",
       "      <td>0.205724</td>\n",
       "      <td>0.097541</td>\n",
       "      <td>26.346253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>0.983471</td>\n",
       "      <td>0.875427</td>\n",
       "      <td>0.982249</td>\n",
       "      <td>0.813108</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.095834</td>\n",
       "      <td>0.077924</td>\n",
       "      <td>78.652311</td>\n",
       "      <td>14.654955</td>\n",
       "      <td>0.076201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197526</td>\n",
       "      <td>4.988127</td>\n",
       "      <td>5.025667</td>\n",
       "      <td>0.434292</td>\n",
       "      <td>0.411813</td>\n",
       "      <td>434.292294</td>\n",
       "      <td>411.812744</td>\n",
       "      <td>0.668605</td>\n",
       "      <td>0.391756</td>\n",
       "      <td>22.479550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90363</th>\n",
       "      <td>0.981290</td>\n",
       "      <td>0.999815</td>\n",
       "      <td>0.979137</td>\n",
       "      <td>0.539137</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.045748</td>\n",
       "      <td>0.024664</td>\n",
       "      <td>89.208355</td>\n",
       "      <td>662.927875</td>\n",
       "      <td>0.313969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021212</td>\n",
       "      <td>27.043527</td>\n",
       "      <td>33.842834</td>\n",
       "      <td>0.486492</td>\n",
       "      <td>0.426726</td>\n",
       "      <td>486.491882</td>\n",
       "      <td>426.726379</td>\n",
       "      <td>0.180399</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>59.765503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90436</th>\n",
       "      <td>0.840182</td>\n",
       "      <td>0.964685</td>\n",
       "      <td>0.912374</td>\n",
       "      <td>0.929614</td>\n",
       "      <td>0.174137</td>\n",
       "      <td>1.302896</td>\n",
       "      <td>1.211190</td>\n",
       "      <td>35.339254</td>\n",
       "      <td>421.807833</td>\n",
       "      <td>0.364517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815806</td>\n",
       "      <td>1.169336</td>\n",
       "      <td>0.992702</td>\n",
       "      <td>0.625478</td>\n",
       "      <td>0.660142</td>\n",
       "      <td>625.477792</td>\n",
       "      <td>660.142273</td>\n",
       "      <td>0.539491</td>\n",
       "      <td>0.661084</td>\n",
       "      <td>34.664481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90471</th>\n",
       "      <td>0.934574</td>\n",
       "      <td>0.990226</td>\n",
       "      <td>0.962033</td>\n",
       "      <td>0.207586</td>\n",
       "      <td>0.067665</td>\n",
       "      <td>0.048846</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>87.813810</td>\n",
       "      <td>1305.229228</td>\n",
       "      <td>0.687272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041887</td>\n",
       "      <td>16.713342</td>\n",
       "      <td>12.430686</td>\n",
       "      <td>0.489099</td>\n",
       "      <td>0.464213</td>\n",
       "      <td>489.099111</td>\n",
       "      <td>464.212738</td>\n",
       "      <td>0.110732</td>\n",
       "      <td>0.061717</td>\n",
       "      <td>24.886373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90474</th>\n",
       "      <td>0.966291</td>\n",
       "      <td>0.775944</td>\n",
       "      <td>0.951105</td>\n",
       "      <td>0.894507</td>\n",
       "      <td>0.034291</td>\n",
       "      <td>0.475204</td>\n",
       "      <td>0.425074</td>\n",
       "      <td>84.213252</td>\n",
       "      <td>1124.497811</td>\n",
       "      <td>0.947106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102344</td>\n",
       "      <td>8.222727</td>\n",
       "      <td>5.568754</td>\n",
       "      <td>0.112914</td>\n",
       "      <td>0.133656</td>\n",
       "      <td>112.913699</td>\n",
       "      <td>133.655716</td>\n",
       "      <td>-0.277423</td>\n",
       "      <td>-0.156866</td>\n",
       "      <td>20.742017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90489</th>\n",
       "      <td>0.991717</td>\n",
       "      <td>0.154364</td>\n",
       "      <td>0.819846</td>\n",
       "      <td>0.702724</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>0.668195</td>\n",
       "      <td>0.469557</td>\n",
       "      <td>20.552189</td>\n",
       "      <td>2378.066404</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936373</td>\n",
       "      <td>0.819906</td>\n",
       "      <td>0.325207</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.222666</td>\n",
       "      <td>201.274096</td>\n",
       "      <td>222.666199</td>\n",
       "      <td>-1.561622</td>\n",
       "      <td>0.113742</td>\n",
       "      <td>21.392103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>960 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "158    0.339585  0.069493  0.814930  0.215956  1.080031  0.261022  0.056369   \n",
       "224    0.990893  0.972872  0.993285  0.677149  0.009149  0.020870  0.014132   \n",
       "255    0.788627  0.115739  0.969286  0.349916  0.237462  0.047987  0.016792   \n",
       "328    0.954422  0.955509  0.987617  0.052284  0.046650  0.013148  0.000687   \n",
       "523    0.983471  0.875427  0.982249  0.813108  0.016667  0.095834  0.077924   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90363  0.981290  0.999815  0.979137  0.539137  0.018887  0.045748  0.024664   \n",
       "90436  0.840182  0.964685  0.912374  0.929614  0.174137  1.302896  1.211190   \n",
       "90471  0.934574  0.990226  0.962033  0.207586  0.067665  0.048846  0.010140   \n",
       "90474  0.966291  0.775944  0.951105  0.894507  0.034291  0.475204  0.425074   \n",
       "90489  0.991717  0.154364  0.819846  0.702724  0.008318  0.668195  0.469557   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_a   mprime_g   mprime_a  \\\n",
       "158    38.372890  3885.035877  0.223047  ...   0.784064   0.827994   0.182820   \n",
       "224    14.520411  6968.795702  0.306753  ...   0.968069   0.476211   0.031684   \n",
       "255    13.828972   113.301969  0.921797  ...   0.971023   1.016925   0.973122   \n",
       "328    86.528727   449.272884  0.508727  ...   0.063028  13.490962  12.673768   \n",
       "523    78.652311    14.654955  0.076201  ...   0.197526   4.988127   5.025667   \n",
       "...          ...          ...       ...  ...        ...        ...        ...   \n",
       "90363  89.208355   662.927875  0.313969  ...   0.021212  27.043527  33.842834   \n",
       "90436  35.339254   421.807833  0.364517  ...   0.815806   1.169336   0.992702   \n",
       "90471  87.813810  1305.229228  0.687272  ...   0.041887  16.713342  12.430686   \n",
       "90474  84.213252  1124.497811  0.947106  ...   0.102344   8.222727   5.568754   \n",
       "90489  20.552189  2378.066404  0.972647  ...   0.936373   0.819906   0.325207   \n",
       "\n",
       "       BOA_fraction  BOA_fraction_pred      BOA_RT     BOA_pred  alpha_pred  \\\n",
       "158        0.060016           0.094407   60.015525    94.407310    0.135579   \n",
       "224        0.985817           1.008013  985.817193  1008.012878   -4.675118   \n",
       "255        0.092290           0.113015   92.289962   113.015335   -0.170595   \n",
       "328        0.435286           0.408940  435.286133   408.939880    0.205724   \n",
       "523        0.434292           0.411813  434.292294   411.812744    0.668605   \n",
       "...             ...                ...         ...          ...         ...   \n",
       "90363      0.486492           0.426726  486.491882   426.726379    0.180399   \n",
       "90436      0.625478           0.660142  625.477792   660.142273    0.539491   \n",
       "90471      0.489099           0.464213  489.099111   464.212738    0.110732   \n",
       "90474      0.112914           0.133656  112.913699   133.655716   -0.277423   \n",
       "90489      0.201274           0.222666  201.274096   222.666199   -1.561622   \n",
       "\n",
       "          alpha     Erreur  \n",
       "158    0.851628  34.391785  \n",
       "224    0.216218  22.195685  \n",
       "255    0.700542  20.725373  \n",
       "328    0.097541  26.346253  \n",
       "523    0.391756  22.479550  \n",
       "...         ...        ...  \n",
       "90363  0.002536  59.765503  \n",
       "90436  0.661084  34.664481  \n",
       "90471  0.061717  24.886373  \n",
       "90474 -0.156866  20.742017  \n",
       "90489  0.113742  21.392103  \n",
       "\n",
       "[960 rows x 25 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real[X_test_real['Erreur']>=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOA_RT contient Inf : False\n",
      "BOA_pred contient Inf : True\n",
      "BOA_RT contient NaN : False\n",
      "BOA_pred contient NaN : False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vérifie les infinis\n",
    "print(\"BOA_RT contient Inf :\", np.isinf(X_test_real['BOA_RT']).any())\n",
    "print(\"BOA_pred contient Inf :\", np.isinf(X_test_real['BOA_pred']).any())\n",
    "\n",
    "# Vérifie les NaNs\n",
    "print(\"BOA_RT contient NaN :\", X_test_real['BOA_RT'].isna().any())\n",
    "print(\"BOA_pred contient NaN :\", X_test_real['BOA_pred'].isna().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_BOA_IA(Tg_abs,Mg,Ta_abs,Ma,alpha,delta_g_scat,AODS):\n",
    "    numerator= 1000*(Tg_abs**Mg)*(Ta_abs**Ma)\n",
    "    denominator=1+alpha*delta_g_scat*Mg+(alpha*(1/3)*AODS)*Ma\n",
    "    \n",
    "    BOA_ia= numerator / denominator\n",
    "    return BOA_ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes avec Inf dans BOA_pred :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>0.877229</td>\n",
       "      <td>0.152220</td>\n",
       "      <td>0.831589</td>\n",
       "      <td>0.852392</td>\n",
       "      <td>0.130988</td>\n",
       "      <td>1.249368</td>\n",
       "      <td>1.064951</td>\n",
       "      <td>89.985541</td>\n",
       "      <td>2877.306578</td>\n",
       "      <td>0.876668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>0.012651</td>\n",
       "      <td>27.217166</td>\n",
       "      <td>18.752627</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.097828</td>\n",
       "      <td>-0.097828</td>\n",
       "      <td>inf</td>\n",
       "      <td>2.612085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8289</th>\n",
       "      <td>0.975981</td>\n",
       "      <td>0.272728</td>\n",
       "      <td>0.861939</td>\n",
       "      <td>0.748362</td>\n",
       "      <td>0.024312</td>\n",
       "      <td>0.590417</td>\n",
       "      <td>0.441845</td>\n",
       "      <td>89.423959</td>\n",
       "      <td>513.089170</td>\n",
       "      <td>0.777840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032060</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>29.462709</td>\n",
       "      <td>41.769395</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>-0.020888</td>\n",
       "      <td>-0.145599</td>\n",
       "      <td>-0.145599</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.808511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17860</th>\n",
       "      <td>0.956120</td>\n",
       "      <td>0.467668</td>\n",
       "      <td>0.636342</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.817985</td>\n",
       "      <td>0.365966</td>\n",
       "      <td>89.367327</td>\n",
       "      <td>222.750242</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032651</td>\n",
       "      <td>0.019210</td>\n",
       "      <td>29.878344</td>\n",
       "      <td>46.570483</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>-0.012042</td>\n",
       "      <td>-0.142414</td>\n",
       "      <td>-0.142414</td>\n",
       "      <td>inf</td>\n",
       "      <td>3.245207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28819</th>\n",
       "      <td>0.757926</td>\n",
       "      <td>0.091870</td>\n",
       "      <td>0.873886</td>\n",
       "      <td>0.772973</td>\n",
       "      <td>0.277170</td>\n",
       "      <td>0.593786</td>\n",
       "      <td>0.458981</td>\n",
       "      <td>89.614225</td>\n",
       "      <td>45.818359</td>\n",
       "      <td>0.384029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030144</td>\n",
       "      <td>0.016338</td>\n",
       "      <td>33.006107</td>\n",
       "      <td>59.821935</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>-0.014707</td>\n",
       "      <td>-0.054643</td>\n",
       "      <td>-0.054643</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.754105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>0.791410</td>\n",
       "      <td>0.491256</td>\n",
       "      <td>0.592520</td>\n",
       "      <td>0.434791</td>\n",
       "      <td>0.233939</td>\n",
       "      <td>0.925977</td>\n",
       "      <td>0.402607</td>\n",
       "      <td>89.669515</td>\n",
       "      <td>625.046765</td>\n",
       "      <td>0.175599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029604</td>\n",
       "      <td>0.015738</td>\n",
       "      <td>31.512932</td>\n",
       "      <td>46.486176</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>-0.073472</td>\n",
       "      <td>-0.073472</td>\n",
       "      <td>inf</td>\n",
       "      <td>15.495546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32822</th>\n",
       "      <td>0.862867</td>\n",
       "      <td>0.433229</td>\n",
       "      <td>0.858311</td>\n",
       "      <td>0.249329</td>\n",
       "      <td>0.147494</td>\n",
       "      <td>0.203537</td>\n",
       "      <td>0.050748</td>\n",
       "      <td>89.915112</td>\n",
       "      <td>62.657872</td>\n",
       "      <td>0.714013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027318</td>\n",
       "      <td>0.013290</td>\n",
       "      <td>36.352136</td>\n",
       "      <td>72.924195</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>-0.151623</td>\n",
       "      <td>-0.151623</td>\n",
       "      <td>inf</td>\n",
       "      <td>11.156664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34028</th>\n",
       "      <td>0.913785</td>\n",
       "      <td>0.233169</td>\n",
       "      <td>0.953676</td>\n",
       "      <td>0.632550</td>\n",
       "      <td>0.090160</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>0.081651</td>\n",
       "      <td>89.867727</td>\n",
       "      <td>177.226526</td>\n",
       "      <td>0.019382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027746</td>\n",
       "      <td>0.013734</td>\n",
       "      <td>35.339028</td>\n",
       "      <td>66.635415</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>-0.018445</td>\n",
       "      <td>-0.200009</td>\n",
       "      <td>-0.200009</td>\n",
       "      <td>inf</td>\n",
       "      <td>2.141626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35288</th>\n",
       "      <td>0.365234</td>\n",
       "      <td>0.202715</td>\n",
       "      <td>0.751287</td>\n",
       "      <td>0.651047</td>\n",
       "      <td>1.007217</td>\n",
       "      <td>0.819501</td>\n",
       "      <td>0.533533</td>\n",
       "      <td>89.757370</td>\n",
       "      <td>484.501151</td>\n",
       "      <td>0.540442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028766</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>32.940904</td>\n",
       "      <td>52.953818</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>-0.001282</td>\n",
       "      <td>-0.023476</td>\n",
       "      <td>-0.023476</td>\n",
       "      <td>inf</td>\n",
       "      <td>5.638409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50363</th>\n",
       "      <td>0.613289</td>\n",
       "      <td>0.312638</td>\n",
       "      <td>0.627556</td>\n",
       "      <td>0.499441</td>\n",
       "      <td>0.488918</td>\n",
       "      <td>0.930805</td>\n",
       "      <td>0.464883</td>\n",
       "      <td>88.766153</td>\n",
       "      <td>19.530294</td>\n",
       "      <td>0.299897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039422</td>\n",
       "      <td>0.027282</td>\n",
       "      <td>25.311487</td>\n",
       "      <td>36.297997</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>-0.055555</td>\n",
       "      <td>-0.055555</td>\n",
       "      <td>inf</td>\n",
       "      <td>6.617930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61655</th>\n",
       "      <td>0.969413</td>\n",
       "      <td>0.103506</td>\n",
       "      <td>0.966317</td>\n",
       "      <td>0.919750</td>\n",
       "      <td>0.031065</td>\n",
       "      <td>0.426964</td>\n",
       "      <td>0.392701</td>\n",
       "      <td>89.929943</td>\n",
       "      <td>241.982808</td>\n",
       "      <td>0.792710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027185</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>35.809323</td>\n",
       "      <td>67.362341</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.034433</td>\n",
       "      <td>-0.100703</td>\n",
       "      <td>-0.100703</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.163120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66383</th>\n",
       "      <td>0.611993</td>\n",
       "      <td>0.402104</td>\n",
       "      <td>0.791268</td>\n",
       "      <td>0.361549</td>\n",
       "      <td>0.491034</td>\n",
       "      <td>0.366698</td>\n",
       "      <td>0.132580</td>\n",
       "      <td>89.680157</td>\n",
       "      <td>250.576852</td>\n",
       "      <td>0.343685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029502</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>32.965224</td>\n",
       "      <td>56.463624</td>\n",
       "      <td>0.017477</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>-0.053526</td>\n",
       "      <td>-0.053526</td>\n",
       "      <td>inf</td>\n",
       "      <td>17.476563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67891</th>\n",
       "      <td>0.807391</td>\n",
       "      <td>0.219937</td>\n",
       "      <td>0.841507</td>\n",
       "      <td>0.724592</td>\n",
       "      <td>0.213947</td>\n",
       "      <td>0.626564</td>\n",
       "      <td>0.454003</td>\n",
       "      <td>89.622377</td>\n",
       "      <td>468.301969</td>\n",
       "      <td>0.790715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030063</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>31.577154</td>\n",
       "      <td>48.698200</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>-0.070794</td>\n",
       "      <td>-0.070794</td>\n",
       "      <td>inf</td>\n",
       "      <td>3.592250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "2379   0.877229  0.152220  0.831589  0.852392  0.130988  1.249368  1.064951   \n",
       "8289   0.975981  0.272728  0.861939  0.748362  0.024312  0.590417  0.441845   \n",
       "17860  0.956120  0.467668  0.636342  0.447400  0.044872  0.817985  0.365966   \n",
       "28819  0.757926  0.091870  0.873886  0.772973  0.277170  0.593786  0.458981   \n",
       "29336  0.791410  0.491256  0.592520  0.434791  0.233939  0.925977  0.402607   \n",
       "32822  0.862867  0.433229  0.858311  0.249329  0.147494  0.203537  0.050748   \n",
       "34028  0.913785  0.233169  0.953676  0.632550  0.090160  0.129082  0.081651   \n",
       "35288  0.365234  0.202715  0.751287  0.651047  1.007217  0.819501  0.533533   \n",
       "50363  0.613289  0.312638  0.627556  0.499441  0.488918  0.930805  0.464883   \n",
       "61655  0.969413  0.103506  0.966317  0.919750  0.031065  0.426964  0.392701   \n",
       "66383  0.611993  0.402104  0.791268  0.361549  0.491034  0.366698  0.132580   \n",
       "67891  0.807391  0.219937  0.841507  0.724592  0.213947  0.626564  0.454003   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_g  muprime_a   mprime_g  \\\n",
       "2379   89.985541  2877.306578  0.876668  ...   0.026688   0.012651  27.217166   \n",
       "8289   89.423959   513.089170  0.777840  ...   0.032060   0.018524  29.462709   \n",
       "17860  89.367327   222.750242  0.010789  ...   0.032651   0.019210  29.878344   \n",
       "28819  89.614225    45.818359  0.384029  ...   0.030144   0.016338  33.006107   \n",
       "29336  89.669515   625.046765  0.175599  ...   0.029604   0.015738  31.512932   \n",
       "32822  89.915112    62.657872  0.714013  ...   0.027318   0.013290  36.352136   \n",
       "34028  89.867727   177.226526  0.019382  ...   0.027746   0.013734  35.339028   \n",
       "35288  89.757370   484.501151  0.540442  ...   0.028766   0.014822  32.940904   \n",
       "50363  88.766153    19.530294  0.299897  ...   0.039422   0.027282  25.311487   \n",
       "61655  89.929943   241.982808  0.792710  ...   0.027185   0.013153  35.809323   \n",
       "66383  89.680157   250.576852  0.343685  ...   0.029502   0.015625  32.965224   \n",
       "67891  89.622377   468.301969  0.790715  ...   0.030063   0.016248  31.577154   \n",
       "\n",
       "        mprime_a  BOA_fraction  BOA_fraction_pred  alpha_pred     alpha  \\\n",
       "2379   18.752627      0.002612          -0.013425   -0.097828 -0.097828   \n",
       "8289   41.769395      0.000809          -0.020888   -0.145599 -0.145599   \n",
       "17860  46.570483      0.003245          -0.012042   -0.142414 -0.142414   \n",
       "28819  59.821935      0.000754          -0.014707   -0.054643 -0.054643   \n",
       "29336  46.486176      0.015496           0.023010   -0.073472 -0.073472   \n",
       "32822  72.924195      0.011157           0.011503   -0.151623 -0.151623   \n",
       "34028  66.635415      0.002142          -0.018445   -0.200009 -0.200009   \n",
       "35288  52.953818      0.005638          -0.001282   -0.023476 -0.023476   \n",
       "50363  36.297997      0.006618           0.000214   -0.055555 -0.055555   \n",
       "61655  67.362341      0.000163          -0.034433   -0.100703 -0.100703   \n",
       "66383  56.463624      0.017477           0.025441   -0.053526 -0.053526   \n",
       "67891  48.698200      0.003592          -0.001315   -0.070794 -0.070794   \n",
       "\n",
       "       BOA_pred     BOA_RT  \n",
       "2379        inf   2.612085  \n",
       "8289        inf   0.808511  \n",
       "17860       inf   3.245207  \n",
       "28819       inf   0.754105  \n",
       "29336       inf  15.495546  \n",
       "32822       inf  11.156664  \n",
       "34028       inf   2.141626  \n",
       "35288       inf   5.638409  \n",
       "50363       inf   6.617930  \n",
       "61655       inf   0.163120  \n",
       "66383       inf  17.476563  \n",
       "67891       inf   3.592250  \n",
       "\n",
       "[12 rows x 24 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infs = X_test_real[np.isinf(X_test_real['BOA_pred'])]\n",
    "print(\"Lignes avec Inf dans BOA_pred :\")\n",
    "infs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>alpha_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159680</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>3187.429121</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>0.896171</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>1.060831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347342</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.056472</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>0.601568</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.362287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307069</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.507886</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>0.290517</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.213682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126659</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>3187.535216</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>0.380374</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.405693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356896</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.812230</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>0.829379</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.139722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326002</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.288112</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>0.798049</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.243669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190999</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.551091</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>0.092493</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>-0.142425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27703</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.523112</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>0.732899</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.178618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.127582</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>0.906780</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.230817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189606</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.650486</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>0.729445</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.232249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "159680  0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "347342  0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "307069  0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "126659  0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "356896  0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "326002  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "190999  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "27703   0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "76624   0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "189606  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "              SZA            Z  R_scence  ...         mu_a  muprime_g  \\\n",
       "159680  17.879232  3858.242750  0.894296  ...  3187.429121   0.951776   \n",
       "347342  61.931220  1112.943981  0.419185  ...  3186.056472   0.471695   \n",
       "307069  76.068422    15.772549  0.729906  ...  3185.507886   0.243492   \n",
       "126659  63.055806  4070.432067  0.251342  ...  3187.535216   0.454355   \n",
       "356896   2.351437   624.459254  0.881197  ...  3185.812230   0.999159   \n",
       "...           ...          ...       ...  ...          ...        ...   \n",
       "326002  15.792506  1576.224940  0.696585  ...  3186.288112   0.962308   \n",
       "190999  88.522985  2102.182329  0.968884  ...  3186.551091   0.042399   \n",
       "27703   24.631558    46.223183  0.761757  ...  3185.523112   0.909141   \n",
       "76624   30.749744  1255.164902  0.559097  ...  3186.127582   0.859623   \n",
       "189606   6.366867   300.971201  0.195906  ...  3185.650486   0.993841   \n",
       "\n",
       "        muprime_a   mprime_g   mprime_a      BOA_RT     alpha  \\\n",
       "159680   0.951721   0.684362   0.152645  914.444755 -0.224256   \n",
       "347342   0.470791   1.873413   1.217586  594.575497  0.376680   \n",
       "307069   0.241375   4.099723   4.110381  284.591943  0.238902   \n",
       "126659   0.453397   1.400187   0.288163  373.718733  0.471005   \n",
       "356896   0.999158   0.933753   0.732430  832.137702  0.128247   \n",
       "...           ...        ...        ...         ...       ...   \n",
       "326002   0.962266   0.872219   0.472533  801.588284  0.194530   \n",
       "190999   0.030856  18.672395  11.328534   32.089238 -0.142424   \n",
       "27703    0.909037   1.094304   1.074933  735.627029  0.149952   \n",
       "76624    0.859456   1.011868   0.621184  924.379368  0.143431   \n",
       "189606   0.993834   0.973105   0.865627  730.440816  0.229081   \n",
       "\n",
       "        BOA_fraction_pred  BOA_fraction  alpha_pred  \n",
       "159680           0.896171      0.914445    1.060831  \n",
       "347342           0.601568      0.594575    0.362287  \n",
       "307069           0.290517      0.284592    0.213682  \n",
       "126659           0.380374      0.373719    0.405693  \n",
       "356896           0.829379      0.832138    0.139722  \n",
       "...                   ...           ...         ...  \n",
       "326002           0.798049      0.801588    0.243669  \n",
       "190999           0.092493      0.032089   -0.142425  \n",
       "27703            0.732899      0.735627    0.178618  \n",
       "76624            0.906780      0.924379    0.230817  \n",
       "189606           0.729445      0.730441    0.232249  \n",
       "\n",
       "[90626 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['alpha_pred']=X_test.apply(\n",
    "    lambda row: calculate_alpha(\n",
    "        row['BOA_fraction_pred'],\n",
    "        row['Ta_abs'],\n",
    "        row['Tg_abs'],\n",
    "        row['mprime_g'],\n",
    "        row['mprime_a'],\n",
    "        row['GOD'],\n",
    "        row['AODS'],\n",
    "    ), axis=1\n",
    ")\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>BOA_RT_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159680</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>0.896171</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>1.060831</td>\n",
       "      <td>896.171387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347342</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>0.601568</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.362287</td>\n",
       "      <td>601.568298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307069</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>0.290517</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.213682</td>\n",
       "      <td>290.516998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126659</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>0.380374</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.405693</td>\n",
       "      <td>380.373718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356896</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>0.829379</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.139722</td>\n",
       "      <td>829.379211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326002</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>0.798049</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.243669</td>\n",
       "      <td>798.048645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190999</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>0.092493</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>-0.142425</td>\n",
       "      <td>92.492775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27703</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>0.732899</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.178618</td>\n",
       "      <td>732.898804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>0.906780</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.230817</td>\n",
       "      <td>906.780029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189606</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>0.729445</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.232249</td>\n",
       "      <td>729.445312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "159680  0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "347342  0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "307069  0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "126659  0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "356896  0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "326002  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "190999  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "27703   0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "76624   0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "189606  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "              SZA            Z  R_scence  ...  muprime_g  muprime_a  \\\n",
       "159680  17.879232  3858.242750  0.894296  ...   0.951776   0.951721   \n",
       "347342  61.931220  1112.943981  0.419185  ...   0.471695   0.470791   \n",
       "307069  76.068422    15.772549  0.729906  ...   0.243492   0.241375   \n",
       "126659  63.055806  4070.432067  0.251342  ...   0.454355   0.453397   \n",
       "356896   2.351437   624.459254  0.881197  ...   0.999159   0.999158   \n",
       "...           ...          ...       ...  ...        ...        ...   \n",
       "326002  15.792506  1576.224940  0.696585  ...   0.962308   0.962266   \n",
       "190999  88.522985  2102.182329  0.968884  ...   0.042399   0.030856   \n",
       "27703   24.631558    46.223183  0.761757  ...   0.909141   0.909037   \n",
       "76624   30.749744  1255.164902  0.559097  ...   0.859623   0.859456   \n",
       "189606   6.366867   300.971201  0.195906  ...   0.993841   0.993834   \n",
       "\n",
       "         mprime_g   mprime_a      BOA_RT     alpha  BOA_fraction_pred  \\\n",
       "159680   0.684362   0.152645  914.444755 -0.224256           0.896171   \n",
       "347342   1.873413   1.217586  594.575497  0.376680           0.601568   \n",
       "307069   4.099723   4.110381  284.591943  0.238902           0.290517   \n",
       "126659   1.400187   0.288163  373.718733  0.471005           0.380374   \n",
       "356896   0.933753   0.732430  832.137702  0.128247           0.829379   \n",
       "...           ...        ...         ...       ...                ...   \n",
       "326002   0.872219   0.472533  801.588284  0.194530           0.798049   \n",
       "190999  18.672395  11.328534   32.089238 -0.142424           0.092493   \n",
       "27703    1.094304   1.074933  735.627029  0.149952           0.732899   \n",
       "76624    1.011868   0.621184  924.379368  0.143431           0.906780   \n",
       "189606   0.973105   0.865627  730.440816  0.229081           0.729445   \n",
       "\n",
       "        BOA_fraction  alpha_pred  BOA_RT_pred  \n",
       "159680      0.914445    1.060831   896.171387  \n",
       "347342      0.594575    0.362287   601.568298  \n",
       "307069      0.284592    0.213682   290.516998  \n",
       "126659      0.373719    0.405693   380.373718  \n",
       "356896      0.832138    0.139722   829.379211  \n",
       "...              ...         ...          ...  \n",
       "326002      0.801588    0.243669   798.048645  \n",
       "190999      0.032089   -0.142425    92.492775  \n",
       "27703       0.735627    0.178618   732.898804  \n",
       "76624       0.924379    0.230817   906.780029  \n",
       "189606      0.730441    0.232249   729.445312  \n",
       "\n",
       "[90626 rows x 24 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['BOA_RT_pred'] = X_test['BOA_fraction_pred'] * 1000\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.739169428433469"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mae = mean_absolute_error(X_test['BOA_RT'], X_test['BOA_RT_pred'])\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class TransmissionLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        alpha, m_g, m_a, dtau_gas_scat, dtau_aero_scat, T_gas_abs, T_aero_abs = inputs\n",
    "\n",
    "        # Dénominateur (1 + α⋅Δτ_gas⋅m_g + α/3⋅Δτ_aero⋅m_a)\n",
    "        denom = 1 + alpha * dtau_gas_scat * m_g + (alpha / 3.0) * dtau_aero_scat * m_a\n",
    "\n",
    "        # Absorptions\n",
    "        T_abs = tf.pow(T_gas_abs, m_g) * tf.pow(T_aero_abs, m_a)\n",
    "\n",
    "        # E_BOA / E_TOA\n",
    "        E_BOA_div_TOA = (T_abs/ denom) \n",
    "        return E_BOA_div_TOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Entrée principale : 18 descripteurs\n",
    "features_input = Input(shape=(18,), name='features')\n",
    "\n",
    "# Autres entrées nécessaires à la formule analytique\n",
    "m_g_input = Input(shape=(1,), name='mprime_g')\n",
    "m_a_input = Input(shape=(1,), name='mprime_a')\n",
    "dtau_gas_scat_input = Input(shape=(1,), name='GOD')\n",
    "dtau_aero_scat_input = Input(shape=(1,), name='AODS')\n",
    "T_gas_abs_input = Input(shape=(1,), name='Tg_abs')\n",
    "T_aero_abs_input = Input(shape=(1,), name='Ta_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(232, activation='elu')(features_input)\n",
    "x = Dropout(0.111)(x)\n",
    "x = Dense(249, activation='elu')(x)\n",
    "x = Dropout(0.138)(x)\n",
    "x = Dense(174, activation='elu')(x)\n",
    "x = Dropout(0.136)(x)\n",
    "\n",
    "# Prédiction de alpha\n",
    "alpha_output = Dense(1, name='alpha')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_BOA_div_TOA = TransmissionLayer()([\n",
    "    alpha_output,\n",
    "    m_g_input,\n",
    "    m_a_input,\n",
    "    dtau_gas_scat_input,\n",
    "    dtau_aero_scat_input,\n",
    "    T_gas_abs_input,\n",
    "    T_aero_abs_input\n",
    "])\n",
    "model = Model(\n",
    "    inputs=[features_input, m_g_input, m_a_input, dtau_gas_scat_input, dtau_aero_scat_input, T_gas_abs_input, T_aero_abs_input],\n",
    "    outputs=E_BOA_div_TOA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00034),\n",
    "    loss='mae',  # ou 'mae' selon ce que tu veux\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1676 - mae: 0.1676\n",
      "Epoch 1: val_mae improved from inf to 0.04998, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.1675 - mae: 0.1675 - val_loss: 0.0500 - val_mae: 0.0500 - learning_rate: 3.4066e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0778 - mae: 0.0778\n",
      "Epoch 2: val_mae improved from 0.04998 to 0.04223, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0778 - mae: 0.0778 - val_loss: 0.0422 - val_mae: 0.0422 - learning_rate: 3.4066e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0638 - mae: 0.0638\n",
      "Epoch 3: val_mae improved from 0.04223 to 0.02865, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0638 - mae: 0.0638 - val_loss: 0.0287 - val_mae: 0.0287 - learning_rate: 3.4066e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0596 - mae: 0.0596\n",
      "Epoch 4: val_mae did not improve from 0.02865\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0596 - mae: 0.0596 - val_loss: 0.0325 - val_mae: 0.0325 - learning_rate: 3.4066e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m3390/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0574 - mae: 0.0574\n",
      "Epoch 5: val_mae did not improve from 0.02865\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0574 - mae: 0.0574 - val_loss: 0.0338 - val_mae: 0.0338 - learning_rate: 3.4066e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0551 - mae: 0.0551\n",
      "Epoch 6: val_mae did not improve from 0.02865\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0551 - mae: 0.0551 - val_loss: 0.0382 - val_mae: 0.0382 - learning_rate: 3.4066e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0546 - mae: 0.0546\n",
      "Epoch 7: val_mae improved from 0.02865 to 0.02631, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0546 - mae: 0.0546 - val_loss: 0.0263 - val_mae: 0.0263 - learning_rate: 3.4066e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0530 - mae: 0.0530\n",
      "Epoch 8: val_mae improved from 0.02631 to 0.02561, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0530 - mae: 0.0530 - val_loss: 0.0256 - val_mae: 0.0256 - learning_rate: 3.4066e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0524 - mae: 0.0524\n",
      "Epoch 9: val_mae improved from 0.02561 to 0.02558, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0524 - mae: 0.0524 - val_loss: 0.0256 - val_mae: 0.0256 - learning_rate: 3.4066e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0513 - mae: 0.0513\n",
      "Epoch 10: val_mae did not improve from 0.02558\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0513 - mae: 0.0513 - val_loss: 0.0276 - val_mae: 0.0276 - learning_rate: 3.4066e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0505 - mae: 0.0505\n",
      "Epoch 11: val_mae did not improve from 0.02558\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0505 - mae: 0.0505 - val_loss: 0.0310 - val_mae: 0.0310 - learning_rate: 3.4066e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0505 - mae: 0.0505\n",
      "Epoch 12: val_mae improved from 0.02558 to 0.02147, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 0.0505 - mae: 0.0505 - val_loss: 0.0215 - val_mae: 0.0215 - learning_rate: 3.4066e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m   9/3399\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 0.0468 - mae: 0.0468  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     28\u001b[39m csv_logger = CSVLogger(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtraining_log_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Train phase :\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_mae',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Model checkpoint (save best model)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='deep_model.keras',\n",
    "    monitor='val_mae',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. CSV logger (log training history)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_logger = CSVLogger(f'training_log_{timestamp}.csv')\n",
    "\n",
    "# Train phase :\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_cb, csv_logger],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = data.iloc[:, :18].values\n",
    "\n",
    "# autres colonnes utilisées dans la couche analytique\n",
    "X_inputs = {\n",
    "    'features': X_features,\n",
    "    'mprime_g': data['mprime_g'].values.reshape(-1, 1),\n",
    "    'mprime_a': data['mprime_a'].values.reshape(-1, 1),\n",
    "    'GOD': data['GOD'].values.reshape(-1, 1),\n",
    "    'AODS': data['AODS'].values.reshape(-1, 1),\n",
    "    'Tg_abs': data['Tg_abs'].values.reshape(-1, 1),\n",
    "    'Ta_abs': data['Ta_abs'].values.reshape(-1, 1),\n",
    "}\n",
    "\n",
    "# y = valeur cible (E_BOA / E_TOA) → entre 0 et 1\n",
    "y = data['BOA_fraction'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_mae',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Model checkpoint (save best model)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='deep_model_1.keras',\n",
    "    monitor='val_mae',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. CSV logger (log training history)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_logger = CSVLogger(f'training_log_{timestamp}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4524/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5624 - mae: 0.5624\n",
      "Epoch 1: val_mae improved from inf to 0.55872, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5624 - mae: 0.5624 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m4523/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5572 - mae: 0.5572\n",
      "Epoch 2: val_mae improved from 0.55872 to 0.55869, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5572 - mae: 0.5572 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m4521/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5579 - mae: 0.5579\n",
      "Epoch 3: val_mae improved from 0.55869 to 0.55869, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5579 - mae: 0.5579 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m4525/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5572 - mae: 0.5572\n",
      "Epoch 4: val_mae improved from 0.55869 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5572 - mae: 0.5572 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5584 - mae: 0.5584\n",
      "Epoch 5: val_mae improved from 0.55868 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5584 - mae: 0.5584 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m4531/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5585 - mae: 0.5585\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00016999999934341758.\n",
      "\n",
      "Epoch 6: val_mae improved from 0.55868 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5585 - mae: 0.5585 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m4523/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5596 - mae: 0.5596\n",
      "Epoch 7: val_mae improved from 0.55868 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5596 - mae: 0.5596 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m4529/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 8: val_mae improved from 0.55868 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 9: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m4530/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 10: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m4528/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5574 - mae: 0.5574\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 8.499999967170879e-05.\n",
      "\n",
      "Epoch 11: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5575 - mae: 0.5575 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5578 - mae: 0.5578\n",
      "Epoch 12: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5578 - mae: 0.5578 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 13: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5575 - mae: 0.5575\n",
      "Epoch 14: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5575 - mae: 0.5575 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5576 - mae: 0.5576\n",
      "Epoch 15: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.5576 - mae: 0.5576 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m4524/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5574 - mae: 0.5574\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.2499999835854396e-05.\n",
      "\n",
      "Epoch 16: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5574 - mae: 0.5574 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5580 - mae: 0.5580\n",
      "Epoch 17: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5580 - mae: 0.5580 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5574 - mae: 0.5574\n",
      "Epoch 18: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5574 - mae: 0.5574 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 19: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m4531/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5580 - mae: 0.5580\n",
      "Epoch 20: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5580 - mae: 0.5580 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5580 - mae: 0.5580\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 2.1249999917927198e-05.\n",
      "\n",
      "Epoch 21: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5580 - mae: 0.5580 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5585 - mae: 0.5585\n",
      "Epoch 22: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5585 - mae: 0.5585 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 2.1250e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m4529/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5571 - mae: 0.5571\n",
      "Epoch 23: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.5571 - mae: 0.5571 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 2.1250e-05\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7bd752273710>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_inputs,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_cb, csv_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_22\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_22\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,872</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,017</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">43,500</span> │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_8       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                     │                   │            │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)       │      \u001b[38;5;34m4,872\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)       │     \u001b[38;5;34m58,017\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │     \u001b[38;5;34m43,500\u001b[0m │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_8       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                     │                   │            │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_8[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,389</span> (415.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,389\u001b[0m (415.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,389</span> (415.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,389\u001b[0m (415.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Lambda\n",
    "\n",
    "# Chargement des paramètres optimaux\n",
    "with open(\"best_params.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Fonction d'activation personnalisée qui ne reçoit les 6 paramètres qu'en sortie\n",
    "def transmission_activation(inputs):\n",
    "    alpha, m_g, m_a, dtau_gas_scat, dtau_aero_scat, T_gas_abs, T_aero_abs = inputs  # Décomposer la liste d'inputs\n",
    "    \n",
    "    # Dénominateur\n",
    "    denom = 1 + alpha * dtau_gas_scat * m_g + (alpha / 3.0) * dtau_aero_scat * m_a\n",
    "    \n",
    "    # Absorption\n",
    "    T_abs = tf.pow(T_gas_abs, m_g) * tf.pow(T_aero_abs, m_a)\n",
    "    \n",
    "    # Transmission finale\n",
    "    return T_abs / denom\n",
    "\n",
    "# Définition des entrées complètes\n",
    "input_features = Input(shape=(X_train.shape[1],))  # Inclut toutes les colonnes pour l'entraînement\n",
    "\n",
    "# Couches cachées\n",
    "hidden = Dense(params[\"units1\"], activation=params[\"activation\"])(input_features)\n",
    "hidden = Dropout(params[\"dropout1\"])(hidden)\n",
    "\n",
    "hidden = Dense(params[\"units2\"], activation=params[\"activation\"])(hidden)\n",
    "hidden = Dropout(params[\"dropout2\"])(hidden)\n",
    "\n",
    "if params[\"n_layers\"] >= 3:\n",
    "    hidden = Dense(params[\"units3\"], activation=params[\"activation\"])(hidden)\n",
    "    hidden = Dropout(params[\"dropout3\"])(hidden)\n",
    "\n",
    "if params[\"n_layers\"] == 4:\n",
    "    hidden = Dense(params[\"units4\"], activation=params[\"activation\"])(hidden)\n",
    "    hidden = Dropout(params[\"dropout4\"])(hidden)\n",
    "\n",
    "# Transmission directe de la sortie avant activation\n",
    "alpha = Lambda(lambda x: x)(hidden)  # Passage direct sans transformation\n",
    "\n",
    "# Entrées des 6 paramètres après entraînement\n",
    "input_m_g = Input(shape=(1,))\n",
    "input_m_a = Input(shape=(1,))\n",
    "input_dtau_gas_scat = Input(shape=(1,))\n",
    "input_dtau_aero_scat = Input(shape=(1,))\n",
    "input_T_gas_abs = Input(shape=(1,))\n",
    "input_T_aero_abs = Input(shape=(1,))\n",
    "\n",
    "# Application de la fonction analytique sur la sortie après entraînement\n",
    "# Application de la fonction analytique sur la sortie\n",
    "output = Lambda(lambda inputs: transmission_activation(inputs), output_shape=(None, 1))(\n",
    "    [alpha, input_m_g, input_m_a, input_dtau_gas_scat, input_dtau_aero_scat, input_T_gas_abs, input_T_aero_abs]\n",
    ")\n",
    "\n",
    "# Définition du modèle final\n",
    "model = Model(inputs=[input_features, input_m_g, input_m_a, input_dtau_gas_scat, input_dtau_aero_scat, input_T_gas_abs, input_T_aero_abs], outputs=output)\n",
    "\n",
    "# Compilation du modèle\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n",
    "\n",
    "# Affichage du résumé du modèle\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ features            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,872</span> │ features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,017</span> │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">43,500</span> │ dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ alpha (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">175</span> │ dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ m_g (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ m_a (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dtau_gas_scat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dtau_aero_scat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ T_gas_abs           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ T_aero_abs          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ alpha[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ m_g[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                     │                   │            │ m_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                     │                   │            │ dtau_gas_scat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ dtau_aero_scat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ T_gas_abs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ T_aero_abs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ features            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)       │      \u001b[38;5;34m4,872\u001b[0m │ features[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)       │     \u001b[38;5;34m58,017\u001b[0m │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │     \u001b[38;5;34m43,500\u001b[0m │ dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ alpha (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m175\u001b[0m │ dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ m_g (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ m_a (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dtau_gas_scat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dtau_aero_scat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ T_gas_abs           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ T_aero_abs          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ alpha[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ m_g[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                     │                   │            │ m_a[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                     │                   │            │ dtau_gas_scat[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ dtau_aero_scat[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ T_gas_abs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ T_aero_abs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,564</span> (416.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,564\u001b[0m (416.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,564</span> (416.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,564\u001b[0m (416.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Lambda\n",
    "\n",
    "# Chargement des meilleurs hyperparamètres\n",
    "with open(\"best_params.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# ----------------------------\n",
    "# Fonction de transmission personnalisée\n",
    "# ----------------------------\n",
    "def transmission_activation(inputs):\n",
    "    alpha, m_g, m_a, dtau_gas_scat, dtau_aero_scat, T_gas_abs, T_aero_abs = inputs\n",
    "    \n",
    "    denom = 1 + alpha * dtau_gas_scat * m_g + (alpha / 3.0) * dtau_aero_scat * m_a\n",
    "    T_abs = tf.pow(T_gas_abs, m_g) * tf.pow(T_aero_abs, m_a)\n",
    "    \n",
    "    return T_abs / denom\n",
    "\n",
    "# ----------------------------\n",
    "# Définition des entrées\n",
    "# ----------------------------\n",
    "input_features = Input(shape=(X_train.shape[1],), name=\"features\")\n",
    "\n",
    "input_m_g = Input(shape=(1,), name=\"m_g\")\n",
    "input_m_a = Input(shape=(1,), name=\"m_a\")\n",
    "input_dtau_gas_scat = Input(shape=(1,), name=\"dtau_gas_scat\")\n",
    "input_dtau_aero_scat = Input(shape=(1,), name=\"dtau_aero_scat\")\n",
    "input_T_gas_abs = Input(shape=(1,), name=\"T_gas_abs\")\n",
    "input_T_aero_abs = Input(shape=(1,), name=\"T_aero_abs\")\n",
    "\n",
    "# ----------------------------\n",
    "# Réseau de neurones MLP\n",
    "# ----------------------------\n",
    "x = Dense(params[\"units1\"], activation=params[\"activation\"])(input_features)\n",
    "x = Dropout(params[\"dropout1\"])(x)\n",
    "\n",
    "x = Dense(params[\"units2\"], activation=params[\"activation\"])(x)\n",
    "x = Dropout(params[\"dropout2\"])(x)\n",
    "\n",
    "if params[\"n_layers\"] >= 3:\n",
    "    x = Dense(params[\"units3\"], activation=params[\"activation\"])(x)\n",
    "    x = Dropout(params[\"dropout3\"])(x)\n",
    "\n",
    "if params[\"n_layers\"] == 4:\n",
    "    x = Dense(params[\"units4\"], activation=params[\"activation\"])(x)\n",
    "    x = Dropout(params[\"dropout4\"])(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Dernier neurone = alpha\n",
    "# ----------------------------\n",
    "alpha = Dense(1, name=\"alpha\")(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Couche de sortie : transmission personnalisée\n",
    "# ----------------------------\n",
    "output = Lambda(lambda inputs: transmission_activation(inputs), output_shape=(1,))(\n",
    "    [alpha, input_m_g, input_m_a, input_dtau_gas_scat, input_dtau_aero_scat, input_T_gas_abs, input_T_aero_abs]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Modèle final\n",
    "# ----------------------------\n",
    "model = Model(\n",
    "    inputs=[\n",
    "        input_features,\n",
    "        input_m_g,\n",
    "        input_m_a,\n",
    "        input_dtau_gas_scat,\n",
    "        input_dtau_aero_scat,\n",
    "        input_T_gas_abs,\n",
    "        input_T_aero_abs\n",
    "    ],\n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "# Compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n",
    "\n",
    "# Résumé\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362503,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "X_train=data.drop(columns=['BOA_fraction'])\n",
    "y_train=data['BOA_fraction']\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m_g = X_train[\"mprime_g\"].values.reshape(-1, 1)\n",
    "X_train_m_a = X_train[\"mprime_a\"].values.reshape(-1, 1)\n",
    "X_train_dtau_gas_scat = X_train[\"GOD\"].values.reshape(-1, 1)\n",
    "X_train_dtau_aero_scat = X_train[\"AODS\"].values.reshape(-1, 1)\n",
    "X_train_T_gas_abs = X_train[\"Tg_abs\"].values.reshape(-1, 1)\n",
    "X_train_T_aero_abs = X_train[\"Ta_abs\"].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11329/11329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 4ms/step - loss: 0.5633 - mae: 0.5633\n",
      "Epoch 2/100\n",
      "\u001b[1m11329/11329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 4ms/step - loss: 0.5579 - mae: 0.5579\n",
      "Epoch 3/100\n",
      "\u001b[1m11329/11329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 4ms/step - loss: 0.5578 - mae: 0.5578\n",
      "Epoch 4/100\n",
      "\u001b[1m11329/11329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 4ms/step - loss: 0.5583 - mae: 0.5583\n",
      "Epoch 5/100\n",
      "\u001b[1m 4660/11329\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 4ms/step - loss: 0.5584 - mae: 0.5584"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_m_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_m_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_dtau_gas_scat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_dtau_aero_scat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_T_gas_abs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_T_aero_abs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train, X_train_m_g, X_train_m_a, X_train_dtau_gas_scat, X_train_dtau_aero_scat, X_train_T_gas_abs, X_train_T_aero_abs], \n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myptd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
