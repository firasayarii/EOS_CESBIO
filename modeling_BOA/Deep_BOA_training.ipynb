{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:55:56.741400: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-06 10:55:57.021095: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-06 10:55:57.278648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749200157.536308   10357 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749200157.628980   10357 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749200158.127578   10357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749200158.127611   10357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749200158.127615   10357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749200158.127619   10357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-06 10:55:58.208499: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler , OneHotEncoder\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>g1</th>\n",
       "      <th>Cos(SZA)</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984234</td>\n",
       "      <td>0.967931</td>\n",
       "      <td>0.849827</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.402452</td>\n",
       "      <td>0.239730</td>\n",
       "      <td>81.357114</td>\n",
       "      <td>1137.064781</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>0.946187</td>\n",
       "      <td>0.150275</td>\n",
       "      <td>708.015229</td>\n",
       "      <td>3186.068532</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.151289</td>\n",
       "      <td>5.695816</td>\n",
       "      <td>3.743539</td>\n",
       "      <td>429.332315</td>\n",
       "      <td>0.133511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.808703</td>\n",
       "      <td>0.502863</td>\n",
       "      <td>0.971936</td>\n",
       "      <td>0.754977</td>\n",
       "      <td>0.212324</td>\n",
       "      <td>0.116173</td>\n",
       "      <td>0.087708</td>\n",
       "      <td>69.669734</td>\n",
       "      <td>2695.217813</td>\n",
       "      <td>0.364243</td>\n",
       "      <td>0.910936</td>\n",
       "      <td>0.347431</td>\n",
       "      <td>708.188358</td>\n",
       "      <td>3186.847609</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.347828</td>\n",
       "      <td>2.122562</td>\n",
       "      <td>0.747097</td>\n",
       "      <td>194.210750</td>\n",
       "      <td>0.363279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266592</td>\n",
       "      <td>0.973465</td>\n",
       "      <td>0.845395</td>\n",
       "      <td>0.580608</td>\n",
       "      <td>1.322036</td>\n",
       "      <td>0.400463</td>\n",
       "      <td>0.232512</td>\n",
       "      <td>17.550390</td>\n",
       "      <td>1081.897319</td>\n",
       "      <td>0.672173</td>\n",
       "      <td>0.896924</td>\n",
       "      <td>0.953452</td>\n",
       "      <td>708.009100</td>\n",
       "      <td>3186.040949</td>\n",
       "      <td>0.953519</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>0.929959</td>\n",
       "      <td>0.610609</td>\n",
       "      <td>648.404343</td>\n",
       "      <td>0.280041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.830782</td>\n",
       "      <td>0.963485</td>\n",
       "      <td>0.971736</td>\n",
       "      <td>0.270474</td>\n",
       "      <td>0.185388</td>\n",
       "      <td>0.039302</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>44.256621</td>\n",
       "      <td>81.149965</td>\n",
       "      <td>0.775990</td>\n",
       "      <td>0.869940</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>707.897906</td>\n",
       "      <td>3185.540575</td>\n",
       "      <td>0.716701</td>\n",
       "      <td>0.716328</td>\n",
       "      <td>1.382759</td>\n",
       "      <td>1.340499</td>\n",
       "      <td>885.138727</td>\n",
       "      <td>0.125099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600291</td>\n",
       "      <td>0.839312</td>\n",
       "      <td>0.797427</td>\n",
       "      <td>0.652968</td>\n",
       "      <td>0.510340</td>\n",
       "      <td>0.652287</td>\n",
       "      <td>0.425923</td>\n",
       "      <td>24.480144</td>\n",
       "      <td>932.892398</td>\n",
       "      <td>0.777209</td>\n",
       "      <td>0.878946</td>\n",
       "      <td>0.910105</td>\n",
       "      <td>707.992544</td>\n",
       "      <td>3185.966446</td>\n",
       "      <td>0.910238</td>\n",
       "      <td>0.910135</td>\n",
       "      <td>0.990440</td>\n",
       "      <td>0.689159</td>\n",
       "      <td>602.697524</td>\n",
       "      <td>0.320636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362498</th>\n",
       "      <td>0.765135</td>\n",
       "      <td>0.641704</td>\n",
       "      <td>0.963197</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.267702</td>\n",
       "      <td>0.214810</td>\n",
       "      <td>0.177313</td>\n",
       "      <td>22.962506</td>\n",
       "      <td>183.428535</td>\n",
       "      <td>0.740243</td>\n",
       "      <td>0.668902</td>\n",
       "      <td>0.920760</td>\n",
       "      <td>707.909270</td>\n",
       "      <td>3185.591714</td>\n",
       "      <td>0.920877</td>\n",
       "      <td>0.920786</td>\n",
       "      <td>1.064013</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>534.140819</td>\n",
       "      <td>0.364438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362499</th>\n",
       "      <td>0.749872</td>\n",
       "      <td>0.800834</td>\n",
       "      <td>0.842750</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.287853</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.437803</td>\n",
       "      <td>55.826486</td>\n",
       "      <td>452.874619</td>\n",
       "      <td>0.016449</td>\n",
       "      <td>0.818049</td>\n",
       "      <td>0.561701</td>\n",
       "      <td>707.939208</td>\n",
       "      <td>3185.726437</td>\n",
       "      <td>0.562559</td>\n",
       "      <td>0.561892</td>\n",
       "      <td>1.690357</td>\n",
       "      <td>1.419079</td>\n",
       "      <td>387.264753</td>\n",
       "      <td>0.564494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362500</th>\n",
       "      <td>0.538644</td>\n",
       "      <td>0.788402</td>\n",
       "      <td>0.827843</td>\n",
       "      <td>0.727779</td>\n",
       "      <td>0.618701</td>\n",
       "      <td>0.694040</td>\n",
       "      <td>0.505108</td>\n",
       "      <td>6.365350</td>\n",
       "      <td>423.130196</td>\n",
       "      <td>0.050525</td>\n",
       "      <td>0.896188</td>\n",
       "      <td>0.993835</td>\n",
       "      <td>707.935903</td>\n",
       "      <td>3185.711565</td>\n",
       "      <td>0.993844</td>\n",
       "      <td>0.993837</td>\n",
       "      <td>0.959983</td>\n",
       "      <td>0.814335</td>\n",
       "      <td>476.450471</td>\n",
       "      <td>0.591383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362501</th>\n",
       "      <td>0.634018</td>\n",
       "      <td>0.991370</td>\n",
       "      <td>0.965507</td>\n",
       "      <td>0.661306</td>\n",
       "      <td>0.455678</td>\n",
       "      <td>0.103640</td>\n",
       "      <td>0.068538</td>\n",
       "      <td>7.738936</td>\n",
       "      <td>2006.981168</td>\n",
       "      <td>0.832208</td>\n",
       "      <td>0.860886</td>\n",
       "      <td>0.990892</td>\n",
       "      <td>708.111887</td>\n",
       "      <td>3186.503491</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.990895</td>\n",
       "      <td>0.807461</td>\n",
       "      <td>0.369966</td>\n",
       "      <td>1015.811664</td>\n",
       "      <td>-0.093105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362502</th>\n",
       "      <td>0.323002</td>\n",
       "      <td>0.940875</td>\n",
       "      <td>0.665318</td>\n",
       "      <td>0.306548</td>\n",
       "      <td>1.130098</td>\n",
       "      <td>0.587625</td>\n",
       "      <td>0.180135</td>\n",
       "      <td>21.099784</td>\n",
       "      <td>354.152329</td>\n",
       "      <td>0.982156</td>\n",
       "      <td>0.832794</td>\n",
       "      <td>0.932955</td>\n",
       "      <td>707.928239</td>\n",
       "      <td>3185.677076</td>\n",
       "      <td>0.933053</td>\n",
       "      <td>0.932977</td>\n",
       "      <td>1.030396</td>\n",
       "      <td>0.897896</td>\n",
       "      <td>416.207341</td>\n",
       "      <td>0.463743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362503 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0       0.984234  0.967931  0.849827  0.595673  0.015892  0.402452  0.239730   \n",
       "1       0.808703  0.502863  0.971936  0.754977  0.212324  0.116173  0.087708   \n",
       "2       0.266592  0.973465  0.845395  0.580608  1.322036  0.400463  0.232512   \n",
       "3       0.830782  0.963485  0.971736  0.270474  0.185388  0.039302  0.010630   \n",
       "4       0.600291  0.839312  0.797427  0.652968  0.510340  0.652287  0.425923   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "362498  0.765135  0.641704  0.963197  0.825442  0.267702  0.214810  0.177313   \n",
       "362499  0.749872  0.800834  0.842750  0.719020  0.287853  0.608888  0.437803   \n",
       "362500  0.538644  0.788402  0.827843  0.727779  0.618701  0.694040  0.505108   \n",
       "362501  0.634018  0.991370  0.965507  0.661306  0.455678  0.103640  0.068538   \n",
       "362502  0.323002  0.940875  0.665318  0.306548  1.130098  0.587625  0.180135   \n",
       "\n",
       "              SZA            Z  R_scence        g1  Cos(SZA)        mu_g  \\\n",
       "0       81.357114  1137.064781  0.370253  0.946187  0.150275  708.015229   \n",
       "1       69.669734  2695.217813  0.364243  0.910936  0.347431  708.188358   \n",
       "2       17.550390  1081.897319  0.672173  0.896924  0.953452  708.009100   \n",
       "3       44.256621    81.149965  0.775990  0.869940  0.716221  707.897906   \n",
       "4       24.480144   932.892398  0.777209  0.878946  0.910105  707.992544   \n",
       "...           ...          ...       ...       ...       ...         ...   \n",
       "362498  22.962506   183.428535  0.740243  0.668902  0.920760  707.909270   \n",
       "362499  55.826486   452.874619  0.016449  0.818049  0.561701  707.939208   \n",
       "362500   6.365350   423.130196  0.050525  0.896188  0.993835  707.935903   \n",
       "362501   7.738936  2006.981168  0.832208  0.860886  0.990892  708.111887   \n",
       "362502  21.099784   354.152329  0.982156  0.832794  0.932955  707.928239   \n",
       "\n",
       "               mu_a  muprime_g  muprime_a  mprime_g  mprime_a       BOA_RT  \\\n",
       "0       3186.068532   0.154730   0.151289  5.695816  3.743539   429.332315   \n",
       "1       3186.847609   0.349206   0.347828  2.122562  0.747097   194.210750   \n",
       "2       3186.040949   0.953519   0.953467  0.929959  0.610609   648.404343   \n",
       "3       3185.540575   0.716701   0.716328  1.382759  1.340499   885.138727   \n",
       "4       3185.966446   0.910238   0.910135  0.990440  0.689159   602.697524   \n",
       "...             ...        ...        ...       ...       ...          ...   \n",
       "362498  3185.591714   0.920877   0.920786  1.064013  0.990855   534.140819   \n",
       "362499  3185.726437   0.562559   0.561892  1.690357  1.419079   387.264753   \n",
       "362500  3185.711565   0.993844   0.993837  0.959983  0.814335   476.450471   \n",
       "362501  3186.503491   0.990905   0.990895  0.807461  0.369966  1015.811664   \n",
       "362502  3185.677076   0.933053   0.932977  1.030396  0.897896   416.207341   \n",
       "\n",
       "           alpha  \n",
       "0       0.133511  \n",
       "1       0.363279  \n",
       "2       0.280041  \n",
       "3       0.125099  \n",
       "4       0.320636  \n",
       "...          ...  \n",
       "362498  0.364438  \n",
       "362499  0.564494  \n",
       "362500  0.591383  \n",
       "362501 -0.093105  \n",
       "362502  0.463743  \n",
       "\n",
       "[362503 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('no_outliers.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>Cos(SZA)</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984234</td>\n",
       "      <td>0.967931</td>\n",
       "      <td>0.849827</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.402452</td>\n",
       "      <td>0.239730</td>\n",
       "      <td>81.357114</td>\n",
       "      <td>1137.064781</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150275</td>\n",
       "      <td>708.015229</td>\n",
       "      <td>3186.068532</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.151289</td>\n",
       "      <td>5.695816</td>\n",
       "      <td>3.743539</td>\n",
       "      <td>429.332315</td>\n",
       "      <td>0.133511</td>\n",
       "      <td>0.429332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.808703</td>\n",
       "      <td>0.502863</td>\n",
       "      <td>0.971936</td>\n",
       "      <td>0.754977</td>\n",
       "      <td>0.212324</td>\n",
       "      <td>0.116173</td>\n",
       "      <td>0.087708</td>\n",
       "      <td>69.669734</td>\n",
       "      <td>2695.217813</td>\n",
       "      <td>0.364243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347431</td>\n",
       "      <td>708.188358</td>\n",
       "      <td>3186.847609</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.347828</td>\n",
       "      <td>2.122562</td>\n",
       "      <td>0.747097</td>\n",
       "      <td>194.210750</td>\n",
       "      <td>0.363279</td>\n",
       "      <td>0.194211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266592</td>\n",
       "      <td>0.973465</td>\n",
       "      <td>0.845395</td>\n",
       "      <td>0.580608</td>\n",
       "      <td>1.322036</td>\n",
       "      <td>0.400463</td>\n",
       "      <td>0.232512</td>\n",
       "      <td>17.550390</td>\n",
       "      <td>1081.897319</td>\n",
       "      <td>0.672173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953452</td>\n",
       "      <td>708.009100</td>\n",
       "      <td>3186.040949</td>\n",
       "      <td>0.953519</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>0.929959</td>\n",
       "      <td>0.610609</td>\n",
       "      <td>648.404343</td>\n",
       "      <td>0.280041</td>\n",
       "      <td>0.648404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.830782</td>\n",
       "      <td>0.963485</td>\n",
       "      <td>0.971736</td>\n",
       "      <td>0.270474</td>\n",
       "      <td>0.185388</td>\n",
       "      <td>0.039302</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>44.256621</td>\n",
       "      <td>81.149965</td>\n",
       "      <td>0.775990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>707.897906</td>\n",
       "      <td>3185.540575</td>\n",
       "      <td>0.716701</td>\n",
       "      <td>0.716328</td>\n",
       "      <td>1.382759</td>\n",
       "      <td>1.340499</td>\n",
       "      <td>885.138727</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>0.885139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600291</td>\n",
       "      <td>0.839312</td>\n",
       "      <td>0.797427</td>\n",
       "      <td>0.652968</td>\n",
       "      <td>0.510340</td>\n",
       "      <td>0.652287</td>\n",
       "      <td>0.425923</td>\n",
       "      <td>24.480144</td>\n",
       "      <td>932.892398</td>\n",
       "      <td>0.777209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910105</td>\n",
       "      <td>707.992544</td>\n",
       "      <td>3185.966446</td>\n",
       "      <td>0.910238</td>\n",
       "      <td>0.910135</td>\n",
       "      <td>0.990440</td>\n",
       "      <td>0.689159</td>\n",
       "      <td>602.697524</td>\n",
       "      <td>0.320636</td>\n",
       "      <td>0.602698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362498</th>\n",
       "      <td>0.765135</td>\n",
       "      <td>0.641704</td>\n",
       "      <td>0.963197</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.267702</td>\n",
       "      <td>0.214810</td>\n",
       "      <td>0.177313</td>\n",
       "      <td>22.962506</td>\n",
       "      <td>183.428535</td>\n",
       "      <td>0.740243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920760</td>\n",
       "      <td>707.909270</td>\n",
       "      <td>3185.591714</td>\n",
       "      <td>0.920877</td>\n",
       "      <td>0.920786</td>\n",
       "      <td>1.064013</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>534.140819</td>\n",
       "      <td>0.364438</td>\n",
       "      <td>0.534141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362499</th>\n",
       "      <td>0.749872</td>\n",
       "      <td>0.800834</td>\n",
       "      <td>0.842750</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.287853</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.437803</td>\n",
       "      <td>55.826486</td>\n",
       "      <td>452.874619</td>\n",
       "      <td>0.016449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561701</td>\n",
       "      <td>707.939208</td>\n",
       "      <td>3185.726437</td>\n",
       "      <td>0.562559</td>\n",
       "      <td>0.561892</td>\n",
       "      <td>1.690357</td>\n",
       "      <td>1.419079</td>\n",
       "      <td>387.264753</td>\n",
       "      <td>0.564494</td>\n",
       "      <td>0.387265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362500</th>\n",
       "      <td>0.538644</td>\n",
       "      <td>0.788402</td>\n",
       "      <td>0.827843</td>\n",
       "      <td>0.727779</td>\n",
       "      <td>0.618701</td>\n",
       "      <td>0.694040</td>\n",
       "      <td>0.505108</td>\n",
       "      <td>6.365350</td>\n",
       "      <td>423.130196</td>\n",
       "      <td>0.050525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993835</td>\n",
       "      <td>707.935903</td>\n",
       "      <td>3185.711565</td>\n",
       "      <td>0.993844</td>\n",
       "      <td>0.993837</td>\n",
       "      <td>0.959983</td>\n",
       "      <td>0.814335</td>\n",
       "      <td>476.450471</td>\n",
       "      <td>0.591383</td>\n",
       "      <td>0.476450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362501</th>\n",
       "      <td>0.634018</td>\n",
       "      <td>0.991370</td>\n",
       "      <td>0.965507</td>\n",
       "      <td>0.661306</td>\n",
       "      <td>0.455678</td>\n",
       "      <td>0.103640</td>\n",
       "      <td>0.068538</td>\n",
       "      <td>7.738936</td>\n",
       "      <td>2006.981168</td>\n",
       "      <td>0.832208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990892</td>\n",
       "      <td>708.111887</td>\n",
       "      <td>3186.503491</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.990895</td>\n",
       "      <td>0.807461</td>\n",
       "      <td>0.369966</td>\n",
       "      <td>1015.811664</td>\n",
       "      <td>-0.093105</td>\n",
       "      <td>1.015812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362502</th>\n",
       "      <td>0.323002</td>\n",
       "      <td>0.940875</td>\n",
       "      <td>0.665318</td>\n",
       "      <td>0.306548</td>\n",
       "      <td>1.130098</td>\n",
       "      <td>0.587625</td>\n",
       "      <td>0.180135</td>\n",
       "      <td>21.099784</td>\n",
       "      <td>354.152329</td>\n",
       "      <td>0.982156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932955</td>\n",
       "      <td>707.928239</td>\n",
       "      <td>3185.677076</td>\n",
       "      <td>0.933053</td>\n",
       "      <td>0.932977</td>\n",
       "      <td>1.030396</td>\n",
       "      <td>0.897896</td>\n",
       "      <td>416.207341</td>\n",
       "      <td>0.463743</td>\n",
       "      <td>0.416207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362503 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0       0.984234  0.967931  0.849827  0.595673  0.015892  0.402452  0.239730   \n",
       "1       0.808703  0.502863  0.971936  0.754977  0.212324  0.116173  0.087708   \n",
       "2       0.266592  0.973465  0.845395  0.580608  1.322036  0.400463  0.232512   \n",
       "3       0.830782  0.963485  0.971736  0.270474  0.185388  0.039302  0.010630   \n",
       "4       0.600291  0.839312  0.797427  0.652968  0.510340  0.652287  0.425923   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "362498  0.765135  0.641704  0.963197  0.825442  0.267702  0.214810  0.177313   \n",
       "362499  0.749872  0.800834  0.842750  0.719020  0.287853  0.608888  0.437803   \n",
       "362500  0.538644  0.788402  0.827843  0.727779  0.618701  0.694040  0.505108   \n",
       "362501  0.634018  0.991370  0.965507  0.661306  0.455678  0.103640  0.068538   \n",
       "362502  0.323002  0.940875  0.665318  0.306548  1.130098  0.587625  0.180135   \n",
       "\n",
       "              SZA            Z  R_scence  ...  Cos(SZA)        mu_g  \\\n",
       "0       81.357114  1137.064781  0.370253  ...  0.150275  708.015229   \n",
       "1       69.669734  2695.217813  0.364243  ...  0.347431  708.188358   \n",
       "2       17.550390  1081.897319  0.672173  ...  0.953452  708.009100   \n",
       "3       44.256621    81.149965  0.775990  ...  0.716221  707.897906   \n",
       "4       24.480144   932.892398  0.777209  ...  0.910105  707.992544   \n",
       "...           ...          ...       ...  ...       ...         ...   \n",
       "362498  22.962506   183.428535  0.740243  ...  0.920760  707.909270   \n",
       "362499  55.826486   452.874619  0.016449  ...  0.561701  707.939208   \n",
       "362500   6.365350   423.130196  0.050525  ...  0.993835  707.935903   \n",
       "362501   7.738936  2006.981168  0.832208  ...  0.990892  708.111887   \n",
       "362502  21.099784   354.152329  0.982156  ...  0.932955  707.928239   \n",
       "\n",
       "               mu_a  muprime_g  muprime_a  mprime_g  mprime_a       BOA_RT  \\\n",
       "0       3186.068532   0.154730   0.151289  5.695816  3.743539   429.332315   \n",
       "1       3186.847609   0.349206   0.347828  2.122562  0.747097   194.210750   \n",
       "2       3186.040949   0.953519   0.953467  0.929959  0.610609   648.404343   \n",
       "3       3185.540575   0.716701   0.716328  1.382759  1.340499   885.138727   \n",
       "4       3185.966446   0.910238   0.910135  0.990440  0.689159   602.697524   \n",
       "...             ...        ...        ...       ...       ...          ...   \n",
       "362498  3185.591714   0.920877   0.920786  1.064013  0.990855   534.140819   \n",
       "362499  3185.726437   0.562559   0.561892  1.690357  1.419079   387.264753   \n",
       "362500  3185.711565   0.993844   0.993837  0.959983  0.814335   476.450471   \n",
       "362501  3186.503491   0.990905   0.990895  0.807461  0.369966  1015.811664   \n",
       "362502  3185.677076   0.933053   0.932977  1.030396  0.897896   416.207341   \n",
       "\n",
       "           alpha  BOA_fraction  \n",
       "0       0.133511      0.429332  \n",
       "1       0.363279      0.194211  \n",
       "2       0.280041      0.648404  \n",
       "3       0.125099      0.885139  \n",
       "4       0.320636      0.602698  \n",
       "...          ...           ...  \n",
       "362498  0.364438      0.534141  \n",
       "362499  0.564494      0.387265  \n",
       "362500  0.591383      0.476450  \n",
       "362501 -0.093105      1.015812  \n",
       "362502  0.463743      0.416207  \n",
       "\n",
       "[362503 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['BOA_fraction']=data['BOA_RT']/1000\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOA_RT=data['BOA_RT']\n",
    "data.drop(columns=['BOA_RT'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>g1</th>\n",
       "      <th>Cos(SZA)</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984234</td>\n",
       "      <td>0.967931</td>\n",
       "      <td>0.849827</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.402452</td>\n",
       "      <td>0.239730</td>\n",
       "      <td>81.357114</td>\n",
       "      <td>1137.064781</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>0.946187</td>\n",
       "      <td>0.150275</td>\n",
       "      <td>708.015229</td>\n",
       "      <td>3186.068532</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.151289</td>\n",
       "      <td>5.695816</td>\n",
       "      <td>3.743539</td>\n",
       "      <td>0.429332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.808703</td>\n",
       "      <td>0.502863</td>\n",
       "      <td>0.971936</td>\n",
       "      <td>0.754977</td>\n",
       "      <td>0.212324</td>\n",
       "      <td>0.116173</td>\n",
       "      <td>0.087708</td>\n",
       "      <td>69.669734</td>\n",
       "      <td>2695.217813</td>\n",
       "      <td>0.364243</td>\n",
       "      <td>0.910936</td>\n",
       "      <td>0.347431</td>\n",
       "      <td>708.188358</td>\n",
       "      <td>3186.847609</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.347828</td>\n",
       "      <td>2.122562</td>\n",
       "      <td>0.747097</td>\n",
       "      <td>0.194211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266592</td>\n",
       "      <td>0.973465</td>\n",
       "      <td>0.845395</td>\n",
       "      <td>0.580608</td>\n",
       "      <td>1.322036</td>\n",
       "      <td>0.400463</td>\n",
       "      <td>0.232512</td>\n",
       "      <td>17.550390</td>\n",
       "      <td>1081.897319</td>\n",
       "      <td>0.672173</td>\n",
       "      <td>0.896924</td>\n",
       "      <td>0.953452</td>\n",
       "      <td>708.009100</td>\n",
       "      <td>3186.040949</td>\n",
       "      <td>0.953519</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>0.929959</td>\n",
       "      <td>0.610609</td>\n",
       "      <td>0.648404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.830782</td>\n",
       "      <td>0.963485</td>\n",
       "      <td>0.971736</td>\n",
       "      <td>0.270474</td>\n",
       "      <td>0.185388</td>\n",
       "      <td>0.039302</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>44.256621</td>\n",
       "      <td>81.149965</td>\n",
       "      <td>0.775990</td>\n",
       "      <td>0.869940</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>707.897906</td>\n",
       "      <td>3185.540575</td>\n",
       "      <td>0.716701</td>\n",
       "      <td>0.716328</td>\n",
       "      <td>1.382759</td>\n",
       "      <td>1.340499</td>\n",
       "      <td>0.885139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600291</td>\n",
       "      <td>0.839312</td>\n",
       "      <td>0.797427</td>\n",
       "      <td>0.652968</td>\n",
       "      <td>0.510340</td>\n",
       "      <td>0.652287</td>\n",
       "      <td>0.425923</td>\n",
       "      <td>24.480144</td>\n",
       "      <td>932.892398</td>\n",
       "      <td>0.777209</td>\n",
       "      <td>0.878946</td>\n",
       "      <td>0.910105</td>\n",
       "      <td>707.992544</td>\n",
       "      <td>3185.966446</td>\n",
       "      <td>0.910238</td>\n",
       "      <td>0.910135</td>\n",
       "      <td>0.990440</td>\n",
       "      <td>0.689159</td>\n",
       "      <td>0.602698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362498</th>\n",
       "      <td>0.765135</td>\n",
       "      <td>0.641704</td>\n",
       "      <td>0.963197</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.267702</td>\n",
       "      <td>0.214810</td>\n",
       "      <td>0.177313</td>\n",
       "      <td>22.962506</td>\n",
       "      <td>183.428535</td>\n",
       "      <td>0.740243</td>\n",
       "      <td>0.668902</td>\n",
       "      <td>0.920760</td>\n",
       "      <td>707.909270</td>\n",
       "      <td>3185.591714</td>\n",
       "      <td>0.920877</td>\n",
       "      <td>0.920786</td>\n",
       "      <td>1.064013</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>0.534141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362499</th>\n",
       "      <td>0.749872</td>\n",
       "      <td>0.800834</td>\n",
       "      <td>0.842750</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.287853</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.437803</td>\n",
       "      <td>55.826486</td>\n",
       "      <td>452.874619</td>\n",
       "      <td>0.016449</td>\n",
       "      <td>0.818049</td>\n",
       "      <td>0.561701</td>\n",
       "      <td>707.939208</td>\n",
       "      <td>3185.726437</td>\n",
       "      <td>0.562559</td>\n",
       "      <td>0.561892</td>\n",
       "      <td>1.690357</td>\n",
       "      <td>1.419079</td>\n",
       "      <td>0.387265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362500</th>\n",
       "      <td>0.538644</td>\n",
       "      <td>0.788402</td>\n",
       "      <td>0.827843</td>\n",
       "      <td>0.727779</td>\n",
       "      <td>0.618701</td>\n",
       "      <td>0.694040</td>\n",
       "      <td>0.505108</td>\n",
       "      <td>6.365350</td>\n",
       "      <td>423.130196</td>\n",
       "      <td>0.050525</td>\n",
       "      <td>0.896188</td>\n",
       "      <td>0.993835</td>\n",
       "      <td>707.935903</td>\n",
       "      <td>3185.711565</td>\n",
       "      <td>0.993844</td>\n",
       "      <td>0.993837</td>\n",
       "      <td>0.959983</td>\n",
       "      <td>0.814335</td>\n",
       "      <td>0.476450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362501</th>\n",
       "      <td>0.634018</td>\n",
       "      <td>0.991370</td>\n",
       "      <td>0.965507</td>\n",
       "      <td>0.661306</td>\n",
       "      <td>0.455678</td>\n",
       "      <td>0.103640</td>\n",
       "      <td>0.068538</td>\n",
       "      <td>7.738936</td>\n",
       "      <td>2006.981168</td>\n",
       "      <td>0.832208</td>\n",
       "      <td>0.860886</td>\n",
       "      <td>0.990892</td>\n",
       "      <td>708.111887</td>\n",
       "      <td>3186.503491</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.990895</td>\n",
       "      <td>0.807461</td>\n",
       "      <td>0.369966</td>\n",
       "      <td>1.015812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362502</th>\n",
       "      <td>0.323002</td>\n",
       "      <td>0.940875</td>\n",
       "      <td>0.665318</td>\n",
       "      <td>0.306548</td>\n",
       "      <td>1.130098</td>\n",
       "      <td>0.587625</td>\n",
       "      <td>0.180135</td>\n",
       "      <td>21.099784</td>\n",
       "      <td>354.152329</td>\n",
       "      <td>0.982156</td>\n",
       "      <td>0.832794</td>\n",
       "      <td>0.932955</td>\n",
       "      <td>707.928239</td>\n",
       "      <td>3185.677076</td>\n",
       "      <td>0.933053</td>\n",
       "      <td>0.932977</td>\n",
       "      <td>1.030396</td>\n",
       "      <td>0.897896</td>\n",
       "      <td>0.416207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362503 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0       0.984234  0.967931  0.849827  0.595673  0.015892  0.402452  0.239730   \n",
       "1       0.808703  0.502863  0.971936  0.754977  0.212324  0.116173  0.087708   \n",
       "2       0.266592  0.973465  0.845395  0.580608  1.322036  0.400463  0.232512   \n",
       "3       0.830782  0.963485  0.971736  0.270474  0.185388  0.039302  0.010630   \n",
       "4       0.600291  0.839312  0.797427  0.652968  0.510340  0.652287  0.425923   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "362498  0.765135  0.641704  0.963197  0.825442  0.267702  0.214810  0.177313   \n",
       "362499  0.749872  0.800834  0.842750  0.719020  0.287853  0.608888  0.437803   \n",
       "362500  0.538644  0.788402  0.827843  0.727779  0.618701  0.694040  0.505108   \n",
       "362501  0.634018  0.991370  0.965507  0.661306  0.455678  0.103640  0.068538   \n",
       "362502  0.323002  0.940875  0.665318  0.306548  1.130098  0.587625  0.180135   \n",
       "\n",
       "              SZA            Z  R_scence        g1  Cos(SZA)        mu_g  \\\n",
       "0       81.357114  1137.064781  0.370253  0.946187  0.150275  708.015229   \n",
       "1       69.669734  2695.217813  0.364243  0.910936  0.347431  708.188358   \n",
       "2       17.550390  1081.897319  0.672173  0.896924  0.953452  708.009100   \n",
       "3       44.256621    81.149965  0.775990  0.869940  0.716221  707.897906   \n",
       "4       24.480144   932.892398  0.777209  0.878946  0.910105  707.992544   \n",
       "...           ...          ...       ...       ...       ...         ...   \n",
       "362498  22.962506   183.428535  0.740243  0.668902  0.920760  707.909270   \n",
       "362499  55.826486   452.874619  0.016449  0.818049  0.561701  707.939208   \n",
       "362500   6.365350   423.130196  0.050525  0.896188  0.993835  707.935903   \n",
       "362501   7.738936  2006.981168  0.832208  0.860886  0.990892  708.111887   \n",
       "362502  21.099784   354.152329  0.982156  0.832794  0.932955  707.928239   \n",
       "\n",
       "               mu_a  muprime_g  muprime_a  mprime_g  mprime_a  BOA_fraction  \n",
       "0       3186.068532   0.154730   0.151289  5.695816  3.743539      0.429332  \n",
       "1       3186.847609   0.349206   0.347828  2.122562  0.747097      0.194211  \n",
       "2       3186.040949   0.953519   0.953467  0.929959  0.610609      0.648404  \n",
       "3       3185.540575   0.716701   0.716328  1.382759  1.340499      0.885139  \n",
       "4       3185.966446   0.910238   0.910135  0.990440  0.689159      0.602698  \n",
       "...             ...        ...        ...       ...       ...           ...  \n",
       "362498  3185.591714   0.920877   0.920786  1.064013  0.990855      0.534141  \n",
       "362499  3185.726437   0.562559   0.561892  1.690357  1.419079      0.387265  \n",
       "362500  3185.711565   0.993844   0.993837  0.959983  0.814335      0.476450  \n",
       "362501  3186.503491   0.990905   0.990895  0.807461  0.369966      1.015812  \n",
       "362502  3185.677076   0.933053   0.932977  1.030396  0.897896      0.416207  \n",
       "\n",
       "[362503 rows x 19 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = data['alpha']\n",
    "data.drop(columns=['alpha'],inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "cols_to_scale = data.drop(columns=['BOA_fraction']).columns\n",
    "X_scaled = scaler.fit_transform(data[cols_to_scale])\n",
    "y_scaled=scaler_y.fit_transform(data['BOA_fraction'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled,y_scaled,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_with_negative_penalty(y_true, y_pred):\n",
    "    # MAE = moyenne des erreurs absolues\n",
    "    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "    \n",
    "    # Pénalité pour les valeurs négatives\n",
    "    penalty = tf.reduce_mean(tf.maximum(-y_pred, 0.0)) * 10  # ajuste le facteur selon tes besoins\n",
    "\n",
    "    return mae + penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,017</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">43,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">175</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)            │         \u001b[38;5;34m4,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)            │        \u001b[38;5;34m58,017\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)            │        \u001b[38;5;34m43,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m175\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,100</span> (414.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,100\u001b[0m (414.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,100</span> (414.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,100\u001b[0m (414.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"best_params.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Dense(params[\"units1\"], activation=params[\"activation\"], input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(params[\"dropout1\"]))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Dense(params[\"units2\"], activation=params[\"activation\"]))\n",
    "model.add(Dropout(params[\"dropout2\"]))\n",
    "\n",
    "# Layer 3 (if applicable)\n",
    "if params[\"n_layers\"] >= 3:\n",
    "    model.add(Dense(params[\"units3\"], activation=params[\"activation\"]))\n",
    "    model.add(Dropout(params[\"dropout3\"]))\n",
    "\n",
    "# Layer 4 (if applicable)\n",
    "if params[\"n_layers\"] == 4:\n",
    "    model.add(Dense(params[\"units4\"], activation=params[\"activation\"]))\n",
    "    model.add(Dropout(params[\"dropout4\"]))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "model.compile(optimizer=optimizer, loss=mae_with_negative_penalty, metrics=['mae'])\n",
    "\n",
    "# Affichage du résumé du modèle\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1531 - mae: 0.1239\n",
      "Epoch 1: val_mae improved from inf to 0.02929, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 0.1530 - mae: 0.1238 - val_loss: 0.0293 - val_mae: 0.0293 - learning_rate: 3.4066e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0379 - mae: 0.0368\n",
      "Epoch 2: val_mae improved from 0.02929 to 0.01551, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 0.0379 - mae: 0.0368 - val_loss: 0.0156 - val_mae: 0.0155 - learning_rate: 3.4066e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m1707/3399\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 0.0261 - mae: 0.0257"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     28\u001b[39m csv_logger = CSVLogger(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtraining_log_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Train phase :\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    130\u001b[39m args = args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[32m    131\u001b[39m kwargs = kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m function = \u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[39m, in \u001b[36mtrace_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    175\u001b[39m     args = tracing_options.input_signature\n\u001b[32m    176\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m   concrete_function = \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options.bind_graph_to_function:\n\u001b[32m    183\u001b[39m   concrete_function._garbage_collector.release()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:230\u001b[39m, in \u001b[36m_maybe_define_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    220\u001b[39m current_func_context = function_context.make_function_context(\n\u001b[32m    221\u001b[39m     tracing_options.scope_type\n\u001b[32m    222\u001b[39m )\n\u001b[32m    224\u001b[39m capture_types = (\n\u001b[32m    225\u001b[39m     tracing_options.function_captures.capture_types\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_captures\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    228\u001b[39m )\n\u001b[32m    229\u001b[39m lookup_func_type, lookup_func_context = (\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43mfunction_type_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_canonicalized_monomorphic_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapture_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolymorphic_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m )\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    239\u001b[39m   concrete_function = tracing_options.function_cache.lookup(\n\u001b[32m    240\u001b[39m       lookup_func_type, current_func_context\n\u001b[32m    241\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:375\u001b[39m, in \u001b[36mmake_canonicalized_monomorphic_type\u001b[39m\u001b[34m(args, kwargs, capture_types, polymorphic_type)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generates function type given the function arguments.\"\"\"\u001b[39;00m\n\u001b[32m    369\u001b[39m kwargs = {\n\u001b[32m    370\u001b[39m     function_type_lib.sanitize_arg_name(name): value\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m    372\u001b[39m }\n\u001b[32m    374\u001b[39m function_type, type_context = (\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[43mfunction_type_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanonicalize_to_monomorphic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolymorphic_type\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m )\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m function_type, type_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:587\u001b[39m, in \u001b[36mcanonicalize_to_monomorphic\u001b[39m\u001b[34m(args, kwargs, default_values, capture_types, polymorphic_type)\u001b[39m\n\u001b[32m    581\u001b[39m       parameters.append(\n\u001b[32m    582\u001b[39m           _make_validated_mono_param(kwarg_name, arg[kwarg_name],\n\u001b[32m    583\u001b[39m                                      Parameter.KEYWORD_ONLY, type_context,\n\u001b[32m    584\u001b[39m                                      poly_parameter.type_constraint))\n\u001b[32m    585\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    586\u001b[39m     parameters.append(\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m         \u001b[43m_make_validated_mono_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly_parameter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mtype_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mpoly_parameter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype_constraint\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m FunctionType(parameters, capture_types), type_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:532\u001b[39m, in \u001b[36m_make_validated_mono_param\u001b[39m\u001b[34m(name, value, kind, type_context, poly_type)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m poly_type \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mono_type.is_subtype_of(poly_type):\n\u001b[32m    529\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParameter `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` was expected to be of type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    530\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoly_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmono_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:73\u001b[39m, in \u001b[36mParameter.__init__\u001b[39m\u001b[34m(self, name, kind, optional, type_constraint)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m type_constraint \u001b[38;5;129;01mand\u001b[39;00m kind \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.VAR_POSITIONAL, \u001b[38;5;28mself\u001b[39m.VAR_KEYWORD]:\n\u001b[32m     71\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mVariable args/kwargs can not have type constraints.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(type_constraint, (trace.TraceType, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m))):\n\u001b[32m     74\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m     75\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mType constraints can only be an instance of a TraceType but got \u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m     76\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mtype_constraint=\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(type_constraint) + \u001b[33m\"\u001b[39m\u001b[33m for Parameter \u001b[39m\u001b[33m\"\u001b[39m + name)\n\u001b[32m     78\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     79\u001b[39m     name,\n\u001b[32m     80\u001b[39m     kind,\n\u001b[32m     81\u001b[39m     default=CAPTURED_DEFAULT_VALUE \u001b[38;5;28;01mif\u001b[39;00m optional \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.empty,\n\u001b[32m     82\u001b[39m     annotation=type_constraint\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m type_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.empty)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen abc>:117\u001b[39m, in \u001b[36m__instancecheck__\u001b[39m\u001b[34m(cls, instance)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_mae',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Model checkpoint (save best model)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='deep_model.keras',\n",
    "    monitor='val_mae',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. CSV logger (log training history)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_logger = CSVLogger(f'training_log_{timestamp}.csv')\n",
    "\n",
    "# Train phase :\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_cb, csv_logger],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2833/2833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scaling(X,scaler,y,scaler_y):\n",
    "    '''cols_to_scale_inv = X.drop(columns=['mprime_a_bin_0', 'mprime_a_bin_1',\n",
    "       'mprime_a_bin_2', 'mprime_a_bin_3', 'mprime_a_bin_4', 'mprime_g_bin_0',\n",
    "       'mprime_g_bin_1', 'mprime_g_bin_2', 'mprime_g_bin_3', 'mprime_g_bin_4'\n",
    "       ]).columns\n",
    "    cols_to_encode_inv = ['mprime_a_bin_0', 'mprime_a_bin_1',\n",
    "       'mprime_a_bin_2', 'mprime_a_bin_3', 'mprime_a_bin_4', 'mprime_g_bin_0',\n",
    "       'mprime_g_bin_1', 'mprime_g_bin_2', 'mprime_g_bin_3', 'mprime_g_bin_4',\n",
    "       ]'''\n",
    "    #X_scaled_part = X_test[cols_to_scale_inv].values\n",
    "    #X_encoded_part = X_test[cols_to_encode_inv].values\n",
    "    # 2. Inverser la standardisation\n",
    "    X_original_scaled = scaler.inverse_transform(X)\n",
    "\n",
    "    # 3. Inverser l'encodage\n",
    "    #X_original_categoricals = encoder.inverse_transform(X_encoded_part)\n",
    "    X_real= pd.DataFrame(\n",
    "        X_original_scaled,\n",
    "        columns=['Tg_scat', 'Tg_abs', 'Ta_abs', 'SSA', 'GOD', 'AOD', 'AODS', 'SZA', 'Z',\n",
    "        'R_scence', 'g1', 'Cos(SZA)', 'mu_g', 'mu_a', 'muprime_g', 'muprime_a',\n",
    "        'mprime_g', 'mprime_a'],\n",
    "        index=X_scaled.index if isinstance(X_scaled, pd.DataFrame) else None\n",
    "    )\n",
    "    '''df_categoricals = pd.DataFrame(\n",
    "        X_original_categoricals,\n",
    "        columns=['mprime_a_bin', 'mprime_g_bin'],\n",
    "        index=X_test.index\n",
    "    )'''\n",
    "    y_real=scaler_y.inverse_transform(y)\n",
    "    return X_real , y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real,y_test_real=inverse_scaling(X_test,scaler,y_test,scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>g1</th>\n",
       "      <th>Cos(SZA)</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>0.849374</td>\n",
       "      <td>0.951706</td>\n",
       "      <td>708.317583</td>\n",
       "      <td>3187.429121</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>0.814209</td>\n",
       "      <td>0.470531</td>\n",
       "      <td>708.012549</td>\n",
       "      <td>3186.056472</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>0.915442</td>\n",
       "      <td>0.240763</td>\n",
       "      <td>707.890641</td>\n",
       "      <td>3185.507886</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>0.852120</td>\n",
       "      <td>0.453122</td>\n",
       "      <td>708.341159</td>\n",
       "      <td>3187.535216</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>0.869006</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>707.958273</td>\n",
       "      <td>3185.812230</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>0.959487</td>\n",
       "      <td>0.962254</td>\n",
       "      <td>708.064025</td>\n",
       "      <td>3186.288112</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>0.812618</td>\n",
       "      <td>0.025776</td>\n",
       "      <td>708.122465</td>\n",
       "      <td>3186.551091</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.909007</td>\n",
       "      <td>707.894025</td>\n",
       "      <td>3185.523112</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>0.976101</td>\n",
       "      <td>0.859409</td>\n",
       "      <td>708.028352</td>\n",
       "      <td>3186.127582</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>0.843193</td>\n",
       "      <td>0.993832</td>\n",
       "      <td>707.922330</td>\n",
       "      <td>3185.650486</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence        g1  Cos(SZA)        mu_g  \\\n",
       "0      17.879232  3858.242750  0.894296  0.849374  0.951706  708.317583   \n",
       "1      61.931220  1112.943981  0.419185  0.814209  0.470531  708.012549   \n",
       "2      76.068422    15.772549  0.729906  0.915442  0.240763  707.890641   \n",
       "3      63.055806  4070.432067  0.251342  0.852120  0.453122  708.341159   \n",
       "4       2.351437   624.459254  0.881197  0.869006  0.999158  707.958273   \n",
       "...          ...          ...       ...       ...       ...         ...   \n",
       "90621  15.792506  1576.224940  0.696585  0.959487  0.962254  708.064025   \n",
       "90622  88.522985  2102.182329  0.968884  0.812618  0.025776  708.122465   \n",
       "90623  24.631558    46.223183  0.761757  0.789790  0.909007  707.894025   \n",
       "90624  30.749744  1255.164902  0.559097  0.976101  0.859409  708.028352   \n",
       "90625   6.366867   300.971201  0.195906  0.843193  0.993832  707.922330   \n",
       "\n",
       "              mu_a  muprime_g  muprime_a   mprime_g   mprime_a  \n",
       "0      3187.429121   0.951776   0.951721   0.684362   0.152645  \n",
       "1      3186.056472   0.471695   0.470791   1.873413   1.217586  \n",
       "2      3185.507886   0.243492   0.241375   4.099723   4.110381  \n",
       "3      3187.535216   0.454355   0.453397   1.400187   0.288163  \n",
       "4      3185.812230   0.999159   0.999158   0.933753   0.732430  \n",
       "...            ...        ...        ...        ...        ...  \n",
       "90621  3186.288112   0.962308   0.962266   0.872219   0.472533  \n",
       "90622  3186.551091   0.042399   0.030856  18.672395  11.328534  \n",
       "90623  3185.523112   0.909141   0.909037   1.094304   1.074933  \n",
       "90624  3186.127582   0.859623   0.859456   1.011868   0.621184  \n",
       "90625  3185.650486   0.993841   0.993834   0.973105   0.865627  \n",
       "\n",
       "[90626 rows x 18 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91444475],\n",
       "       [0.5945755 ],\n",
       "       [0.28459194],\n",
       "       ...,\n",
       "       [0.73562703],\n",
       "       [0.92437937],\n",
       "       [0.73044082]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91500145],\n",
       "       [0.59740365],\n",
       "       [0.28012058],\n",
       "       ...,\n",
       "       [0.7342156 ],\n",
       "       [0.9207122 ],\n",
       "       [0.7328465 ]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_real=scaler_y.inverse_transform(y_pred)\n",
    "y_pred_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91444475],\n",
       "       [0.5945755 ],\n",
       "       [0.28459194],\n",
       "       ...,\n",
       "       [0.73562703],\n",
       "       [0.92437937],\n",
       "       [0.73044082]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real['BOA_fraction']=y_test_real\n",
    "X_test_real['BOA_fraction_pred']=y_pred_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>708.317583</td>\n",
       "      <td>3187.429121</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.915001</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>915.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>708.012549</td>\n",
       "      <td>3186.056472</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.597404</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>597.403625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>707.890641</td>\n",
       "      <td>3185.507886</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280121</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>280.120575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>708.341159</td>\n",
       "      <td>3187.535216</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.372752</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>372.752289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>707.958273</td>\n",
       "      <td>3185.812230</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.833779</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>833.779480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>708.064025</td>\n",
       "      <td>3186.288112</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800691</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>800.691040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>708.122465</td>\n",
       "      <td>3186.551091</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>37.870831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>707.894025</td>\n",
       "      <td>3185.523112</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.734216</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>734.215637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>708.028352</td>\n",
       "      <td>3186.127582</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.920712</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>920.712158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>707.922330</td>\n",
       "      <td>3185.650486</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.732846</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>732.846497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...        mu_g         mu_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...  708.317583  3187.429121   \n",
       "1      61.931220  1112.943981  0.419185  ...  708.012549  3186.056472   \n",
       "2      76.068422    15.772549  0.729906  ...  707.890641  3185.507886   \n",
       "3      63.055806  4070.432067  0.251342  ...  708.341159  3187.535216   \n",
       "4       2.351437   624.459254  0.881197  ...  707.958273  3185.812230   \n",
       "...          ...          ...       ...  ...         ...          ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...  708.064025  3186.288112   \n",
       "90622  88.522985  2102.182329  0.968884  ...  708.122465  3186.551091   \n",
       "90623  24.631558    46.223183  0.761757  ...  707.894025  3185.523112   \n",
       "90624  30.749744  1255.164902  0.559097  ...  708.028352  3186.127582   \n",
       "90625   6.366867   300.971201  0.195906  ...  707.922330  3185.650486   \n",
       "\n",
       "       muprime_g  muprime_a   mprime_g   mprime_a  BOA_fraction  \\\n",
       "0       0.951776   0.951721   0.684362   0.152645      0.914445   \n",
       "1       0.471695   0.470791   1.873413   1.217586      0.594575   \n",
       "2       0.243492   0.241375   4.099723   4.110381      0.284592   \n",
       "3       0.454355   0.453397   1.400187   0.288163      0.373719   \n",
       "4       0.999159   0.999158   0.933753   0.732430      0.832138   \n",
       "...          ...        ...        ...        ...           ...   \n",
       "90621   0.962308   0.962266   0.872219   0.472533      0.801588   \n",
       "90622   0.042399   0.030856  18.672395  11.328534      0.032089   \n",
       "90623   0.909141   0.909037   1.094304   1.074933      0.735627   \n",
       "90624   0.859623   0.859456   1.011868   0.621184      0.924379   \n",
       "90625   0.993841   0.993834   0.973105   0.865627      0.730441   \n",
       "\n",
       "       BOA_fraction_pred      BOA_RT    BOA_pred  \n",
       "0               0.915001  914.444755  915.001465  \n",
       "1               0.597404  594.575497  597.403625  \n",
       "2               0.280121  284.591943  280.120575  \n",
       "3               0.372752  373.718733  372.752289  \n",
       "4               0.833779  832.137702  833.779480  \n",
       "...                  ...         ...         ...  \n",
       "90621           0.800691  801.588284  800.691040  \n",
       "90622           0.037871   32.089238   37.870831  \n",
       "90623           0.734216  735.627029  734.215637  \n",
       "90624           0.920712  924.379368  920.712158  \n",
       "90625           0.732846  730.440816  732.846497  \n",
       "\n",
       "[90626 rows x 22 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real['BOA_RT']=X_test_real['BOA_fraction']*1000\n",
    "X_test_real['BOA_pred']=X_test_real['BOA_fraction_pred']*1000\n",
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.803465</td>\n",
       "      <td>0.105465</td>\n",
       "      <td>0.996433</td>\n",
       "      <td>0.870484</td>\n",
       "      <td>0.218822</td>\n",
       "      <td>0.027593</td>\n",
       "      <td>0.024019</td>\n",
       "      <td>88.806839</td>\n",
       "      <td>235.894735</td>\n",
       "      <td>0.575316</td>\n",
       "      <td>...</td>\n",
       "      <td>707.915099</td>\n",
       "      <td>3185.617947</td>\n",
       "      <td>0.038936</td>\n",
       "      <td>0.026698</td>\n",
       "      <td>25.018925</td>\n",
       "      <td>33.288926</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>-0.023689</td>\n",
       "      <td>1.129110</td>\n",
       "      <td>-23.689295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.962999</td>\n",
       "      <td>0.475789</td>\n",
       "      <td>0.805453</td>\n",
       "      <td>0.776911</td>\n",
       "      <td>0.037703</td>\n",
       "      <td>0.969794</td>\n",
       "      <td>0.753444</td>\n",
       "      <td>88.655540</td>\n",
       "      <td>208.317651</td>\n",
       "      <td>0.024482</td>\n",
       "      <td>...</td>\n",
       "      <td>707.912035</td>\n",
       "      <td>3185.604159</td>\n",
       "      <td>0.040762</td>\n",
       "      <td>0.028891</td>\n",
       "      <td>23.971421</td>\n",
       "      <td>31.188786</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>-0.004016</td>\n",
       "      <td>3.870029</td>\n",
       "      <td>-4.016435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.559104</td>\n",
       "      <td>0.273698</td>\n",
       "      <td>0.863488</td>\n",
       "      <td>0.705478</td>\n",
       "      <td>0.581420</td>\n",
       "      <td>0.498351</td>\n",
       "      <td>0.351575</td>\n",
       "      <td>84.646672</td>\n",
       "      <td>473.351227</td>\n",
       "      <td>0.868874</td>\n",
       "      <td>...</td>\n",
       "      <td>707.941483</td>\n",
       "      <td>3185.736676</td>\n",
       "      <td>0.100270</td>\n",
       "      <td>0.094936</td>\n",
       "      <td>9.462080</td>\n",
       "      <td>8.313499</td>\n",
       "      <td>0.011149</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>11.149368</td>\n",
       "      <td>-0.122689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>0.954885</td>\n",
       "      <td>0.435707</td>\n",
       "      <td>0.948093</td>\n",
       "      <td>0.955365</td>\n",
       "      <td>0.046164</td>\n",
       "      <td>1.194175</td>\n",
       "      <td>1.140872</td>\n",
       "      <td>85.481567</td>\n",
       "      <td>84.371008</td>\n",
       "      <td>0.589548</td>\n",
       "      <td>...</td>\n",
       "      <td>707.898263</td>\n",
       "      <td>3185.542186</td>\n",
       "      <td>0.086851</td>\n",
       "      <td>0.080712</td>\n",
       "      <td>11.406541</td>\n",
       "      <td>11.877960</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>-0.004921</td>\n",
       "      <td>5.698806</td>\n",
       "      <td>-4.920907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>0.797444</td>\n",
       "      <td>0.270825</td>\n",
       "      <td>0.939270</td>\n",
       "      <td>0.851809</td>\n",
       "      <td>0.226344</td>\n",
       "      <td>0.422778</td>\n",
       "      <td>0.360126</td>\n",
       "      <td>87.828576</td>\n",
       "      <td>240.976985</td>\n",
       "      <td>0.311731</td>\n",
       "      <td>...</td>\n",
       "      <td>707.915664</td>\n",
       "      <td>3185.620488</td>\n",
       "      <td>0.051553</td>\n",
       "      <td>0.041651</td>\n",
       "      <td>18.884901</td>\n",
       "      <td>21.283589</td>\n",
       "      <td>0.005788</td>\n",
       "      <td>-0.002479</td>\n",
       "      <td>5.788132</td>\n",
       "      <td>-2.479282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89300</th>\n",
       "      <td>0.704009</td>\n",
       "      <td>0.267212</td>\n",
       "      <td>0.858066</td>\n",
       "      <td>0.259349</td>\n",
       "      <td>0.350965</td>\n",
       "      <td>0.206676</td>\n",
       "      <td>0.053601</td>\n",
       "      <td>86.292227</td>\n",
       "      <td>1095.058574</td>\n",
       "      <td>0.029437</td>\n",
       "      <td>...</td>\n",
       "      <td>708.010562</td>\n",
       "      <td>3186.047529</td>\n",
       "      <td>0.074141</td>\n",
       "      <td>0.066999</td>\n",
       "      <td>11.942693</td>\n",
       "      <td>8.632559</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>-0.003394</td>\n",
       "      <td>9.429188</td>\n",
       "      <td>-3.394374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89411</th>\n",
       "      <td>0.696551</td>\n",
       "      <td>0.070951</td>\n",
       "      <td>0.956512</td>\n",
       "      <td>0.346376</td>\n",
       "      <td>0.361614</td>\n",
       "      <td>0.068024</td>\n",
       "      <td>0.023562</td>\n",
       "      <td>87.337220</td>\n",
       "      <td>1664.216419</td>\n",
       "      <td>0.943114</td>\n",
       "      <td>...</td>\n",
       "      <td>708.073802</td>\n",
       "      <td>3186.332108</td>\n",
       "      <td>0.058489</td>\n",
       "      <td>0.049613</td>\n",
       "      <td>14.210759</td>\n",
       "      <td>8.770562</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>-0.028216</td>\n",
       "      <td>1.438845</td>\n",
       "      <td>-28.216393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89976</th>\n",
       "      <td>0.774865</td>\n",
       "      <td>0.141154</td>\n",
       "      <td>0.976055</td>\n",
       "      <td>0.974520</td>\n",
       "      <td>0.255066</td>\n",
       "      <td>0.951193</td>\n",
       "      <td>0.926956</td>\n",
       "      <td>69.507183</td>\n",
       "      <td>892.445872</td>\n",
       "      <td>0.678209</td>\n",
       "      <td>...</td>\n",
       "      <td>707.988050</td>\n",
       "      <td>3185.946223</td>\n",
       "      <td>0.351849</td>\n",
       "      <td>0.350483</td>\n",
       "      <td>2.573826</td>\n",
       "      <td>1.826170</td>\n",
       "      <td>0.007140</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>7.139998</td>\n",
       "      <td>-0.026237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90200</th>\n",
       "      <td>0.402163</td>\n",
       "      <td>0.151652</td>\n",
       "      <td>0.871290</td>\n",
       "      <td>0.211994</td>\n",
       "      <td>0.910897</td>\n",
       "      <td>0.174846</td>\n",
       "      <td>0.037066</td>\n",
       "      <td>71.809721</td>\n",
       "      <td>949.383000</td>\n",
       "      <td>0.122373</td>\n",
       "      <td>...</td>\n",
       "      <td>707.994376</td>\n",
       "      <td>3185.974691</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.312627</td>\n",
       "      <td>2.864060</td>\n",
       "      <td>1.989840</td>\n",
       "      <td>0.007736</td>\n",
       "      <td>-0.004556</td>\n",
       "      <td>7.736398</td>\n",
       "      <td>-4.556273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90578</th>\n",
       "      <td>0.569530</td>\n",
       "      <td>0.249183</td>\n",
       "      <td>0.795300</td>\n",
       "      <td>0.551670</td>\n",
       "      <td>0.562944</td>\n",
       "      <td>0.510864</td>\n",
       "      <td>0.281828</td>\n",
       "      <td>82.697695</td>\n",
       "      <td>90.238448</td>\n",
       "      <td>0.432696</td>\n",
       "      <td>...</td>\n",
       "      <td>707.898915</td>\n",
       "      <td>3185.545119</td>\n",
       "      <td>0.132348</td>\n",
       "      <td>0.128308</td>\n",
       "      <td>7.480467</td>\n",
       "      <td>7.449933</td>\n",
       "      <td>0.007190</td>\n",
       "      <td>-0.011720</td>\n",
       "      <td>7.190046</td>\n",
       "      <td>-11.720368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "186    0.803465  0.105465  0.996433  0.870484  0.218822  0.027593  0.024019   \n",
       "421    0.962999  0.475789  0.805453  0.776911  0.037703  0.969794  0.753444   \n",
       "482    0.559104  0.273698  0.863488  0.705478  0.581420  0.498351  0.351575   \n",
       "1148   0.954885  0.435707  0.948093  0.955365  0.046164  1.194175  1.140872   \n",
       "1703   0.797444  0.270825  0.939270  0.851809  0.226344  0.422778  0.360126   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "89300  0.704009  0.267212  0.858066  0.259349  0.350965  0.206676  0.053601   \n",
       "89411  0.696551  0.070951  0.956512  0.346376  0.361614  0.068024  0.023562   \n",
       "89976  0.774865  0.141154  0.976055  0.974520  0.255066  0.951193  0.926956   \n",
       "90200  0.402163  0.151652  0.871290  0.211994  0.910897  0.174846  0.037066   \n",
       "90578  0.569530  0.249183  0.795300  0.551670  0.562944  0.510864  0.281828   \n",
       "\n",
       "             SZA            Z  R_scence  ...        mu_g         mu_a  \\\n",
       "186    88.806839   235.894735  0.575316  ...  707.915099  3185.617947   \n",
       "421    88.655540   208.317651  0.024482  ...  707.912035  3185.604159   \n",
       "482    84.646672   473.351227  0.868874  ...  707.941483  3185.736676   \n",
       "1148   85.481567    84.371008  0.589548  ...  707.898263  3185.542186   \n",
       "1703   87.828576   240.976985  0.311731  ...  707.915664  3185.620488   \n",
       "...          ...          ...       ...  ...         ...          ...   \n",
       "89300  86.292227  1095.058574  0.029437  ...  708.010562  3186.047529   \n",
       "89411  87.337220  1664.216419  0.943114  ...  708.073802  3186.332108   \n",
       "89976  69.507183   892.445872  0.678209  ...  707.988050  3185.946223   \n",
       "90200  71.809721   949.383000  0.122373  ...  707.994376  3185.974691   \n",
       "90578  82.697695    90.238448  0.432696  ...  707.898915  3185.545119   \n",
       "\n",
       "       muprime_g  muprime_a   mprime_g   mprime_a  BOA_fraction  \\\n",
       "186     0.038936   0.026698  25.018925  33.288926      0.001129   \n",
       "421     0.040762   0.028891  23.971421  31.188786      0.003870   \n",
       "482     0.100270   0.094936   9.462080   8.313499      0.011149   \n",
       "1148    0.086851   0.080712  11.406541  11.877960      0.005699   \n",
       "1703    0.051553   0.041651  18.884901  21.283589      0.005788   \n",
       "...          ...        ...        ...        ...           ...   \n",
       "89300   0.074141   0.066999  11.942693   8.632559      0.009429   \n",
       "89411   0.058489   0.049613  14.210759   8.770562      0.001439   \n",
       "89976   0.351849   0.350483   2.573826   1.826170      0.007140   \n",
       "90200   0.314200   0.312627   2.864060   1.989840      0.007736   \n",
       "90578   0.132348   0.128308   7.480467   7.449933      0.007190   \n",
       "\n",
       "       BOA_fraction_pred     BOA_RT   BOA_pred  \n",
       "186            -0.023689   1.129110 -23.689295  \n",
       "421            -0.004016   3.870029  -4.016435  \n",
       "482            -0.000123  11.149368  -0.122689  \n",
       "1148           -0.004921   5.698806  -4.920907  \n",
       "1703           -0.002479   5.788132  -2.479282  \n",
       "...                  ...        ...        ...  \n",
       "89300          -0.003394   9.429188  -3.394374  \n",
       "89411          -0.028216   1.438845 -28.216393  \n",
       "89976          -0.000026   7.139998  -0.026237  \n",
       "90200          -0.004556   7.736398  -4.556273  \n",
       "90578          -0.011720   7.190046 -11.720368  \n",
       "\n",
       "[188 rows x 22 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real[X_test_real['BOA_pred']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_AI</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.915612</td>\n",
       "      <td>-0.304591</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>915.611863</td>\n",
       "      <td>915.611863</td>\n",
       "      <td>914.444755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.594615</td>\n",
       "      <td>0.376599</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>594.614685</td>\n",
       "      <td>594.614685</td>\n",
       "      <td>594.575497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280167</td>\n",
       "      <td>0.258431</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>280.167222</td>\n",
       "      <td>280.167222</td>\n",
       "      <td>284.591943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>372.900099</td>\n",
       "      <td>372.900099</td>\n",
       "      <td>373.718733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.834354</td>\n",
       "      <td>0.119085</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>834.353626</td>\n",
       "      <td>834.353626</td>\n",
       "      <td>832.137702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800986</td>\n",
       "      <td>0.202855</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>800.986350</td>\n",
       "      <td>800.986350</td>\n",
       "      <td>801.588284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>36.498807</td>\n",
       "      <td>36.498807</td>\n",
       "      <td>32.089238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.734697</td>\n",
       "      <td>0.159697</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>734.697282</td>\n",
       "      <td>734.697282</td>\n",
       "      <td>735.627029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.922319</td>\n",
       "      <td>0.153491</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>922.318637</td>\n",
       "      <td>922.318637</td>\n",
       "      <td>924.379368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>0.228026</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>730.773032</td>\n",
       "      <td>730.773032</td>\n",
       "      <td>730.440816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_a   mprime_g   mprime_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...   0.951721   0.684362   0.152645   \n",
       "1      61.931220  1112.943981  0.419185  ...   0.470791   1.873413   1.217586   \n",
       "2      76.068422    15.772549  0.729906  ...   0.241375   4.099723   4.110381   \n",
       "3      63.055806  4070.432067  0.251342  ...   0.453397   1.400187   0.288163   \n",
       "4       2.351437   624.459254  0.881197  ...   0.999158   0.933753   0.732430   \n",
       "...          ...          ...       ...  ...        ...        ...        ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...   0.962266   0.872219   0.472533   \n",
       "90622  88.522985  2102.182329  0.968884  ...   0.030856  18.672395  11.328534   \n",
       "90623  24.631558    46.223183  0.761757  ...   0.909037   1.094304   1.074933   \n",
       "90624  30.749744  1255.164902  0.559097  ...   0.859456   1.011868   0.621184   \n",
       "90625   6.366867   300.971201  0.195906  ...   0.993834   0.973105   0.865627   \n",
       "\n",
       "       BOA_fraction  BOA_fraction_pred  alpha_pred     alpha      BOA_AI  \\\n",
       "0          0.914445           0.915612   -0.304591 -0.224256  915.611863   \n",
       "1          0.594575           0.594615    0.376599  0.376680  594.614685   \n",
       "2          0.284592           0.280167    0.258431  0.238902  280.167222   \n",
       "3          0.373719           0.372900    0.479201  0.471005  372.900099   \n",
       "4          0.832138           0.834354    0.119085  0.128247  834.353626   \n",
       "...             ...                ...         ...       ...         ...   \n",
       "90621      0.801588           0.800986    0.202855  0.194530  800.986350   \n",
       "90622      0.032089           0.036499   -0.142424 -0.142424   36.498807   \n",
       "90623      0.735627           0.734697    0.159697  0.149952  734.697282   \n",
       "90624      0.924379           0.922319    0.153491  0.143431  922.318637   \n",
       "90625      0.730441           0.730773    0.228026  0.229081  730.773032   \n",
       "\n",
       "         BOA_pred      BOA_RT  \n",
       "0      915.611863  914.444755  \n",
       "1      594.614685  594.575497  \n",
       "2      280.167222  284.591943  \n",
       "3      372.900099  373.718733  \n",
       "4      834.353626  832.137702  \n",
       "...           ...         ...  \n",
       "90621  800.986350  801.588284  \n",
       "90622   36.498807   32.089238  \n",
       "90623  734.697282  735.627029  \n",
       "90624  922.318637  924.379368  \n",
       "90625  730.773032  730.440816  \n",
       "\n",
       "[90626 rows x 25 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calculate_BOA(thetha,z,Tg_abs,Tg_scat,optical_depth,albedo,alpha):   \n",
    "    def muprime(z,h,µ):\n",
    "        RAYON_TERRESTRE=6371\n",
    "        eta = (RAYON_TERRESTRE*1000 + z) / h\n",
    "        root = (eta*µ)**2  + 2 * eta + 1\n",
    "        sum = (root)**0.5 - eta * µ\n",
    "        if sum > 0 :\n",
    "            return 1/sum\n",
    "        return 1 \n",
    "    Ha=2000\n",
    "    Hg=9000\n",
    "    angle_rad = math.radians(thetha)\n",
    "    µ=math.cos(angle_rad)  \n",
    "    Y_a=muprime(z,Ha,µ)\n",
    "    Y_g=muprime(z,Hg,µ)\n",
    "    Ma=math.exp(-z/Ha)/Y_a\n",
    "    Mg=math.exp(-z/Hg)/Y_g\n",
    "    delta_a_scat=optical_depth*albedo\n",
    "    delta_g_scat=-math.log(Tg_scat)\n",
    "    Ta_abs=math.exp(-optical_depth*(1-albedo))\n",
    "    numerator= 1000*(Tg_abs**Mg)*(Ta_abs**Ma)\n",
    "    denominator=1+alpha*delta_g_scat*Mg+(alpha*(1/3)*delta_a_scat)*Ma\n",
    "    BOA_ia= numerator / denominator\n",
    "    return BOA_ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alpha(BOA_fraction,Ta_abs,Tg_abs,mprime_g,mprime_a,GOD,AODS):   \n",
    "    numerator= (1/BOA_fraction)*(Tg_abs**(mprime_g))*(Ta_abs**(mprime_a))-1\n",
    "    denominator=(GOD * mprime_g) + (AODS * mprime_a) / 3\n",
    "    alpha= numerator / denominator\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_RT_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.915612</td>\n",
       "      <td>-0.304591</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>915.611863</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>914.444755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.594615</td>\n",
       "      <td>0.376599</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>594.614685</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>594.575497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280167</td>\n",
       "      <td>0.258431</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>280.167222</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>284.591943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>372.900099</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.718733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.834354</td>\n",
       "      <td>0.119085</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>834.353626</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>832.137702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800986</td>\n",
       "      <td>0.202855</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>800.986350</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>801.588284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>36.498807</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>32.089238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.734697</td>\n",
       "      <td>0.159697</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>734.697282</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>735.627029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.922319</td>\n",
       "      <td>0.153491</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>922.318637</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>924.379368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>0.228026</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>730.773032</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>730.440816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_a   mprime_g   mprime_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...   0.951721   0.684362   0.152645   \n",
       "1      61.931220  1112.943981  0.419185  ...   0.470791   1.873413   1.217586   \n",
       "2      76.068422    15.772549  0.729906  ...   0.241375   4.099723   4.110381   \n",
       "3      63.055806  4070.432067  0.251342  ...   0.453397   1.400187   0.288163   \n",
       "4       2.351437   624.459254  0.881197  ...   0.999158   0.933753   0.732430   \n",
       "...          ...          ...       ...  ...        ...        ...        ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...   0.962266   0.872219   0.472533   \n",
       "90622  88.522985  2102.182329  0.968884  ...   0.030856  18.672395  11.328534   \n",
       "90623  24.631558    46.223183  0.761757  ...   0.909037   1.094304   1.074933   \n",
       "90624  30.749744  1255.164902  0.559097  ...   0.859456   1.011868   0.621184   \n",
       "90625   6.366867   300.971201  0.195906  ...   0.993834   0.973105   0.865627   \n",
       "\n",
       "       BOA_fraction  BOA_fraction_pred  alpha_pred     alpha    BOA_pred  \\\n",
       "0          0.914445           0.915612   -0.304591 -0.224256  915.611863   \n",
       "1          0.594575           0.594615    0.376599  0.376680  594.614685   \n",
       "2          0.284592           0.280167    0.258431  0.238902  280.167222   \n",
       "3          0.373719           0.372900    0.479201  0.471005  372.900099   \n",
       "4          0.832138           0.834354    0.119085  0.128247  834.353626   \n",
       "...             ...                ...         ...       ...         ...   \n",
       "90621      0.801588           0.800986    0.202855  0.194530  800.986350   \n",
       "90622      0.032089           0.036499   -0.142424 -0.142424   36.498807   \n",
       "90623      0.735627           0.734697    0.159697  0.149952  734.697282   \n",
       "90624      0.924379           0.922319    0.153491  0.143431  922.318637   \n",
       "90625      0.730441           0.730773    0.228026  0.229081  730.773032   \n",
       "\n",
       "           BOA_RT  BOA_RT_test  \n",
       "0      914.444755   914.444755  \n",
       "1      594.575497   594.575497  \n",
       "2      284.591943   284.591943  \n",
       "3      373.718733   373.718733  \n",
       "4      832.137702   832.137702  \n",
       "...           ...          ...  \n",
       "90621  801.588284   801.588284  \n",
       "90622   32.089238    32.089238  \n",
       "90623  735.627029   735.627029  \n",
       "90624  924.379368   924.379368  \n",
       "90625  730.440816   730.440816  \n",
       "\n",
       "[90626 rows x 25 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real['alpha_pred']=X_test_real.apply(\n",
    "    lambda row: calculate_alpha(\n",
    "        row['BOA_fraction_pred'],\n",
    "        row['Ta_abs'],\n",
    "        row['Tg_abs'],\n",
    "        row['mprime_g'],\n",
    "        row['mprime_a'],\n",
    "        row['GOD'],\n",
    "        row['AODS'],\n",
    "    ), axis=1\n",
    ")\n",
    "X_test_real['alpha']=X_test_real.apply(\n",
    "    lambda row: calculate_alpha(\n",
    "        row['BOA_fraction'],\n",
    "        row['Ta_abs'],\n",
    "        row['Tg_abs'],\n",
    "        row['mprime_g'],\n",
    "        row['mprime_a'],\n",
    "        row['GOD'],\n",
    "        row['AODS'],\n",
    "    ), axis=1\n",
    ")\n",
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_RT_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.915612</td>\n",
       "      <td>-0.304591</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>915.611863</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>914.444755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.594615</td>\n",
       "      <td>0.376599</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>594.614685</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>594.575497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280167</td>\n",
       "      <td>0.258431</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>280.167222</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>284.591943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>372.900099</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.718733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.834354</td>\n",
       "      <td>0.119085</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>834.353626</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>832.137702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800986</td>\n",
       "      <td>0.202855</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>800.986350</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>801.588284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>36.498807</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>32.089238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.734697</td>\n",
       "      <td>0.159697</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>734.697282</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>735.627029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.922319</td>\n",
       "      <td>0.153491</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>922.318637</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>924.379368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>0.228026</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>730.773032</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>730.440816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_a   mprime_g   mprime_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...   0.951721   0.684362   0.152645   \n",
       "1      61.931220  1112.943981  0.419185  ...   0.470791   1.873413   1.217586   \n",
       "2      76.068422    15.772549  0.729906  ...   0.241375   4.099723   4.110381   \n",
       "3      63.055806  4070.432067  0.251342  ...   0.453397   1.400187   0.288163   \n",
       "4       2.351437   624.459254  0.881197  ...   0.999158   0.933753   0.732430   \n",
       "...          ...          ...       ...  ...        ...        ...        ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...   0.962266   0.872219   0.472533   \n",
       "90622  88.522985  2102.182329  0.968884  ...   0.030856  18.672395  11.328534   \n",
       "90623  24.631558    46.223183  0.761757  ...   0.909037   1.094304   1.074933   \n",
       "90624  30.749744  1255.164902  0.559097  ...   0.859456   1.011868   0.621184   \n",
       "90625   6.366867   300.971201  0.195906  ...   0.993834   0.973105   0.865627   \n",
       "\n",
       "       BOA_fraction  BOA_fraction_pred  alpha_pred     alpha    BOA_pred  \\\n",
       "0          0.914445           0.915612   -0.304591 -0.224256  915.611863   \n",
       "1          0.594575           0.594615    0.376599  0.376680  594.614685   \n",
       "2          0.284592           0.280167    0.258431  0.238902  280.167222   \n",
       "3          0.373719           0.372900    0.479201  0.471005  372.900099   \n",
       "4          0.832138           0.834354    0.119085  0.128247  834.353626   \n",
       "...             ...                ...         ...       ...         ...   \n",
       "90621      0.801588           0.800986    0.202855  0.194530  800.986350   \n",
       "90622      0.032089           0.036499   -0.142424 -0.142424   36.498807   \n",
       "90623      0.735627           0.734697    0.159697  0.149952  734.697282   \n",
       "90624      0.924379           0.922319    0.153491  0.143431  922.318637   \n",
       "90625      0.730441           0.730773    0.228026  0.229081  730.773032   \n",
       "\n",
       "           BOA_RT  BOA_RT_test  \n",
       "0      914.444755   914.444755  \n",
       "1      594.575497   594.575497  \n",
       "2      284.591943   284.591943  \n",
       "3      373.718733   373.718733  \n",
       "4      832.137702   832.137702  \n",
       "...           ...          ...  \n",
       "90621  801.588284   801.588284  \n",
       "90622   32.089238    32.089238  \n",
       "90623  735.627029   735.627029  \n",
       "90624  924.379368   924.379368  \n",
       "90625  730.440816   730.440816  \n",
       "\n",
       "[90626 rows x 25 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real['BOA_pred'] = X_test_real.apply(lambda row: calculate_BOA(row['SZA'], row['Z'],row['Tg_abs'],row['Tg_scat'],row['AOD'],row['SSA'],row['alpha_pred']), axis=1)\n",
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_BOA_IA(Tg_abs,Mg,Ta_abs,Ma,alpha,delta_g_scat,AODS):\n",
    "    numerator= 1000*(Tg_abs**Mg)*(Ta_abs**Ma)\n",
    "    denominator=1+alpha*delta_g_scat*Mg+(alpha*(1/3)*AODS)*Ma\n",
    "    \n",
    "    BOA_ia= numerator / denominator\n",
    "    return BOA_ia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real['BOA_pred'] = X_test_real.apply(lambda row: calculate_BOA_IA(row['Tg_abs'], row['mprime_g'],row['Ta_abs'],row['mprime_a'],row['alpha_pred'],row['GOD'],row['AODS']), axis=1)\n",
    "X_test_real['BOA_RT']=1000*X_test_real['BOA_fraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real['BOA_RT_test'] = X_test_real.apply(lambda row: calculate_BOA_IA(row['Tg_abs'], row['mprime_g'],row['Ta_abs'],row['mprime_a'],row['alpha'],row['GOD'],row['AODS']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_RT_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.915612</td>\n",
       "      <td>-0.304591</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>915.611863</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>914.444755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.594615</td>\n",
       "      <td>0.376599</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>594.614685</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>594.575497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.280167</td>\n",
       "      <td>0.258431</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>280.167222</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>284.591943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>372.900099</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.718733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.834354</td>\n",
       "      <td>0.119085</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>834.353626</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>832.137702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90621</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.800986</td>\n",
       "      <td>0.202855</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>800.986350</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>801.588284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90622</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>36.498807</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>32.089238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90623</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.734697</td>\n",
       "      <td>0.159697</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>734.697282</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>735.627029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.922319</td>\n",
       "      <td>0.153491</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>922.318637</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>924.379368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90625</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>0.228026</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>730.773032</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>730.440816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0      0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "1      0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "2      0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "3      0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "4      0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "90621  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "90622  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "90623  0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "90624  0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "90625  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_a   mprime_g   mprime_a  \\\n",
       "0      17.879232  3858.242750  0.894296  ...   0.951721   0.684362   0.152645   \n",
       "1      61.931220  1112.943981  0.419185  ...   0.470791   1.873413   1.217586   \n",
       "2      76.068422    15.772549  0.729906  ...   0.241375   4.099723   4.110381   \n",
       "3      63.055806  4070.432067  0.251342  ...   0.453397   1.400187   0.288163   \n",
       "4       2.351437   624.459254  0.881197  ...   0.999158   0.933753   0.732430   \n",
       "...          ...          ...       ...  ...        ...        ...        ...   \n",
       "90621  15.792506  1576.224940  0.696585  ...   0.962266   0.872219   0.472533   \n",
       "90622  88.522985  2102.182329  0.968884  ...   0.030856  18.672395  11.328534   \n",
       "90623  24.631558    46.223183  0.761757  ...   0.909037   1.094304   1.074933   \n",
       "90624  30.749744  1255.164902  0.559097  ...   0.859456   1.011868   0.621184   \n",
       "90625   6.366867   300.971201  0.195906  ...   0.993834   0.973105   0.865627   \n",
       "\n",
       "       BOA_fraction  BOA_fraction_pred  alpha_pred     alpha    BOA_pred  \\\n",
       "0          0.914445           0.915612   -0.304591 -0.224256  915.611863   \n",
       "1          0.594575           0.594615    0.376599  0.376680  594.614685   \n",
       "2          0.284592           0.280167    0.258431  0.238902  280.167222   \n",
       "3          0.373719           0.372900    0.479201  0.471005  372.900099   \n",
       "4          0.832138           0.834354    0.119085  0.128247  834.353626   \n",
       "...             ...                ...         ...       ...         ...   \n",
       "90621      0.801588           0.800986    0.202855  0.194530  800.986350   \n",
       "90622      0.032089           0.036499   -0.142424 -0.142424   36.498807   \n",
       "90623      0.735627           0.734697    0.159697  0.149952  734.697282   \n",
       "90624      0.924379           0.922319    0.153491  0.143431  922.318637   \n",
       "90625      0.730441           0.730773    0.228026  0.229081  730.773032   \n",
       "\n",
       "           BOA_RT  BOA_RT_test  \n",
       "0      914.444755   914.444755  \n",
       "1      594.575497   594.575497  \n",
       "2      284.591943   284.591943  \n",
       "3      373.718733   373.718733  \n",
       "4      832.137702   832.137702  \n",
       "...           ...          ...  \n",
       "90621  801.588284   801.588284  \n",
       "90622   32.089238    32.089238  \n",
       "90623  735.627029   735.627029  \n",
       "90624  924.379368   924.379368  \n",
       "90625  730.440816   730.440816  \n",
       "\n",
       "[90626 rows x 25 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Erreur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04841154997819658"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae=mean_absolute_error(X_test_real['alpha'],X_test_real['alpha_pred'])\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E_BOA Erreur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mae=\u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_real\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBOA_RT\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test_real\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBOA_pred\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m mae\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/sklearn/metrics/_regression.py:277\u001b[39m, in \u001b[36mmean_absolute_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[32m    223\u001b[39m \n\u001b[32m    224\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    272\u001b[39m \u001b[33;03m0.85...\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    274\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    276\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m )\n\u001b[32m    282\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    284\u001b[39m output_errors = _average(\n\u001b[32m    285\u001b[39m     xp.abs(y_pred - y_true), weights=sample_weight, axis=\u001b[32m0\u001b[39m, xp=xp\n\u001b[32m    286\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/sklearn/metrics/_regression.py:198\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[33;03mregression task.\u001b[39;00m\n\u001b[32m    150\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m y_type, y_true, y_pred, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/sklearn/metrics/_regression.py:106\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, multioutput, dtype, xp)\u001b[39m\n\u001b[32m    104\u001b[39m check_consistent_length(y_true, y_pred)\n\u001b[32m    105\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m y_pred = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_true.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    109\u001b[39m     y_true = xp.reshape(y_true, (-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/sklearn/utils/validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae=mean_absolute_error(X_test_real['BOA_RT'],X_test_real['BOA_pred'])\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOA_RT contient Inf : False\n",
      "BOA_pred contient Inf : True\n",
      "BOA_RT contient NaN : False\n",
      "BOA_pred contient NaN : False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vérifie les infinis\n",
    "print(\"BOA_RT contient Inf :\", np.isinf(X_test_real['BOA_RT']).any())\n",
    "print(\"BOA_pred contient Inf :\", np.isinf(X_test_real['BOA_pred']).any())\n",
    "\n",
    "# Vérifie les NaNs\n",
    "print(\"BOA_RT contient NaN :\", X_test_real['BOA_RT'].isna().any())\n",
    "print(\"BOA_pred contient NaN :\", X_test_real['BOA_pred'].isna().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_BOA_IA(Tg_abs,Mg,Ta_abs,Ma,alpha,delta_g_scat,AODS):\n",
    "    numerator= 1000*(Tg_abs**Mg)*(Ta_abs**Ma)\n",
    "    denominator=1+alpha*delta_g_scat*Mg+(alpha*(1/3)*AODS)*Ma\n",
    "    \n",
    "    BOA_ia= numerator / denominator\n",
    "    return BOA_ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes avec Inf dans BOA_pred :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_pred</th>\n",
       "      <th>BOA_RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>0.877229</td>\n",
       "      <td>0.152220</td>\n",
       "      <td>0.831589</td>\n",
       "      <td>0.852392</td>\n",
       "      <td>0.130988</td>\n",
       "      <td>1.249368</td>\n",
       "      <td>1.064951</td>\n",
       "      <td>89.985541</td>\n",
       "      <td>2877.306578</td>\n",
       "      <td>0.876668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>0.012651</td>\n",
       "      <td>27.217166</td>\n",
       "      <td>18.752627</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.097828</td>\n",
       "      <td>-0.097828</td>\n",
       "      <td>inf</td>\n",
       "      <td>2.612085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8289</th>\n",
       "      <td>0.975981</td>\n",
       "      <td>0.272728</td>\n",
       "      <td>0.861939</td>\n",
       "      <td>0.748362</td>\n",
       "      <td>0.024312</td>\n",
       "      <td>0.590417</td>\n",
       "      <td>0.441845</td>\n",
       "      <td>89.423959</td>\n",
       "      <td>513.089170</td>\n",
       "      <td>0.777840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032060</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>29.462709</td>\n",
       "      <td>41.769395</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>-0.020888</td>\n",
       "      <td>-0.145599</td>\n",
       "      <td>-0.145599</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.808511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17860</th>\n",
       "      <td>0.956120</td>\n",
       "      <td>0.467668</td>\n",
       "      <td>0.636342</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.817985</td>\n",
       "      <td>0.365966</td>\n",
       "      <td>89.367327</td>\n",
       "      <td>222.750242</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032651</td>\n",
       "      <td>0.019210</td>\n",
       "      <td>29.878344</td>\n",
       "      <td>46.570483</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>-0.012042</td>\n",
       "      <td>-0.142414</td>\n",
       "      <td>-0.142414</td>\n",
       "      <td>inf</td>\n",
       "      <td>3.245207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28819</th>\n",
       "      <td>0.757926</td>\n",
       "      <td>0.091870</td>\n",
       "      <td>0.873886</td>\n",
       "      <td>0.772973</td>\n",
       "      <td>0.277170</td>\n",
       "      <td>0.593786</td>\n",
       "      <td>0.458981</td>\n",
       "      <td>89.614225</td>\n",
       "      <td>45.818359</td>\n",
       "      <td>0.384029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030144</td>\n",
       "      <td>0.016338</td>\n",
       "      <td>33.006107</td>\n",
       "      <td>59.821935</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>-0.014707</td>\n",
       "      <td>-0.054643</td>\n",
       "      <td>-0.054643</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.754105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29336</th>\n",
       "      <td>0.791410</td>\n",
       "      <td>0.491256</td>\n",
       "      <td>0.592520</td>\n",
       "      <td>0.434791</td>\n",
       "      <td>0.233939</td>\n",
       "      <td>0.925977</td>\n",
       "      <td>0.402607</td>\n",
       "      <td>89.669515</td>\n",
       "      <td>625.046765</td>\n",
       "      <td>0.175599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029604</td>\n",
       "      <td>0.015738</td>\n",
       "      <td>31.512932</td>\n",
       "      <td>46.486176</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>-0.073472</td>\n",
       "      <td>-0.073472</td>\n",
       "      <td>inf</td>\n",
       "      <td>15.495546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32822</th>\n",
       "      <td>0.862867</td>\n",
       "      <td>0.433229</td>\n",
       "      <td>0.858311</td>\n",
       "      <td>0.249329</td>\n",
       "      <td>0.147494</td>\n",
       "      <td>0.203537</td>\n",
       "      <td>0.050748</td>\n",
       "      <td>89.915112</td>\n",
       "      <td>62.657872</td>\n",
       "      <td>0.714013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027318</td>\n",
       "      <td>0.013290</td>\n",
       "      <td>36.352136</td>\n",
       "      <td>72.924195</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>-0.151623</td>\n",
       "      <td>-0.151623</td>\n",
       "      <td>inf</td>\n",
       "      <td>11.156664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34028</th>\n",
       "      <td>0.913785</td>\n",
       "      <td>0.233169</td>\n",
       "      <td>0.953676</td>\n",
       "      <td>0.632550</td>\n",
       "      <td>0.090160</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>0.081651</td>\n",
       "      <td>89.867727</td>\n",
       "      <td>177.226526</td>\n",
       "      <td>0.019382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027746</td>\n",
       "      <td>0.013734</td>\n",
       "      <td>35.339028</td>\n",
       "      <td>66.635415</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>-0.018445</td>\n",
       "      <td>-0.200009</td>\n",
       "      <td>-0.200009</td>\n",
       "      <td>inf</td>\n",
       "      <td>2.141626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35288</th>\n",
       "      <td>0.365234</td>\n",
       "      <td>0.202715</td>\n",
       "      <td>0.751287</td>\n",
       "      <td>0.651047</td>\n",
       "      <td>1.007217</td>\n",
       "      <td>0.819501</td>\n",
       "      <td>0.533533</td>\n",
       "      <td>89.757370</td>\n",
       "      <td>484.501151</td>\n",
       "      <td>0.540442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028766</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>32.940904</td>\n",
       "      <td>52.953818</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>-0.001282</td>\n",
       "      <td>-0.023476</td>\n",
       "      <td>-0.023476</td>\n",
       "      <td>inf</td>\n",
       "      <td>5.638409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50363</th>\n",
       "      <td>0.613289</td>\n",
       "      <td>0.312638</td>\n",
       "      <td>0.627556</td>\n",
       "      <td>0.499441</td>\n",
       "      <td>0.488918</td>\n",
       "      <td>0.930805</td>\n",
       "      <td>0.464883</td>\n",
       "      <td>88.766153</td>\n",
       "      <td>19.530294</td>\n",
       "      <td>0.299897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039422</td>\n",
       "      <td>0.027282</td>\n",
       "      <td>25.311487</td>\n",
       "      <td>36.297997</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>-0.055555</td>\n",
       "      <td>-0.055555</td>\n",
       "      <td>inf</td>\n",
       "      <td>6.617930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61655</th>\n",
       "      <td>0.969413</td>\n",
       "      <td>0.103506</td>\n",
       "      <td>0.966317</td>\n",
       "      <td>0.919750</td>\n",
       "      <td>0.031065</td>\n",
       "      <td>0.426964</td>\n",
       "      <td>0.392701</td>\n",
       "      <td>89.929943</td>\n",
       "      <td>241.982808</td>\n",
       "      <td>0.792710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027185</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>35.809323</td>\n",
       "      <td>67.362341</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.034433</td>\n",
       "      <td>-0.100703</td>\n",
       "      <td>-0.100703</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.163120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66383</th>\n",
       "      <td>0.611993</td>\n",
       "      <td>0.402104</td>\n",
       "      <td>0.791268</td>\n",
       "      <td>0.361549</td>\n",
       "      <td>0.491034</td>\n",
       "      <td>0.366698</td>\n",
       "      <td>0.132580</td>\n",
       "      <td>89.680157</td>\n",
       "      <td>250.576852</td>\n",
       "      <td>0.343685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029502</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>32.965224</td>\n",
       "      <td>56.463624</td>\n",
       "      <td>0.017477</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>-0.053526</td>\n",
       "      <td>-0.053526</td>\n",
       "      <td>inf</td>\n",
       "      <td>17.476563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67891</th>\n",
       "      <td>0.807391</td>\n",
       "      <td>0.219937</td>\n",
       "      <td>0.841507</td>\n",
       "      <td>0.724592</td>\n",
       "      <td>0.213947</td>\n",
       "      <td>0.626564</td>\n",
       "      <td>0.454003</td>\n",
       "      <td>89.622377</td>\n",
       "      <td>468.301969</td>\n",
       "      <td>0.790715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030063</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>31.577154</td>\n",
       "      <td>48.698200</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>-0.070794</td>\n",
       "      <td>-0.070794</td>\n",
       "      <td>inf</td>\n",
       "      <td>3.592250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "2379   0.877229  0.152220  0.831589  0.852392  0.130988  1.249368  1.064951   \n",
       "8289   0.975981  0.272728  0.861939  0.748362  0.024312  0.590417  0.441845   \n",
       "17860  0.956120  0.467668  0.636342  0.447400  0.044872  0.817985  0.365966   \n",
       "28819  0.757926  0.091870  0.873886  0.772973  0.277170  0.593786  0.458981   \n",
       "29336  0.791410  0.491256  0.592520  0.434791  0.233939  0.925977  0.402607   \n",
       "32822  0.862867  0.433229  0.858311  0.249329  0.147494  0.203537  0.050748   \n",
       "34028  0.913785  0.233169  0.953676  0.632550  0.090160  0.129082  0.081651   \n",
       "35288  0.365234  0.202715  0.751287  0.651047  1.007217  0.819501  0.533533   \n",
       "50363  0.613289  0.312638  0.627556  0.499441  0.488918  0.930805  0.464883   \n",
       "61655  0.969413  0.103506  0.966317  0.919750  0.031065  0.426964  0.392701   \n",
       "66383  0.611993  0.402104  0.791268  0.361549  0.491034  0.366698  0.132580   \n",
       "67891  0.807391  0.219937  0.841507  0.724592  0.213947  0.626564  0.454003   \n",
       "\n",
       "             SZA            Z  R_scence  ...  muprime_g  muprime_a   mprime_g  \\\n",
       "2379   89.985541  2877.306578  0.876668  ...   0.026688   0.012651  27.217166   \n",
       "8289   89.423959   513.089170  0.777840  ...   0.032060   0.018524  29.462709   \n",
       "17860  89.367327   222.750242  0.010789  ...   0.032651   0.019210  29.878344   \n",
       "28819  89.614225    45.818359  0.384029  ...   0.030144   0.016338  33.006107   \n",
       "29336  89.669515   625.046765  0.175599  ...   0.029604   0.015738  31.512932   \n",
       "32822  89.915112    62.657872  0.714013  ...   0.027318   0.013290  36.352136   \n",
       "34028  89.867727   177.226526  0.019382  ...   0.027746   0.013734  35.339028   \n",
       "35288  89.757370   484.501151  0.540442  ...   0.028766   0.014822  32.940904   \n",
       "50363  88.766153    19.530294  0.299897  ...   0.039422   0.027282  25.311487   \n",
       "61655  89.929943   241.982808  0.792710  ...   0.027185   0.013153  35.809323   \n",
       "66383  89.680157   250.576852  0.343685  ...   0.029502   0.015625  32.965224   \n",
       "67891  89.622377   468.301969  0.790715  ...   0.030063   0.016248  31.577154   \n",
       "\n",
       "        mprime_a  BOA_fraction  BOA_fraction_pred  alpha_pred     alpha  \\\n",
       "2379   18.752627      0.002612          -0.013425   -0.097828 -0.097828   \n",
       "8289   41.769395      0.000809          -0.020888   -0.145599 -0.145599   \n",
       "17860  46.570483      0.003245          -0.012042   -0.142414 -0.142414   \n",
       "28819  59.821935      0.000754          -0.014707   -0.054643 -0.054643   \n",
       "29336  46.486176      0.015496           0.023010   -0.073472 -0.073472   \n",
       "32822  72.924195      0.011157           0.011503   -0.151623 -0.151623   \n",
       "34028  66.635415      0.002142          -0.018445   -0.200009 -0.200009   \n",
       "35288  52.953818      0.005638          -0.001282   -0.023476 -0.023476   \n",
       "50363  36.297997      0.006618           0.000214   -0.055555 -0.055555   \n",
       "61655  67.362341      0.000163          -0.034433   -0.100703 -0.100703   \n",
       "66383  56.463624      0.017477           0.025441   -0.053526 -0.053526   \n",
       "67891  48.698200      0.003592          -0.001315   -0.070794 -0.070794   \n",
       "\n",
       "       BOA_pred     BOA_RT  \n",
       "2379        inf   2.612085  \n",
       "8289        inf   0.808511  \n",
       "17860       inf   3.245207  \n",
       "28819       inf   0.754105  \n",
       "29336       inf  15.495546  \n",
       "32822       inf  11.156664  \n",
       "34028       inf   2.141626  \n",
       "35288       inf   5.638409  \n",
       "50363       inf   6.617930  \n",
       "61655       inf   0.163120  \n",
       "66383       inf  17.476563  \n",
       "67891       inf   3.592250  \n",
       "\n",
       "[12 rows x 24 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infs = X_test_real[np.isinf(X_test_real['BOA_pred'])]\n",
    "print(\"Lignes avec Inf dans BOA_pred :\")\n",
    "infs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>alpha_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159680</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>3187.429121</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>0.896171</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>1.060831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347342</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.056472</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>0.601568</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.362287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307069</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.507886</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>0.290517</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.213682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126659</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>3187.535216</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>0.380374</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.405693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356896</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.812230</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>0.829379</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.139722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326002</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.288112</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>0.798049</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.243669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190999</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.551091</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>0.092493</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>-0.142425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27703</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.523112</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>0.732899</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.178618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.127582</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>0.906780</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.230817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189606</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.650486</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>0.729445</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.232249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "159680  0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "347342  0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "307069  0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "126659  0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "356896  0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "326002  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "190999  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "27703   0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "76624   0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "189606  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "              SZA            Z  R_scence  ...         mu_a  muprime_g  \\\n",
       "159680  17.879232  3858.242750  0.894296  ...  3187.429121   0.951776   \n",
       "347342  61.931220  1112.943981  0.419185  ...  3186.056472   0.471695   \n",
       "307069  76.068422    15.772549  0.729906  ...  3185.507886   0.243492   \n",
       "126659  63.055806  4070.432067  0.251342  ...  3187.535216   0.454355   \n",
       "356896   2.351437   624.459254  0.881197  ...  3185.812230   0.999159   \n",
       "...           ...          ...       ...  ...          ...        ...   \n",
       "326002  15.792506  1576.224940  0.696585  ...  3186.288112   0.962308   \n",
       "190999  88.522985  2102.182329  0.968884  ...  3186.551091   0.042399   \n",
       "27703   24.631558    46.223183  0.761757  ...  3185.523112   0.909141   \n",
       "76624   30.749744  1255.164902  0.559097  ...  3186.127582   0.859623   \n",
       "189606   6.366867   300.971201  0.195906  ...  3185.650486   0.993841   \n",
       "\n",
       "        muprime_a   mprime_g   mprime_a      BOA_RT     alpha  \\\n",
       "159680   0.951721   0.684362   0.152645  914.444755 -0.224256   \n",
       "347342   0.470791   1.873413   1.217586  594.575497  0.376680   \n",
       "307069   0.241375   4.099723   4.110381  284.591943  0.238902   \n",
       "126659   0.453397   1.400187   0.288163  373.718733  0.471005   \n",
       "356896   0.999158   0.933753   0.732430  832.137702  0.128247   \n",
       "...           ...        ...        ...         ...       ...   \n",
       "326002   0.962266   0.872219   0.472533  801.588284  0.194530   \n",
       "190999   0.030856  18.672395  11.328534   32.089238 -0.142424   \n",
       "27703    0.909037   1.094304   1.074933  735.627029  0.149952   \n",
       "76624    0.859456   1.011868   0.621184  924.379368  0.143431   \n",
       "189606   0.993834   0.973105   0.865627  730.440816  0.229081   \n",
       "\n",
       "        BOA_fraction_pred  BOA_fraction  alpha_pred  \n",
       "159680           0.896171      0.914445    1.060831  \n",
       "347342           0.601568      0.594575    0.362287  \n",
       "307069           0.290517      0.284592    0.213682  \n",
       "126659           0.380374      0.373719    0.405693  \n",
       "356896           0.829379      0.832138    0.139722  \n",
       "...                   ...           ...         ...  \n",
       "326002           0.798049      0.801588    0.243669  \n",
       "190999           0.092493      0.032089   -0.142425  \n",
       "27703            0.732899      0.735627    0.178618  \n",
       "76624            0.906780      0.924379    0.230817  \n",
       "189606           0.729445      0.730441    0.232249  \n",
       "\n",
       "[90626 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['alpha_pred']=X_test.apply(\n",
    "    lambda row: calculate_alpha(\n",
    "        row['BOA_fraction_pred'],\n",
    "        row['Ta_abs'],\n",
    "        row['Tg_abs'],\n",
    "        row['mprime_g'],\n",
    "        row['mprime_a'],\n",
    "        row['GOD'],\n",
    "        row['AODS'],\n",
    "    ), axis=1\n",
    ")\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_fraction_pred</th>\n",
       "      <th>BOA_fraction</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>BOA_RT_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159680</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>0.896171</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>1.060831</td>\n",
       "      <td>896.171387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347342</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>0.601568</td>\n",
       "      <td>0.594575</td>\n",
       "      <td>0.362287</td>\n",
       "      <td>601.568298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307069</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>0.290517</td>\n",
       "      <td>0.284592</td>\n",
       "      <td>0.213682</td>\n",
       "      <td>290.516998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126659</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>0.380374</td>\n",
       "      <td>0.373719</td>\n",
       "      <td>0.405693</td>\n",
       "      <td>380.373718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356896</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>0.829379</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>0.139722</td>\n",
       "      <td>829.379211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326002</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>0.798049</td>\n",
       "      <td>0.801588</td>\n",
       "      <td>0.243669</td>\n",
       "      <td>798.048645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190999</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>0.092493</td>\n",
       "      <td>0.032089</td>\n",
       "      <td>-0.142425</td>\n",
       "      <td>92.492775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27703</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>0.732899</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>0.178618</td>\n",
       "      <td>732.898804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>0.906780</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.230817</td>\n",
       "      <td>906.780029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189606</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>0.729445</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>0.232249</td>\n",
       "      <td>729.445312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "159680  0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "347342  0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "307069  0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "126659  0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "356896  0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "326002  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "190999  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "27703   0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "76624   0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "189606  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "              SZA            Z  R_scence  ...  muprime_g  muprime_a  \\\n",
       "159680  17.879232  3858.242750  0.894296  ...   0.951776   0.951721   \n",
       "347342  61.931220  1112.943981  0.419185  ...   0.471695   0.470791   \n",
       "307069  76.068422    15.772549  0.729906  ...   0.243492   0.241375   \n",
       "126659  63.055806  4070.432067  0.251342  ...   0.454355   0.453397   \n",
       "356896   2.351437   624.459254  0.881197  ...   0.999159   0.999158   \n",
       "...           ...          ...       ...  ...        ...        ...   \n",
       "326002  15.792506  1576.224940  0.696585  ...   0.962308   0.962266   \n",
       "190999  88.522985  2102.182329  0.968884  ...   0.042399   0.030856   \n",
       "27703   24.631558    46.223183  0.761757  ...   0.909141   0.909037   \n",
       "76624   30.749744  1255.164902  0.559097  ...   0.859623   0.859456   \n",
       "189606   6.366867   300.971201  0.195906  ...   0.993841   0.993834   \n",
       "\n",
       "         mprime_g   mprime_a      BOA_RT     alpha  BOA_fraction_pred  \\\n",
       "159680   0.684362   0.152645  914.444755 -0.224256           0.896171   \n",
       "347342   1.873413   1.217586  594.575497  0.376680           0.601568   \n",
       "307069   4.099723   4.110381  284.591943  0.238902           0.290517   \n",
       "126659   1.400187   0.288163  373.718733  0.471005           0.380374   \n",
       "356896   0.933753   0.732430  832.137702  0.128247           0.829379   \n",
       "...           ...        ...         ...       ...                ...   \n",
       "326002   0.872219   0.472533  801.588284  0.194530           0.798049   \n",
       "190999  18.672395  11.328534   32.089238 -0.142424           0.092493   \n",
       "27703    1.094304   1.074933  735.627029  0.149952           0.732899   \n",
       "76624    1.011868   0.621184  924.379368  0.143431           0.906780   \n",
       "189606   0.973105   0.865627  730.440816  0.229081           0.729445   \n",
       "\n",
       "        BOA_fraction  alpha_pred  BOA_RT_pred  \n",
       "159680      0.914445    1.060831   896.171387  \n",
       "347342      0.594575    0.362287   601.568298  \n",
       "307069      0.284592    0.213682   290.516998  \n",
       "126659      0.373719    0.405693   380.373718  \n",
       "356896      0.832138    0.139722   829.379211  \n",
       "...              ...         ...          ...  \n",
       "326002      0.801588    0.243669   798.048645  \n",
       "190999      0.032089   -0.142425    92.492775  \n",
       "27703       0.735627    0.178618   732.898804  \n",
       "76624       0.924379    0.230817   906.780029  \n",
       "189606      0.730441    0.232249   729.445312  \n",
       "\n",
       "[90626 rows x 24 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['BOA_RT_pred'] = X_test['BOA_fraction_pred'] * 1000\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.739169428433469"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mae = mean_absolute_error(X_test['BOA_RT'], X_test['BOA_RT_pred'])\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class TransmissionLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        alpha, m_g, m_a, dtau_gas_scat, dtau_aero_scat, T_gas_abs, T_aero_abs = inputs\n",
    "\n",
    "        # Dénominateur (1 + α⋅Δτ_gas⋅m_g + α/3⋅Δτ_aero⋅m_a)\n",
    "        denom = 1 + alpha * dtau_gas_scat * m_g + (alpha / 3.0) * dtau_aero_scat * m_a\n",
    "\n",
    "        # Absorptions\n",
    "        T_abs = tf.pow(T_gas_abs, m_g) * tf.pow(T_aero_abs, m_a)\n",
    "\n",
    "        # E_BOA / E_TOA\n",
    "        E_BOA_div_TOA = (T_abs/ denom) \n",
    "        return E_BOA_div_TOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Entrée principale : 18 descripteurs\n",
    "features_input = Input(shape=(18,), name='features')\n",
    "\n",
    "# Autres entrées nécessaires à la formule analytique\n",
    "m_g_input = Input(shape=(1,), name='mprime_g')\n",
    "m_a_input = Input(shape=(1,), name='mprime_a')\n",
    "dtau_gas_scat_input = Input(shape=(1,), name='GOD')\n",
    "dtau_aero_scat_input = Input(shape=(1,), name='AODS')\n",
    "T_gas_abs_input = Input(shape=(1,), name='Tg_abs')\n",
    "T_aero_abs_input = Input(shape=(1,), name='Ta_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(232, activation='elu')(features_input)\n",
    "x = Dropout(0.111)(x)\n",
    "x = Dense(249, activation='elu')(x)\n",
    "x = Dropout(0.138)(x)\n",
    "x = Dense(174, activation='elu')(x)\n",
    "x = Dropout(0.136)(x)\n",
    "\n",
    "# Prédiction de alpha\n",
    "alpha_output = Dense(1, name='alpha')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_BOA_div_TOA = TransmissionLayer()([\n",
    "    alpha_output,\n",
    "    m_g_input,\n",
    "    m_a_input,\n",
    "    dtau_gas_scat_input,\n",
    "    dtau_aero_scat_input,\n",
    "    T_gas_abs_input,\n",
    "    T_aero_abs_input\n",
    "])\n",
    "model = Model(\n",
    "    inputs=[features_input, m_g_input, m_a_input, dtau_gas_scat_input, dtau_aero_scat_input, T_gas_abs_input, T_aero_abs_input],\n",
    "    outputs=E_BOA_div_TOA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00034),\n",
    "    loss='mae',  # ou 'mae' selon ce que tu veux\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1676 - mae: 0.1676\n",
      "Epoch 1: val_mae improved from inf to 0.04998, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.1675 - mae: 0.1675 - val_loss: 0.0500 - val_mae: 0.0500 - learning_rate: 3.4066e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0778 - mae: 0.0778\n",
      "Epoch 2: val_mae improved from 0.04998 to 0.04223, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0778 - mae: 0.0778 - val_loss: 0.0422 - val_mae: 0.0422 - learning_rate: 3.4066e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0638 - mae: 0.0638\n",
      "Epoch 3: val_mae improved from 0.04223 to 0.02865, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0638 - mae: 0.0638 - val_loss: 0.0287 - val_mae: 0.0287 - learning_rate: 3.4066e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0596 - mae: 0.0596\n",
      "Epoch 4: val_mae did not improve from 0.02865\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0596 - mae: 0.0596 - val_loss: 0.0325 - val_mae: 0.0325 - learning_rate: 3.4066e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m3390/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0574 - mae: 0.0574\n",
      "Epoch 5: val_mae did not improve from 0.02865\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0574 - mae: 0.0574 - val_loss: 0.0338 - val_mae: 0.0338 - learning_rate: 3.4066e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0551 - mae: 0.0551\n",
      "Epoch 6: val_mae did not improve from 0.02865\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0551 - mae: 0.0551 - val_loss: 0.0382 - val_mae: 0.0382 - learning_rate: 3.4066e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0546 - mae: 0.0546\n",
      "Epoch 7: val_mae improved from 0.02865 to 0.02631, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0546 - mae: 0.0546 - val_loss: 0.0263 - val_mae: 0.0263 - learning_rate: 3.4066e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0530 - mae: 0.0530\n",
      "Epoch 8: val_mae improved from 0.02631 to 0.02561, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0530 - mae: 0.0530 - val_loss: 0.0256 - val_mae: 0.0256 - learning_rate: 3.4066e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0524 - mae: 0.0524\n",
      "Epoch 9: val_mae improved from 0.02561 to 0.02558, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0524 - mae: 0.0524 - val_loss: 0.0256 - val_mae: 0.0256 - learning_rate: 3.4066e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0513 - mae: 0.0513\n",
      "Epoch 10: val_mae did not improve from 0.02558\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0513 - mae: 0.0513 - val_loss: 0.0276 - val_mae: 0.0276 - learning_rate: 3.4066e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0505 - mae: 0.0505\n",
      "Epoch 11: val_mae did not improve from 0.02558\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0505 - mae: 0.0505 - val_loss: 0.0310 - val_mae: 0.0310 - learning_rate: 3.4066e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0505 - mae: 0.0505\n",
      "Epoch 12: val_mae improved from 0.02558 to 0.02147, saving model to deep_model.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 0.0505 - mae: 0.0505 - val_loss: 0.0215 - val_mae: 0.0215 - learning_rate: 3.4066e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m   9/3399\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 0.0468 - mae: 0.0468  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     28\u001b[39m csv_logger = CSVLogger(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtraining_log_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Train phase :\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_mae',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Model checkpoint (save best model)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='deep_model.keras',\n",
    "    monitor='val_mae',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. CSV logger (log training history)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_logger = CSVLogger(f'training_log_{timestamp}.csv')\n",
    "\n",
    "# Train phase :\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_cb, csv_logger],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = data.iloc[:, :18].values\n",
    "\n",
    "# autres colonnes utilisées dans la couche analytique\n",
    "X_inputs = {\n",
    "    'features': X_features,\n",
    "    'mprime_g': data['mprime_g'].values.reshape(-1, 1),\n",
    "    'mprime_a': data['mprime_a'].values.reshape(-1, 1),\n",
    "    'GOD': data['GOD'].values.reshape(-1, 1),\n",
    "    'AODS': data['AODS'].values.reshape(-1, 1),\n",
    "    'Tg_abs': data['Tg_abs'].values.reshape(-1, 1),\n",
    "    'Ta_abs': data['Ta_abs'].values.reshape(-1, 1),\n",
    "}\n",
    "\n",
    "# y = valeur cible (E_BOA / E_TOA) → entre 0 et 1\n",
    "y = data['BOA_fraction'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_mae',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Model checkpoint (save best model)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='deep_model_1.keras',\n",
    "    monitor='val_mae',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. CSV logger (log training history)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_logger = CSVLogger(f'training_log_{timestamp}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4524/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5624 - mae: 0.5624\n",
      "Epoch 1: val_mae improved from inf to 0.55872, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5624 - mae: 0.5624 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m4523/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5572 - mae: 0.5572\n",
      "Epoch 2: val_mae improved from 0.55872 to 0.55869, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5572 - mae: 0.5572 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m4521/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5579 - mae: 0.5579\n",
      "Epoch 3: val_mae improved from 0.55869 to 0.55869, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5579 - mae: 0.5579 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m4525/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5572 - mae: 0.5572\n",
      "Epoch 4: val_mae improved from 0.55869 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5572 - mae: 0.5572 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5584 - mae: 0.5584\n",
      "Epoch 5: val_mae improved from 0.55868 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5584 - mae: 0.5584 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m4531/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5585 - mae: 0.5585\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00016999999934341758.\n",
      "\n",
      "Epoch 6: val_mae improved from 0.55868 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5585 - mae: 0.5585 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 3.4000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m4523/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5596 - mae: 0.5596\n",
      "Epoch 7: val_mae improved from 0.55868 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5596 - mae: 0.5596 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m4529/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 8: val_mae improved from 0.55868 to 0.55868, saving model to deep_model_1.keras\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 9: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m4530/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 10: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m4528/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5574 - mae: 0.5574\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 8.499999967170879e-05.\n",
      "\n",
      "Epoch 11: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5575 - mae: 0.5575 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 1.7000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5578 - mae: 0.5578\n",
      "Epoch 12: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5578 - mae: 0.5578 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 13: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5575 - mae: 0.5575\n",
      "Epoch 14: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5575 - mae: 0.5575 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5576 - mae: 0.5576\n",
      "Epoch 15: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.5576 - mae: 0.5576 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m4524/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5574 - mae: 0.5574\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.2499999835854396e-05.\n",
      "\n",
      "Epoch 16: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5574 - mae: 0.5574 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 8.5000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5580 - mae: 0.5580\n",
      "Epoch 17: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5580 - mae: 0.5580 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5574 - mae: 0.5574\n",
      "Epoch 18: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5574 - mae: 0.5574 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m4527/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5582 - mae: 0.5582\n",
      "Epoch 19: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m4531/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5580 - mae: 0.5580\n",
      "Epoch 20: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - loss: 0.5580 - mae: 0.5580 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5580 - mae: 0.5580\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 2.1249999917927198e-05.\n",
      "\n",
      "Epoch 21: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5580 - mae: 0.5580 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 4.2500e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m4526/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5585 - mae: 0.5585\n",
      "Epoch 22: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5585 - mae: 0.5585 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 2.1250e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m4529/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5571 - mae: 0.5571\n",
      "Epoch 23: val_mae did not improve from 0.55868\n",
      "\u001b[1m4532/4532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.5571 - mae: 0.5571 - val_loss: 0.5587 - val_mae: 0.5587 - learning_rate: 2.1250e-05\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7bd752273710>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_inputs,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_cb, csv_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_22\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_22\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,872</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,017</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">43,500</span> │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_8       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                     │                   │            │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)       │      \u001b[38;5;34m4,872\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)       │     \u001b[38;5;34m58,017\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │     \u001b[38;5;34m43,500\u001b[0m │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_8       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                     │                   │            │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_8[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,389</span> (415.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,389\u001b[0m (415.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,389</span> (415.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,389\u001b[0m (415.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Lambda\n",
    "\n",
    "# Chargement des paramètres optimaux\n",
    "with open(\"best_params.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Fonction d'activation personnalisée qui ne reçoit les 6 paramètres qu'en sortie\n",
    "def transmission_activation(inputs):\n",
    "    alpha, m_g, m_a, dtau_gas_scat, dtau_aero_scat, T_gas_abs, T_aero_abs = inputs  # Décomposer la liste d'inputs\n",
    "    \n",
    "    # Dénominateur\n",
    "    denom = 1 + alpha * dtau_gas_scat * m_g + (alpha / 3.0) * dtau_aero_scat * m_a\n",
    "    \n",
    "    # Absorption\n",
    "    T_abs = tf.pow(T_gas_abs, m_g) * tf.pow(T_aero_abs, m_a)\n",
    "    \n",
    "    # Transmission finale\n",
    "    return T_abs / denom\n",
    "\n",
    "# Définition des entrées complètes\n",
    "input_features = Input(shape=(X_train.shape[1],))  # Inclut toutes les colonnes pour l'entraînement\n",
    "\n",
    "# Couches cachées\n",
    "hidden = Dense(params[\"units1\"], activation=params[\"activation\"])(input_features)\n",
    "hidden = Dropout(params[\"dropout1\"])(hidden)\n",
    "\n",
    "hidden = Dense(params[\"units2\"], activation=params[\"activation\"])(hidden)\n",
    "hidden = Dropout(params[\"dropout2\"])(hidden)\n",
    "\n",
    "if params[\"n_layers\"] >= 3:\n",
    "    hidden = Dense(params[\"units3\"], activation=params[\"activation\"])(hidden)\n",
    "    hidden = Dropout(params[\"dropout3\"])(hidden)\n",
    "\n",
    "if params[\"n_layers\"] == 4:\n",
    "    hidden = Dense(params[\"units4\"], activation=params[\"activation\"])(hidden)\n",
    "    hidden = Dropout(params[\"dropout4\"])(hidden)\n",
    "\n",
    "# Transmission directe de la sortie avant activation\n",
    "alpha = Lambda(lambda x: x)(hidden)  # Passage direct sans transformation\n",
    "\n",
    "# Entrées des 6 paramètres après entraînement\n",
    "input_m_g = Input(shape=(1,))\n",
    "input_m_a = Input(shape=(1,))\n",
    "input_dtau_gas_scat = Input(shape=(1,))\n",
    "input_dtau_aero_scat = Input(shape=(1,))\n",
    "input_T_gas_abs = Input(shape=(1,))\n",
    "input_T_aero_abs = Input(shape=(1,))\n",
    "\n",
    "# Application de la fonction analytique sur la sortie après entraînement\n",
    "# Application de la fonction analytique sur la sortie\n",
    "output = Lambda(lambda inputs: transmission_activation(inputs), output_shape=(None, 1))(\n",
    "    [alpha, input_m_g, input_m_a, input_dtau_gas_scat, input_dtau_aero_scat, input_T_gas_abs, input_T_aero_abs]\n",
    ")\n",
    "\n",
    "# Définition du modèle final\n",
    "model = Model(inputs=[input_features, input_m_g, input_m_a, input_dtau_gas_scat, input_dtau_aero_scat, input_T_gas_abs, input_T_aero_abs], outputs=output)\n",
    "\n",
    "# Compilation du modèle\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n",
    "\n",
    "# Affichage du résumé du modèle\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ features            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,872</span> │ features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,017</span> │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">43,500</span> │ dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ alpha (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">175</span> │ dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ m_g (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ m_a (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dtau_gas_scat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dtau_aero_scat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ T_gas_abs           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ T_aero_abs          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ alpha[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ m_g[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                     │                   │            │ m_a[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                     │                   │            │ dtau_gas_scat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ dtau_aero_scat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ T_gas_abs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ T_aero_abs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ features            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)       │      \u001b[38;5;34m4,872\u001b[0m │ features[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)       │     \u001b[38;5;34m58,017\u001b[0m │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m249\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │     \u001b[38;5;34m43,500\u001b[0m │ dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ alpha (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m175\u001b[0m │ dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ m_g (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ m_a (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dtau_gas_scat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dtau_aero_scat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ T_gas_abs           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ T_aero_abs          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ alpha[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ m_g[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                     │                   │            │ m_a[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                     │                   │            │ dtau_gas_scat[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ dtau_aero_scat[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ T_gas_abs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ T_aero_abs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,564</span> (416.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,564\u001b[0m (416.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,564</span> (416.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,564\u001b[0m (416.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Lambda\n",
    "\n",
    "# Chargement des meilleurs hyperparamètres\n",
    "with open(\"best_params.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# ----------------------------\n",
    "# Fonction de transmission personnalisée\n",
    "# ----------------------------\n",
    "def transmission_activation(inputs):\n",
    "    alpha, m_g, m_a, dtau_gas_scat, dtau_aero_scat, T_gas_abs, T_aero_abs = inputs\n",
    "    \n",
    "    denom = 1 + alpha * dtau_gas_scat * m_g + (alpha / 3.0) * dtau_aero_scat * m_a\n",
    "    T_abs = tf.pow(T_gas_abs, m_g) * tf.pow(T_aero_abs, m_a)\n",
    "    \n",
    "    return T_abs / denom\n",
    "\n",
    "# ----------------------------\n",
    "# Définition des entrées\n",
    "# ----------------------------\n",
    "input_features = Input(shape=(X_train.shape[1],), name=\"features\")\n",
    "\n",
    "input_m_g = Input(shape=(1,), name=\"m_g\")\n",
    "input_m_a = Input(shape=(1,), name=\"m_a\")\n",
    "input_dtau_gas_scat = Input(shape=(1,), name=\"dtau_gas_scat\")\n",
    "input_dtau_aero_scat = Input(shape=(1,), name=\"dtau_aero_scat\")\n",
    "input_T_gas_abs = Input(shape=(1,), name=\"T_gas_abs\")\n",
    "input_T_aero_abs = Input(shape=(1,), name=\"T_aero_abs\")\n",
    "\n",
    "# ----------------------------\n",
    "# Réseau de neurones MLP\n",
    "# ----------------------------\n",
    "x = Dense(params[\"units1\"], activation=params[\"activation\"])(input_features)\n",
    "x = Dropout(params[\"dropout1\"])(x)\n",
    "\n",
    "x = Dense(params[\"units2\"], activation=params[\"activation\"])(x)\n",
    "x = Dropout(params[\"dropout2\"])(x)\n",
    "\n",
    "if params[\"n_layers\"] >= 3:\n",
    "    x = Dense(params[\"units3\"], activation=params[\"activation\"])(x)\n",
    "    x = Dropout(params[\"dropout3\"])(x)\n",
    "\n",
    "if params[\"n_layers\"] == 4:\n",
    "    x = Dense(params[\"units4\"], activation=params[\"activation\"])(x)\n",
    "    x = Dropout(params[\"dropout4\"])(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Dernier neurone = alpha\n",
    "# ----------------------------\n",
    "alpha = Dense(1, name=\"alpha\")(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Couche de sortie : transmission personnalisée\n",
    "# ----------------------------\n",
    "output = Lambda(lambda inputs: transmission_activation(inputs), output_shape=(1,))(\n",
    "    [alpha, input_m_g, input_m_a, input_dtau_gas_scat, input_dtau_aero_scat, input_T_gas_abs, input_T_aero_abs]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Modèle final\n",
    "# ----------------------------\n",
    "model = Model(\n",
    "    inputs=[\n",
    "        input_features,\n",
    "        input_m_g,\n",
    "        input_m_a,\n",
    "        input_dtau_gas_scat,\n",
    "        input_dtau_aero_scat,\n",
    "        input_T_gas_abs,\n",
    "        input_T_aero_abs\n",
    "    ],\n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "# Compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
    "model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n",
    "\n",
    "# Résumé\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362503,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "X_train=data.drop(columns=['BOA_fraction'])\n",
    "y_train=data['BOA_fraction']\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m_g = X_train[\"mprime_g\"].values.reshape(-1, 1)\n",
    "X_train_m_a = X_train[\"mprime_a\"].values.reshape(-1, 1)\n",
    "X_train_dtau_gas_scat = X_train[\"GOD\"].values.reshape(-1, 1)\n",
    "X_train_dtau_aero_scat = X_train[\"AODS\"].values.reshape(-1, 1)\n",
    "X_train_T_gas_abs = X_train[\"Tg_abs\"].values.reshape(-1, 1)\n",
    "X_train_T_aero_abs = X_train[\"Ta_abs\"].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11329/11329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 4ms/step - loss: 0.5633 - mae: 0.5633\n",
      "Epoch 2/100\n",
      "\u001b[1m11329/11329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 4ms/step - loss: 0.5579 - mae: 0.5579\n",
      "Epoch 3/100\n",
      "\u001b[1m11329/11329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 4ms/step - loss: 0.5578 - mae: 0.5578\n",
      "Epoch 4/100\n",
      "\u001b[1m11329/11329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 4ms/step - loss: 0.5583 - mae: 0.5583\n",
      "Epoch 5/100\n",
      "\u001b[1m 4660/11329\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 4ms/step - loss: 0.5584 - mae: 0.5584"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_m_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_m_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_dtau_gas_scat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_dtau_aero_scat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_T_gas_abs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_T_aero_abs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/myptd/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train, X_train_m_g, X_train_m_a, X_train_dtau_gas_scat, X_train_dtau_aero_scat, X_train_T_gas_abs, X_train_T_aero_abs], \n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myptd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
