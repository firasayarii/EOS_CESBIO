{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 22:49:39.965037: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-10 22:49:40.243300: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-10 22:49:40.503996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749588580.752465   32530 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749588580.823510   32530 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749588581.297039   32530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749588581.297072   32530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749588581.297076   32530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749588581.297080   32530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-10 22:49:41.351091: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV,KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error , mean_absolute_error\n",
    "from math import sqrt\n",
    "import joblib\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_no_outliers = pd.read_csv('/home/fayari/Stage/data_exploration_2/no_outliers.csv')\n",
    "new = pd.read_csv('/home/fayari/Stage/modeling_BOA/add_training_v2.csv')\n",
    "\n",
    "# Séparer les features et la cible\n",
    "y_add = new['alpha']\n",
    "X_add = new.drop(columns=['alpha'])\n",
    "\n",
    "# Paramètres\n",
    "n_copies = 650\n",
    "X_augmented = []\n",
    "y_augmented = []\n",
    "\n",
    "# Générer des copies bruitées\n",
    "for _ in range(n_copies):\n",
    "    noise = np.random.normal(loc=0.0, scale=0.01, size=X_add.shape)\n",
    "    X_aug = X_add.values + noise\n",
    "    X_augmented.append(X_aug)\n",
    "    y_augmented.append(y_add.values)\n",
    "\n",
    "# Empiler les données\n",
    "X_augmented = np.vstack(X_augmented)\n",
    "y_augmented = np.hstack(y_augmented)\n",
    "\n",
    "# Reformer un DataFrame avec les colonnes originales\n",
    "df_new_augmented = pd.DataFrame(\n",
    "    np.hstack([X_augmented, y_augmented.reshape(-1, 1)]),\n",
    "    columns=new.columns\n",
    ")\n",
    "\n",
    "# Fusionner avec df_no_outliers\n",
    "df_augmented = pd.concat([df_no_outliers, df_new_augmented], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scene</th>\n",
       "      <th>g1</th>\n",
       "      <th>Cos(SZA)</th>\n",
       "      <th>mu_g</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.068532</td>\n",
       "      <td>0.953130</td>\n",
       "      <td>0.940937</td>\n",
       "      <td>1.222476</td>\n",
       "      <td>0.812762</td>\n",
       "      <td>0.764758</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>0.098065</td>\n",
       "      <td>0.916053</td>\n",
       "      <td>0.848048</td>\n",
       "      <td>707.913889</td>\n",
       "      <td>3185.612500</td>\n",
       "      <td>0.848282</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>1.149748</td>\n",
       "      <td>1.053646</td>\n",
       "      <td>19.107875</td>\n",
       "      <td>0.766006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.302615</td>\n",
       "      <td>0.120446</td>\n",
       "      <td>0.953829</td>\n",
       "      <td>0.941623</td>\n",
       "      <td>1.195295</td>\n",
       "      <td>0.809747</td>\n",
       "      <td>0.762476</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>0.779041</td>\n",
       "      <td>0.915792</td>\n",
       "      <td>0.848048</td>\n",
       "      <td>707.913889</td>\n",
       "      <td>3185.612500</td>\n",
       "      <td>0.848282</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>1.149748</td>\n",
       "      <td>1.053646</td>\n",
       "      <td>49.592110</td>\n",
       "      <td>0.415972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.310783</td>\n",
       "      <td>0.183238</td>\n",
       "      <td>0.954528</td>\n",
       "      <td>0.942295</td>\n",
       "      <td>1.168659</td>\n",
       "      <td>0.806488</td>\n",
       "      <td>0.759950</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>0.370995</td>\n",
       "      <td>0.915532</td>\n",
       "      <td>0.848048</td>\n",
       "      <td>707.913889</td>\n",
       "      <td>3185.612500</td>\n",
       "      <td>0.848282</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>1.149748</td>\n",
       "      <td>1.053646</td>\n",
       "      <td>72.148612</td>\n",
       "      <td>0.543631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.318875</td>\n",
       "      <td>0.243437</td>\n",
       "      <td>0.955227</td>\n",
       "      <td>0.942991</td>\n",
       "      <td>1.142956</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.757681</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>0.241212</td>\n",
       "      <td>0.915271</td>\n",
       "      <td>0.848048</td>\n",
       "      <td>707.913889</td>\n",
       "      <td>3185.612500</td>\n",
       "      <td>0.848282</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>1.149748</td>\n",
       "      <td>1.053646</td>\n",
       "      <td>99.018854</td>\n",
       "      <td>0.566964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.326900</td>\n",
       "      <td>0.308727</td>\n",
       "      <td>0.955787</td>\n",
       "      <td>0.943509</td>\n",
       "      <td>1.118101</td>\n",
       "      <td>0.800492</td>\n",
       "      <td>0.755271</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>0.033981</td>\n",
       "      <td>0.915010</td>\n",
       "      <td>0.848048</td>\n",
       "      <td>707.913889</td>\n",
       "      <td>3185.612500</td>\n",
       "      <td>0.848282</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>1.149748</td>\n",
       "      <td>1.053646</td>\n",
       "      <td>124.868584</td>\n",
       "      <td>0.629959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886745</th>\n",
       "      <td>1.001150</td>\n",
       "      <td>0.783395</td>\n",
       "      <td>0.992109</td>\n",
       "      <td>0.847058</td>\n",
       "      <td>-0.021818</td>\n",
       "      <td>0.026489</td>\n",
       "      <td>0.028190</td>\n",
       "      <td>15.006230</td>\n",
       "      <td>59.021910</td>\n",
       "      <td>0.790729</td>\n",
       "      <td>0.889644</td>\n",
       "      <td>0.961922</td>\n",
       "      <td>707.901412</td>\n",
       "      <td>3185.521670</td>\n",
       "      <td>0.968840</td>\n",
       "      <td>0.969476</td>\n",
       "      <td>1.040402</td>\n",
       "      <td>0.996376</td>\n",
       "      <td>776.673618</td>\n",
       "      <td>-0.218157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886746</th>\n",
       "      <td>0.986933</td>\n",
       "      <td>0.872268</td>\n",
       "      <td>1.002022</td>\n",
       "      <td>0.831011</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.054127</td>\n",
       "      <td>0.036699</td>\n",
       "      <td>14.984698</td>\n",
       "      <td>59.014714</td>\n",
       "      <td>0.795509</td>\n",
       "      <td>0.894563</td>\n",
       "      <td>0.979416</td>\n",
       "      <td>707.892397</td>\n",
       "      <td>3185.526816</td>\n",
       "      <td>0.961248</td>\n",
       "      <td>0.981356</td>\n",
       "      <td>1.008069</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>875.099431</td>\n",
       "      <td>-0.051697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886747</th>\n",
       "      <td>1.000374</td>\n",
       "      <td>0.854945</td>\n",
       "      <td>0.984600</td>\n",
       "      <td>0.817928</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.023073</td>\n",
       "      <td>0.032757</td>\n",
       "      <td>15.000706</td>\n",
       "      <td>58.991118</td>\n",
       "      <td>0.800755</td>\n",
       "      <td>0.885347</td>\n",
       "      <td>0.976809</td>\n",
       "      <td>707.907070</td>\n",
       "      <td>3185.525602</td>\n",
       "      <td>0.949255</td>\n",
       "      <td>0.978790</td>\n",
       "      <td>1.026743</td>\n",
       "      <td>0.985003</td>\n",
       "      <td>843.468220</td>\n",
       "      <td>-0.154843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886748</th>\n",
       "      <td>0.991381</td>\n",
       "      <td>0.831727</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.830063</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.025237</td>\n",
       "      <td>0.020427</td>\n",
       "      <td>14.988263</td>\n",
       "      <td>59.011692</td>\n",
       "      <td>0.796450</td>\n",
       "      <td>0.892745</td>\n",
       "      <td>0.962113</td>\n",
       "      <td>707.888462</td>\n",
       "      <td>3185.511236</td>\n",
       "      <td>0.978011</td>\n",
       "      <td>0.984324</td>\n",
       "      <td>1.033827</td>\n",
       "      <td>1.018744</td>\n",
       "      <td>824.402839</td>\n",
       "      <td>-0.200728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886749</th>\n",
       "      <td>1.022344</td>\n",
       "      <td>0.874214</td>\n",
       "      <td>0.991715</td>\n",
       "      <td>0.821400</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.066491</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>14.993024</td>\n",
       "      <td>58.977706</td>\n",
       "      <td>0.802300</td>\n",
       "      <td>0.897090</td>\n",
       "      <td>0.948003</td>\n",
       "      <td>707.892673</td>\n",
       "      <td>3185.518837</td>\n",
       "      <td>0.969458</td>\n",
       "      <td>0.966472</td>\n",
       "      <td>1.020491</td>\n",
       "      <td>1.011188</td>\n",
       "      <td>867.987125</td>\n",
       "      <td>-0.113329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1886750 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0        0.294500  0.068532  0.953130  0.940937  1.222476  0.812762  0.764758   \n",
       "1        0.302615  0.120446  0.953829  0.941623  1.195295  0.809747  0.762476   \n",
       "2        0.310783  0.183238  0.954528  0.942295  1.168659  0.806488  0.759950   \n",
       "3        0.318875  0.243437  0.955227  0.942991  1.142956  0.803487  0.757681   \n",
       "4        0.326900  0.308727  0.955787  0.943509  1.118101  0.800492  0.755271   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1886745  1.001150  0.783395  0.992109  0.847058 -0.021818  0.026489  0.028190   \n",
       "1886746  0.986933  0.872268  1.002022  0.831011  0.002297  0.054127  0.036699   \n",
       "1886747  1.000374  0.854945  0.984600  0.817928  0.004184  0.023073  0.032757   \n",
       "1886748  0.991381  0.831727  0.998383  0.830063  0.005587  0.025237  0.020427   \n",
       "1886749  1.022344  0.874214  0.991715  0.821400  0.005172  0.066491  0.012089   \n",
       "\n",
       "               SZA           Z   R_scene        g1  Cos(SZA)        mu_g  \\\n",
       "0        32.000000  225.000000  0.098065  0.916053  0.848048  707.913889   \n",
       "1        32.000000  225.000000  0.779041  0.915792  0.848048  707.913889   \n",
       "2        32.000000  225.000000  0.370995  0.915532  0.848048  707.913889   \n",
       "3        32.000000  225.000000  0.241212  0.915271  0.848048  707.913889   \n",
       "4        32.000000  225.000000  0.033981  0.915010  0.848048  707.913889   \n",
       "...            ...         ...       ...       ...       ...         ...   \n",
       "1886745  15.006230   59.021910  0.790729  0.889644  0.961922  707.901412   \n",
       "1886746  14.984698   59.014714  0.795509  0.894563  0.979416  707.892397   \n",
       "1886747  15.000706   58.991118  0.800755  0.885347  0.976809  707.907070   \n",
       "1886748  14.988263   59.011692  0.796450  0.892745  0.962113  707.888462   \n",
       "1886749  14.993024   58.977706  0.802300  0.897090  0.948003  707.892673   \n",
       "\n",
       "                mu_a  muprime_g  muprime_a  mprime_g  mprime_a      BOA_RT  \\\n",
       "0        3185.612500   0.848282   0.848100  1.149748  1.053646   19.107875   \n",
       "1        3185.612500   0.848282   0.848100  1.149748  1.053646   49.592110   \n",
       "2        3185.612500   0.848282   0.848100  1.149748  1.053646   72.148612   \n",
       "3        3185.612500   0.848282   0.848100  1.149748  1.053646   99.018854   \n",
       "4        3185.612500   0.848282   0.848100  1.149748  1.053646  124.868584   \n",
       "...              ...        ...        ...       ...       ...         ...   \n",
       "1886745  3185.521670   0.968840   0.969476  1.040402  0.996376  776.673618   \n",
       "1886746  3185.526816   0.961248   0.981356  1.008069  0.999625  875.099431   \n",
       "1886747  3185.525602   0.949255   0.978790  1.026743  0.985003  843.468220   \n",
       "1886748  3185.511236   0.978011   0.984324  1.033827  1.018744  824.402839   \n",
       "1886749  3185.518837   0.969458   0.966472  1.020491  1.011188  867.987125   \n",
       "\n",
       "            alpha  \n",
       "0        0.766006  \n",
       "1        0.415972  \n",
       "2        0.543631  \n",
       "3        0.566964  \n",
       "4        0.629959  \n",
       "...           ...  \n",
       "1886745 -0.218157  \n",
       "1886746 -0.051697  \n",
       "1886747 -0.154843  \n",
       "1886748 -0.200728  \n",
       "1886749 -0.113329  \n",
       "\n",
       "[1886750 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mprime_a_bin_2</th>\n",
       "      <th>mprime_a_bin_3</th>\n",
       "      <th>mprime_a_bin_4</th>\n",
       "      <th>mprime_g_bin_0</th>\n",
       "      <th>mprime_g_bin_1</th>\n",
       "      <th>mprime_g_bin_2</th>\n",
       "      <th>mprime_g_bin_3</th>\n",
       "      <th>mprime_g_bin_4</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.938938</td>\n",
       "      <td>0.830099</td>\n",
       "      <td>-0.143116</td>\n",
       "      <td>0.205274</td>\n",
       "      <td>-0.748201</td>\n",
       "      <td>-0.102195</td>\n",
       "      <td>-0.148214</td>\n",
       "      <td>1.494961</td>\n",
       "      <td>-0.196407</td>\n",
       "      <td>-0.449844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>429.332315</td>\n",
       "      <td>0.133511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072507</td>\n",
       "      <td>-1.551574</td>\n",
       "      <td>1.189851</td>\n",
       "      <td>0.880461</td>\n",
       "      <td>-0.191308</td>\n",
       "      <td>-0.932700</td>\n",
       "      <td>-0.652588</td>\n",
       "      <td>1.032032</td>\n",
       "      <td>1.082365</td>\n",
       "      <td>-0.470705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.210750</td>\n",
       "      <td>0.363279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.603383</td>\n",
       "      <td>0.858439</td>\n",
       "      <td>-0.191497</td>\n",
       "      <td>0.141423</td>\n",
       "      <td>2.954771</td>\n",
       "      <td>-0.107965</td>\n",
       "      <td>-0.172161</td>\n",
       "      <td>-1.032376</td>\n",
       "      <td>-0.241683</td>\n",
       "      <td>0.598065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>648.404343</td>\n",
       "      <td>0.280041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.181490</td>\n",
       "      <td>0.807331</td>\n",
       "      <td>1.187661</td>\n",
       "      <td>-1.173034</td>\n",
       "      <td>-0.267672</td>\n",
       "      <td>-1.155707</td>\n",
       "      <td>-0.908316</td>\n",
       "      <td>0.025438</td>\n",
       "      <td>-1.062994</td>\n",
       "      <td>0.958397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>885.138727</td>\n",
       "      <td>0.125099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.956225</td>\n",
       "      <td>0.171426</td>\n",
       "      <td>-0.715124</td>\n",
       "      <td>0.448112</td>\n",
       "      <td>0.653581</td>\n",
       "      <td>0.622586</td>\n",
       "      <td>0.469534</td>\n",
       "      <td>-0.757894</td>\n",
       "      <td>-0.363971</td>\n",
       "      <td>0.962627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>602.697524</td>\n",
       "      <td>0.320636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "0  0.938938  0.830099 -0.143116  0.205274 -0.748201 -0.102195 -0.148214   \n",
       "1  0.072507 -1.551574  1.189851  0.880461 -0.191308 -0.932700 -0.652588   \n",
       "2 -2.603383  0.858439 -0.191497  0.141423  2.954771 -0.107965 -0.172161   \n",
       "3  0.181490  0.807331  1.187661 -1.173034 -0.267672 -1.155707 -0.908316   \n",
       "4 -0.956225  0.171426 -0.715124  0.448112  0.653581  0.622586  0.469534   \n",
       "\n",
       "        SZA         Z  R_scence  ...  mprime_a_bin_2  mprime_a_bin_3  \\\n",
       "0  1.494961 -0.196407 -0.449844  ...             0.0             0.0   \n",
       "1  1.032032  1.082365 -0.470705  ...             0.0             0.0   \n",
       "2 -1.032376 -0.241683  0.598065  ...             0.0             0.0   \n",
       "3  0.025438 -1.062994  0.958397  ...             0.0             0.0   \n",
       "4 -0.757894 -0.363971  0.962627  ...             0.0             0.0   \n",
       "\n",
       "   mprime_a_bin_4  mprime_g_bin_0  mprime_g_bin_1  mprime_g_bin_2  \\\n",
       "0             0.0             1.0             0.0             0.0   \n",
       "1             0.0             1.0             0.0             0.0   \n",
       "2             0.0             1.0             0.0             0.0   \n",
       "3             0.0             1.0             0.0             0.0   \n",
       "4             0.0             1.0             0.0             0.0   \n",
       "\n",
       "   mprime_g_bin_3  mprime_g_bin_4      BOA_RT     alpha  \n",
       "0             0.0             0.0  429.332315  0.133511  \n",
       "1             0.0             0.0  194.210750  0.363279  \n",
       "2             0.0             0.0  648.404343  0.280041  \n",
       "3             0.0             0.0  885.138727  0.125099  \n",
       "4             0.0             0.0  602.697524  0.320636  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('preprocessed_data_.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(columns=['alpha'])\n",
    "y=df['alpha']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "BOA_train=X_train['BOA_RT']\n",
    "BOA_test=X_test['BOA_RT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mprime_a_bin_0</th>\n",
       "      <th>mprime_a_bin_1</th>\n",
       "      <th>mprime_a_bin_2</th>\n",
       "      <th>mprime_a_bin_3</th>\n",
       "      <th>mprime_a_bin_4</th>\n",
       "      <th>mprime_g_bin_0</th>\n",
       "      <th>mprime_g_bin_1</th>\n",
       "      <th>mprime_g_bin_2</th>\n",
       "      <th>mprime_g_bin_3</th>\n",
       "      <th>mprime_g_bin_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80661</th>\n",
       "      <td>-0.069120</td>\n",
       "      <td>0.880779</td>\n",
       "      <td>0.640769</td>\n",
       "      <td>-0.173369</td>\n",
       "      <td>-0.088895</td>\n",
       "      <td>-0.790171</td>\n",
       "      <td>-0.665889</td>\n",
       "      <td>1.520224</td>\n",
       "      <td>0.279489</td>\n",
       "      <td>1.004118</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248726</th>\n",
       "      <td>-1.330278</td>\n",
       "      <td>-2.773945</td>\n",
       "      <td>-0.221436</td>\n",
       "      <td>-0.573381</td>\n",
       "      <td>1.036162</td>\n",
       "      <td>-0.425127</td>\n",
       "      <td>-0.545665</td>\n",
       "      <td>-1.069706</td>\n",
       "      <td>1.448924</td>\n",
       "      <td>0.855865</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247518</th>\n",
       "      <td>-0.447697</td>\n",
       "      <td>-1.204405</td>\n",
       "      <td>1.493352</td>\n",
       "      <td>-0.202837</td>\n",
       "      <td>0.204542</td>\n",
       "      <td>-1.268209</td>\n",
       "      <td>-0.942720</td>\n",
       "      <td>0.247516</td>\n",
       "      <td>0.436509</td>\n",
       "      <td>-0.688671</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303607</th>\n",
       "      <td>-1.111384</td>\n",
       "      <td>0.874273</td>\n",
       "      <td>0.948692</td>\n",
       "      <td>-1.774713</td>\n",
       "      <td>0.806064</td>\n",
       "      <td>-1.098430</td>\n",
       "      <td>-0.918408</td>\n",
       "      <td>0.994639</td>\n",
       "      <td>-0.978477</td>\n",
       "      <td>-0.442122</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305103</th>\n",
       "      <td>-3.228436</td>\n",
       "      <td>0.674241</td>\n",
       "      <td>-1.167519</td>\n",
       "      <td>0.118035</td>\n",
       "      <td>4.781522</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.312535</td>\n",
       "      <td>-0.022583</td>\n",
       "      <td>-0.726047</td>\n",
       "      <td>1.270408</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>0.752914</td>\n",
       "      <td>0.744033</td>\n",
       "      <td>0.041159</td>\n",
       "      <td>-1.205458</td>\n",
       "      <td>-0.637513</td>\n",
       "      <td>-0.706761</td>\n",
       "      <td>-0.774369</td>\n",
       "      <td>-0.775916</td>\n",
       "      <td>0.129560</td>\n",
       "      <td>0.250184</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>0.930569</td>\n",
       "      <td>0.612489</td>\n",
       "      <td>-1.441607</td>\n",
       "      <td>0.123058</td>\n",
       "      <td>-0.743313</td>\n",
       "      <td>0.876728</td>\n",
       "      <td>0.471052</td>\n",
       "      <td>-1.627810</td>\n",
       "      <td>-0.521755</td>\n",
       "      <td>-1.252927</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>0.923727</td>\n",
       "      <td>0.498173</td>\n",
       "      <td>-0.628089</td>\n",
       "      <td>-1.309832</td>\n",
       "      <td>-0.739311</td>\n",
       "      <td>-0.445585</td>\n",
       "      <td>-0.719076</td>\n",
       "      <td>-0.930604</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.864780</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146867</th>\n",
       "      <td>0.379052</td>\n",
       "      <td>0.323402</td>\n",
       "      <td>0.139977</td>\n",
       "      <td>0.378624</td>\n",
       "      <td>-0.401067</td>\n",
       "      <td>-0.210751</td>\n",
       "      <td>-0.172633</td>\n",
       "      <td>1.146767</td>\n",
       "      <td>0.084081</td>\n",
       "      <td>1.734614</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>0.682548</td>\n",
       "      <td>0.615331</td>\n",
       "      <td>-0.285352</td>\n",
       "      <td>1.273576</td>\n",
       "      <td>-0.594491</td>\n",
       "      <td>2.124789</td>\n",
       "      <td>2.347421</td>\n",
       "      <td>-1.131450</td>\n",
       "      <td>-0.841141</td>\n",
       "      <td>1.064667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271877 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "80661  -0.069120  0.880779  0.640769 -0.173369 -0.088895 -0.790171 -0.665889   \n",
       "248726 -1.330278 -2.773945 -0.221436 -0.573381  1.036162 -0.425127 -0.545665   \n",
       "247518 -0.447697 -1.204405  1.493352 -0.202837  0.204542 -1.268209 -0.942720   \n",
       "303607 -1.111384  0.874273  0.948692 -1.774713  0.806064 -1.098430 -0.918408   \n",
       "305103 -3.228436  0.674241 -1.167519  0.118035  4.781522  0.640134  0.312535   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "119879  0.752914  0.744033  0.041159 -1.205458 -0.637513 -0.706761 -0.774369   \n",
       "259178  0.930569  0.612489 -1.441607  0.123058 -0.743313  0.876728  0.471052   \n",
       "131932  0.923727  0.498173 -0.628089 -1.309832 -0.739311 -0.445585 -0.719076   \n",
       "146867  0.379052  0.323402  0.139977  0.378624 -0.401067 -0.210751 -0.172633   \n",
       "121958  0.682548  0.615331 -0.285352  1.273576 -0.594491  2.124789  2.347421   \n",
       "\n",
       "             SZA         Z  R_scence  ...  mprime_a_bin_0  mprime_a_bin_1  \\\n",
       "80661   1.520224  0.279489  1.004118  ...             1.0             0.0   \n",
       "248726 -1.069706  1.448924  0.855865  ...             1.0             0.0   \n",
       "247518  0.247516  0.436509 -0.688671  ...             1.0             0.0   \n",
       "303607  0.994639 -0.978477 -0.442122  ...             1.0             0.0   \n",
       "305103 -0.022583 -0.726047  1.270408  ...             1.0             0.0   \n",
       "...          ...       ...       ...  ...             ...             ...   \n",
       "119879 -0.775916  0.129560  0.250184  ...             1.0             0.0   \n",
       "259178 -1.627810 -0.521755 -1.252927  ...             1.0             0.0   \n",
       "131932 -0.930604  0.217773 -0.864780  ...             1.0             0.0   \n",
       "146867  1.146767  0.084081  1.734614  ...             1.0             0.0   \n",
       "121958 -1.131450 -0.841141  1.064667  ...             1.0             0.0   \n",
       "\n",
       "        mprime_a_bin_2  mprime_a_bin_3  mprime_a_bin_4  mprime_g_bin_0  \\\n",
       "80661              0.0             0.0             0.0             1.0   \n",
       "248726             0.0             0.0             0.0             1.0   \n",
       "247518             0.0             0.0             0.0             1.0   \n",
       "303607             0.0             0.0             0.0             1.0   \n",
       "305103             0.0             0.0             0.0             1.0   \n",
       "...                ...             ...             ...             ...   \n",
       "119879             0.0             0.0             0.0             1.0   \n",
       "259178             0.0             0.0             0.0             1.0   \n",
       "131932             0.0             0.0             0.0             1.0   \n",
       "146867             0.0             0.0             0.0             1.0   \n",
       "121958             0.0             0.0             0.0             1.0   \n",
       "\n",
       "        mprime_g_bin_1  mprime_g_bin_2  mprime_g_bin_3  mprime_g_bin_4  \n",
       "80661              0.0             0.0             0.0             0.0  \n",
       "248726             0.0             0.0             0.0             0.0  \n",
       "247518             0.0             0.0             0.0             0.0  \n",
       "303607             0.0             0.0             0.0             0.0  \n",
       "305103             0.0             0.0             0.0             0.0  \n",
       "...                ...             ...             ...             ...  \n",
       "119879             0.0             0.0             0.0             0.0  \n",
       "259178             0.0             0.0             0.0             0.0  \n",
       "131932             0.0             0.0             0.0             0.0  \n",
       "146867             0.0             0.0             0.0             0.0  \n",
       "121958             0.0             0.0             0.0             0.0  \n",
       "\n",
       "[271877 rows x 28 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    362503.000000\n",
       "mean          0.288921\n",
       "std           0.204526\n",
       "min          -0.284378\n",
       "25%           0.166093\n",
       "50%           0.312022\n",
       "75%           0.430461\n",
       "max           0.853075\n",
       "Name: alpha, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best parameters from JSON file\n",
    "with open(\"best_params.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Dense(params[\"units1\"], activation=params[\"activation\"], input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(params[\"dropout1\"]))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Dense(params[\"units2\"], activation=params[\"activation\"]))\n",
    "model.add(Dropout(params[\"dropout2\"]))\n",
    "\n",
    "# Layer 3 (if applicable)\n",
    "if params[\"n_layers\"] >= 3:\n",
    "    model.add(Dense(params[\"units3\"], activation=params[\"activation\"]))\n",
    "    model.add(Dropout(params[\"dropout3\"]))\n",
    "\n",
    "# Layer 4 (if applicable)\n",
    "if params[\"n_layers\"] == 4:\n",
    "    model.add(Dense(params[\"units4\"], activation=params[\"activation\"]))\n",
    "    model.add(Dropout(params[\"dropout4\"]))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=params[\"lr\"])\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0286 - mae: 0.1198\n",
      "Epoch 1: val_mae improved from inf to 0.04726, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 0.0285 - mae: 0.1197 - val_loss: 0.0045 - val_mae: 0.0473 - learning_rate: 3.4066e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0063 - mae: 0.0580\n",
      "Epoch 2: val_mae improved from 0.04726 to 0.03454, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0063 - mae: 0.0580 - val_loss: 0.0025 - val_mae: 0.0345 - learning_rate: 3.4066e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0034 - mae: 0.0418\n",
      "Epoch 3: val_mae improved from 0.03454 to 0.02880, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0034 - mae: 0.0418 - val_loss: 0.0017 - val_mae: 0.0288 - learning_rate: 3.4066e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023 - mae: 0.0337\n",
      "Epoch 4: val_mae improved from 0.02880 to 0.02181, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 0.0023 - mae: 0.0337 - val_loss: 0.0011 - val_mae: 0.0218 - learning_rate: 3.4066e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m3390/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0017 - mae: 0.0295\n",
      "Epoch 5: val_mae improved from 0.02181 to 0.01809, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 8.0941e-04 - val_mae: 0.0181 - learning_rate: 3.4066e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0015 - mae: 0.0274\n",
      "Epoch 6: val_mae did not improve from 0.01809\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 8.1000e-04 - val_mae: 0.0200 - learning_rate: 3.4066e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0013 - mae: 0.0257\n",
      "Epoch 7: val_mae improved from 0.01809 to 0.01318, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0013 - mae: 0.0257 - val_loss: 5.1588e-04 - val_mae: 0.0132 - learning_rate: 3.4066e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0012 - mae: 0.0247\n",
      "Epoch 8: val_mae did not improve from 0.01318\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 4.9307e-04 - val_mae: 0.0144 - learning_rate: 3.4066e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0011 - mae: 0.0238\n",
      "Epoch 9: val_mae did not improve from 0.01318\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 4.7843e-04 - val_mae: 0.0148 - learning_rate: 3.4066e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0010 - mae: 0.0230\n",
      "Epoch 10: val_mae improved from 0.01318 to 0.01106, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 0.0010 - mae: 0.0230 - val_loss: 3.5513e-04 - val_mae: 0.0111 - learning_rate: 3.4066e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0010 - mae: 0.0229\n",
      "Epoch 11: val_mae did not improve from 0.01106\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 4.2054e-04 - val_mae: 0.0139 - learning_rate: 3.4066e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.6126e-04 - mae: 0.0222\n",
      "Epoch 12: val_mae did not improve from 0.01106\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 9.6126e-04 - mae: 0.0222 - val_loss: 4.0231e-04 - val_mae: 0.0137 - learning_rate: 3.4066e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.6203e-04 - mae: 0.0221\n",
      "Epoch 13: val_mae did not improve from 0.01106\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - loss: 9.6199e-04 - mae: 0.0221 - val_loss: 3.6292e-04 - val_mae: 0.0118 - learning_rate: 3.4066e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.0751e-04 - mae: 0.0216\n",
      "Epoch 14: val_mae did not improve from 0.01106\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 16ms/step - loss: 9.0751e-04 - mae: 0.0216 - val_loss: 4.5024e-04 - val_mae: 0.0152 - learning_rate: 3.4066e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.8583e-04 - mae: 0.0213\n",
      "Epoch 15: val_mae improved from 0.01106 to 0.01057, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 7ms/step - loss: 8.8584e-04 - mae: 0.0213 - val_loss: 2.9014e-04 - val_mae: 0.0106 - learning_rate: 3.4066e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.6434e-04 - mae: 0.0211\n",
      "Epoch 16: val_mae did not improve from 0.01057\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 8.6433e-04 - mae: 0.0211 - val_loss: 2.9753e-04 - val_mae: 0.0113 - learning_rate: 3.4066e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5551e-04 - mae: 0.0210\n",
      "Epoch 17: val_mae did not improve from 0.01057\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 8.5550e-04 - mae: 0.0210 - val_loss: 3.2669e-04 - val_mae: 0.0111 - learning_rate: 3.4066e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1634e-04 - mae: 0.0206\n",
      "Epoch 18: val_mae did not improve from 0.01057\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 8.1634e-04 - mae: 0.0206 - val_loss: 4.1438e-04 - val_mae: 0.0147 - learning_rate: 3.4066e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1335e-04 - mae: 0.0205\n",
      "Epoch 19: val_mae did not improve from 0.01057\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 8.1334e-04 - mae: 0.0205 - val_loss: 3.7087e-04 - val_mae: 0.0128 - learning_rate: 3.4066e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.9599e-04 - mae: 0.0203\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001703323214314878.\n",
      "\n",
      "Epoch 20: val_mae did not improve from 0.01057\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 7.9601e-04 - mae: 0.0203 - val_loss: 3.0906e-04 - val_mae: 0.0121 - learning_rate: 3.4066e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9978e-04 - mae: 0.0188\n",
      "Epoch 21: val_mae improved from 0.01057 to 0.00785, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 6.9977e-04 - mae: 0.0188 - val_loss: 1.8305e-04 - val_mae: 0.0078 - learning_rate: 1.7033e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7787e-04 - mae: 0.0186\n",
      "Epoch 22: val_mae did not improve from 0.00785\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 6.7786e-04 - mae: 0.0186 - val_loss: 2.0926e-04 - val_mae: 0.0090 - learning_rate: 1.7033e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.5214e-04 - mae: 0.0184\n",
      "Epoch 23: val_mae did not improve from 0.00785\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 6.5214e-04 - mae: 0.0184 - val_loss: 1.9632e-04 - val_mae: 0.0088 - learning_rate: 1.7033e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6081e-04 - mae: 0.0184\n",
      "Epoch 24: val_mae did not improve from 0.00785\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 6.6081e-04 - mae: 0.0184 - val_loss: 2.1576e-04 - val_mae: 0.0095 - learning_rate: 1.7033e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.6126e-04 - mae: 0.0184\n",
      "Epoch 25: val_mae improved from 0.00785 to 0.00784, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 6.6124e-04 - mae: 0.0184 - val_loss: 1.7286e-04 - val_mae: 0.0078 - learning_rate: 1.7033e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5182e-04 - mae: 0.0183\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.51661607157439e-05.\n",
      "\n",
      "Epoch 26: val_mae did not improve from 0.00784\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 6.5182e-04 - mae: 0.0183 - val_loss: 1.9067e-04 - val_mae: 0.0079 - learning_rate: 1.7033e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9093e-04 - mae: 0.0174\n",
      "Epoch 27: val_mae did not improve from 0.00784\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 5.9094e-04 - mae: 0.0174 - val_loss: 1.8292e-04 - val_mae: 0.0082 - learning_rate: 8.5166e-05\n",
      "Epoch 28/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8831e-04 - mae: 0.0173\n",
      "Epoch 28: val_mae improved from 0.00784 to 0.00765, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 5.8831e-04 - mae: 0.0173 - val_loss: 1.5880e-04 - val_mae: 0.0076 - learning_rate: 8.5166e-05\n",
      "Epoch 29/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7980e-04 - mae: 0.0173\n",
      "Epoch 29: val_mae did not improve from 0.00765\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 5.7980e-04 - mae: 0.0173 - val_loss: 1.8284e-04 - val_mae: 0.0086 - learning_rate: 8.5166e-05\n",
      "Epoch 30/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7666e-04 - mae: 0.0172\n",
      "Epoch 30: val_mae did not improve from 0.00765\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 5.7667e-04 - mae: 0.0172 - val_loss: 1.6540e-04 - val_mae: 0.0079 - learning_rate: 8.5166e-05\n",
      "Epoch 31/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7837e-04 - mae: 0.0172\n",
      "Epoch 31: val_mae did not improve from 0.00765\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 5.7835e-04 - mae: 0.0172 - val_loss: 1.7608e-04 - val_mae: 0.0079 - learning_rate: 8.5166e-05\n",
      "Epoch 32/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6484e-04 - mae: 0.0171\n",
      "Epoch 32: val_mae did not improve from 0.00765\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 5.6485e-04 - mae: 0.0171 - val_loss: 1.6836e-04 - val_mae: 0.0077 - learning_rate: 8.5166e-05\n",
      "Epoch 33/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6603e-04 - mae: 0.0171\n",
      "Epoch 33: val_mae improved from 0.00765 to 0.00724, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 5.6603e-04 - mae: 0.0171 - val_loss: 1.5177e-04 - val_mae: 0.0072 - learning_rate: 8.5166e-05\n",
      "Epoch 34/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7614e-04 - mae: 0.0171\n",
      "Epoch 34: val_mae did not improve from 0.00724\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 5.7613e-04 - mae: 0.0171 - val_loss: 1.6800e-04 - val_mae: 0.0080 - learning_rate: 8.5166e-05\n",
      "Epoch 35/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6710e-04 - mae: 0.0170\n",
      "Epoch 35: val_mae improved from 0.00724 to 0.00661, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 5.6710e-04 - mae: 0.0170 - val_loss: 1.3853e-04 - val_mae: 0.0066 - learning_rate: 8.5166e-05\n",
      "Epoch 36/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5369e-04 - mae: 0.0169\n",
      "Epoch 36: val_mae did not improve from 0.00661\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 5.5369e-04 - mae: 0.0169 - val_loss: 1.4000e-04 - val_mae: 0.0070 - learning_rate: 8.5166e-05\n",
      "Epoch 37/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5568e-04 - mae: 0.0169\n",
      "Epoch 37: val_mae did not improve from 0.00661\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 5.5568e-04 - mae: 0.0169 - val_loss: 1.8070e-04 - val_mae: 0.0077 - learning_rate: 8.5166e-05\n",
      "Epoch 38/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5315e-04 - mae: 0.0169\n",
      "Epoch 38: val_mae did not improve from 0.00661\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 5.5315e-04 - mae: 0.0169 - val_loss: 1.6523e-04 - val_mae: 0.0083 - learning_rate: 8.5166e-05\n",
      "Epoch 39/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5177e-04 - mae: 0.0169\n",
      "Epoch 39: val_mae improved from 0.00661 to 0.00639, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 5.5177e-04 - mae: 0.0169 - val_loss: 1.2824e-04 - val_mae: 0.0064 - learning_rate: 8.5166e-05\n",
      "Epoch 40/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4341e-04 - mae: 0.0167\n",
      "Epoch 40: val_mae did not improve from 0.00639\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 5.4342e-04 - mae: 0.0167 - val_loss: 1.2835e-04 - val_mae: 0.0065 - learning_rate: 8.5166e-05\n",
      "Epoch 41/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4732e-04 - mae: 0.0168\n",
      "Epoch 41: val_mae did not improve from 0.00639\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 5.4732e-04 - mae: 0.0168 - val_loss: 1.8953e-04 - val_mae: 0.0092 - learning_rate: 8.5166e-05\n",
      "Epoch 42/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6387e-04 - mae: 0.0168\n",
      "Epoch 42: val_mae did not improve from 0.00639\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 5.6384e-04 - mae: 0.0168 - val_loss: 1.4507e-04 - val_mae: 0.0070 - learning_rate: 8.5166e-05\n",
      "Epoch 43/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4273e-04 - mae: 0.0167\n",
      "Epoch 43: val_mae improved from 0.00639 to 0.00613, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 5.4273e-04 - mae: 0.0167 - val_loss: 1.1999e-04 - val_mae: 0.0061 - learning_rate: 8.5166e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5155e-04 - mae: 0.0168\n",
      "Epoch 44: val_mae did not improve from 0.00613\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 5.5155e-04 - mae: 0.0168 - val_loss: 1.2729e-04 - val_mae: 0.0064 - learning_rate: 8.5166e-05\n",
      "Epoch 45/200\n",
      "\u001b[1m3390/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4496e-04 - mae: 0.0166\n",
      "Epoch 45: val_mae did not improve from 0.00613\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 5.4496e-04 - mae: 0.0166 - val_loss: 1.5105e-04 - val_mae: 0.0071 - learning_rate: 8.5166e-05\n",
      "Epoch 46/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.3871e-04 - mae: 0.0167\n",
      "Epoch 46: val_mae did not improve from 0.00613\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 5.3872e-04 - mae: 0.0167 - val_loss: 1.2363e-04 - val_mae: 0.0063 - learning_rate: 8.5166e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.3024e-04 - mae: 0.0165\n",
      "Epoch 47: val_mae did not improve from 0.00613\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 12ms/step - loss: 5.3024e-04 - mae: 0.0165 - val_loss: 1.3691e-04 - val_mae: 0.0070 - learning_rate: 8.5166e-05\n",
      "Epoch 48/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.3942e-04 - mae: 0.0167\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 4.258308035787195e-05.\n",
      "\n",
      "Epoch 48: val_mae did not improve from 0.00613\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 5.3942e-04 - mae: 0.0167 - val_loss: 1.4419e-04 - val_mae: 0.0079 - learning_rate: 8.5166e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.0780e-04 - mae: 0.0162\n",
      "Epoch 49: val_mae improved from 0.00613 to 0.00553, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 10ms/step - loss: 5.0780e-04 - mae: 0.0162 - val_loss: 1.0597e-04 - val_mae: 0.0055 - learning_rate: 4.2583e-05\n",
      "Epoch 50/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.0418e-04 - mae: 0.0161\n",
      "Epoch 50: val_mae did not improve from 0.00553\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 11ms/step - loss: 5.0418e-04 - mae: 0.0161 - val_loss: 1.3214e-04 - val_mae: 0.0071 - learning_rate: 4.2583e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.0464e-04 - mae: 0.0161\n",
      "Epoch 51: val_mae did not improve from 0.00553\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 11ms/step - loss: 5.0464e-04 - mae: 0.0161 - val_loss: 1.1549e-04 - val_mae: 0.0064 - learning_rate: 4.2583e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.9993e-04 - mae: 0.0160\n",
      "Epoch 52: val_mae did not improve from 0.00553\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - loss: 4.9993e-04 - mae: 0.0160 - val_loss: 1.3493e-04 - val_mae: 0.0065 - learning_rate: 4.2583e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.9899e-04 - mae: 0.0161\n",
      "Epoch 53: val_mae did not improve from 0.00553\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 4.9899e-04 - mae: 0.0161 - val_loss: 1.0910e-04 - val_mae: 0.0056 - learning_rate: 4.2583e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9882e-04 - mae: 0.0160\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 2.1291540178935975e-05.\n",
      "\n",
      "Epoch 54: val_mae did not improve from 0.00553\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 7ms/step - loss: 4.9883e-04 - mae: 0.0160 - val_loss: 1.1713e-04 - val_mae: 0.0059 - learning_rate: 4.2583e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8590e-04 - mae: 0.0158\n",
      "Epoch 55: val_mae did not improve from 0.00553\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 4.8590e-04 - mae: 0.0158 - val_loss: 1.1253e-04 - val_mae: 0.0062 - learning_rate: 2.1292e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8454e-04 - mae: 0.0158\n",
      "Epoch 56: val_mae improved from 0.00553 to 0.00533, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 4.8454e-04 - mae: 0.0158 - val_loss: 1.0851e-04 - val_mae: 0.0053 - learning_rate: 2.1292e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m3388/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8685e-04 - mae: 0.0157\n",
      "Epoch 57: val_mae did not improve from 0.00533\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 4.8685e-04 - mae: 0.0157 - val_loss: 1.0701e-04 - val_mae: 0.0057 - learning_rate: 2.1292e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8669e-04 - mae: 0.0158\n",
      "Epoch 58: val_mae did not improve from 0.00533\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 4.8669e-04 - mae: 0.0158 - val_loss: 1.2130e-04 - val_mae: 0.0061 - learning_rate: 2.1292e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8145e-04 - mae: 0.0157\n",
      "Epoch 59: val_mae did not improve from 0.00533\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 4.8145e-04 - mae: 0.0157 - val_loss: 1.0636e-04 - val_mae: 0.0056 - learning_rate: 2.1292e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7843e-04 - mae: 0.0157\n",
      "Epoch 60: val_mae did not improve from 0.00533\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 4.7843e-04 - mae: 0.0157 - val_loss: 1.0512e-04 - val_mae: 0.0054 - learning_rate: 2.1292e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8121e-04 - mae: 0.0158\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.0645770089467987e-05.\n",
      "\n",
      "Epoch 61: val_mae did not improve from 0.00533\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 4.8122e-04 - mae: 0.0158 - val_loss: 1.0232e-04 - val_mae: 0.0055 - learning_rate: 2.1292e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7119e-04 - mae: 0.0156\n",
      "Epoch 62: val_mae improved from 0.00533 to 0.00516, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 4.7119e-04 - mae: 0.0156 - val_loss: 9.7994e-05 - val_mae: 0.0052 - learning_rate: 1.0646e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8236e-04 - mae: 0.0157\n",
      "Epoch 63: val_mae improved from 0.00516 to 0.00512, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 4.8235e-04 - mae: 0.0157 - val_loss: 9.8916e-05 - val_mae: 0.0051 - learning_rate: 1.0646e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6959e-04 - mae: 0.0156\n",
      "Epoch 64: val_mae did not improve from 0.00512\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 4.6959e-04 - mae: 0.0156 - val_loss: 9.7952e-05 - val_mae: 0.0052 - learning_rate: 1.0646e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7453e-04 - mae: 0.0156\n",
      "Epoch 65: val_mae did not improve from 0.00512\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 4.7453e-04 - mae: 0.0156 - val_loss: 1.1402e-04 - val_mae: 0.0062 - learning_rate: 1.0646e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7886e-04 - mae: 0.0157\n",
      "Epoch 66: val_mae did not improve from 0.00512\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 4.7885e-04 - mae: 0.0157 - val_loss: 1.0025e-04 - val_mae: 0.0052 - learning_rate: 1.0646e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8051e-04 - mae: 0.0157\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.322885044733994e-06.\n",
      "\n",
      "Epoch 67: val_mae did not improve from 0.00512\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 4.8051e-04 - mae: 0.0157 - val_loss: 9.8759e-05 - val_mae: 0.0052 - learning_rate: 1.0646e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7412e-04 - mae: 0.0156\n",
      "Epoch 68: val_mae did not improve from 0.00512\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 4.7412e-04 - mae: 0.0156 - val_loss: 1.0187e-04 - val_mae: 0.0053 - learning_rate: 5.3229e-06\n",
      "Epoch 69/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7094e-04 - mae: 0.0155\n",
      "Epoch 69: val_mae did not improve from 0.00512\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 4.7094e-04 - mae: 0.0155 - val_loss: 1.0202e-04 - val_mae: 0.0052 - learning_rate: 5.3229e-06\n",
      "Epoch 70/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6254e-04 - mae: 0.0155\n",
      "Epoch 70: val_mae improved from 0.00512 to 0.00502, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 4.6255e-04 - mae: 0.0155 - val_loss: 9.4986e-05 - val_mae: 0.0050 - learning_rate: 5.3229e-06\n",
      "Epoch 71/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7137e-04 - mae: 0.0156\n",
      "Epoch 71: val_mae did not improve from 0.00502\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 4.7136e-04 - mae: 0.0156 - val_loss: 9.3928e-05 - val_mae: 0.0050 - learning_rate: 5.3229e-06\n",
      "Epoch 72/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7446e-04 - mae: 0.0156\n",
      "Epoch 72: val_mae did not improve from 0.00502\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 4.7446e-04 - mae: 0.0156 - val_loss: 9.7717e-05 - val_mae: 0.0053 - learning_rate: 5.3229e-06\n",
      "Epoch 73/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6814e-04 - mae: 0.0155\n",
      "Epoch 73: val_mae did not improve from 0.00502\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 4.6815e-04 - mae: 0.0155 - val_loss: 9.9999e-05 - val_mae: 0.0052 - learning_rate: 5.3229e-06\n",
      "Epoch 74/200\n",
      "\u001b[1m3390/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6860e-04 - mae: 0.0155\n",
      "Epoch 74: val_mae did not improve from 0.00502\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 4.6860e-04 - mae: 0.0155 - val_loss: 1.0073e-04 - val_mae: 0.0052 - learning_rate: 5.3229e-06\n",
      "Epoch 75/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7907e-04 - mae: 0.0156\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 2.661442522366997e-06.\n",
      "\n",
      "Epoch 75: val_mae did not improve from 0.00502\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 4.7906e-04 - mae: 0.0156 - val_loss: 9.7297e-05 - val_mae: 0.0051 - learning_rate: 5.3229e-06\n",
      "Epoch 76/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6716e-04 - mae: 0.0155\n",
      "Epoch 76: val_mae improved from 0.00502 to 0.00500, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 4.6717e-04 - mae: 0.0155 - val_loss: 9.5384e-05 - val_mae: 0.0050 - learning_rate: 2.6614e-06\n",
      "Epoch 77/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6478e-04 - mae: 0.0155\n",
      "Epoch 77: val_mae did not improve from 0.00500\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 4.6478e-04 - mae: 0.0155 - val_loss: 9.7290e-05 - val_mae: 0.0052 - learning_rate: 2.6614e-06\n",
      "Epoch 78/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7737e-04 - mae: 0.0156\n",
      "Epoch 78: val_mae improved from 0.00500 to 0.00500, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 4.7737e-04 - mae: 0.0156 - val_loss: 9.5034e-05 - val_mae: 0.0050 - learning_rate: 2.6614e-06\n",
      "Epoch 79/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6783e-04 - mae: 0.0155\n",
      "Epoch 79: val_mae did not improve from 0.00500\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 4.6783e-04 - mae: 0.0155 - val_loss: 9.5053e-05 - val_mae: 0.0050 - learning_rate: 2.6614e-06\n",
      "Epoch 80/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6699e-04 - mae: 0.0155\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.3307212611834984e-06.\n",
      "\n",
      "Epoch 80: val_mae did not improve from 0.00500\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 4.6699e-04 - mae: 0.0155 - val_loss: 9.5278e-05 - val_mae: 0.0051 - learning_rate: 2.6614e-06\n",
      "Epoch 81/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6526e-04 - mae: 0.0154\n",
      "Epoch 81: val_mae did not improve from 0.00500\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 4.6526e-04 - mae: 0.0154 - val_loss: 9.5747e-05 - val_mae: 0.0050 - learning_rate: 1.3307e-06\n",
      "Epoch 82/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6948e-04 - mae: 0.0155\n",
      "Epoch 82: val_mae did not improve from 0.00500\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 4.6948e-04 - mae: 0.0155 - val_loss: 9.5720e-05 - val_mae: 0.0051 - learning_rate: 1.3307e-06\n",
      "Epoch 83/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7029e-04 - mae: 0.0155\n",
      "Epoch 83: val_mae did not improve from 0.00500\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 4.7028e-04 - mae: 0.0155 - val_loss: 9.5085e-05 - val_mae: 0.0051 - learning_rate: 1.3307e-06\n",
      "Epoch 84/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6529e-04 - mae: 0.0155\n",
      "Epoch 84: val_mae did not improve from 0.00500\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 4.6529e-04 - mae: 0.0155 - val_loss: 9.4756e-05 - val_mae: 0.0050 - learning_rate: 1.3307e-06\n",
      "Epoch 85/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7006e-04 - mae: 0.0155\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\n",
      "Epoch 85: val_mae did not improve from 0.00500\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 4.7006e-04 - mae: 0.0155 - val_loss: 9.6243e-05 - val_mae: 0.0050 - learning_rate: 1.3307e-06\n",
      "Epoch 86/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7444e-04 - mae: 0.0156\n",
      "Epoch 86: val_mae improved from 0.00500 to 0.00499, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 4.7444e-04 - mae: 0.0156 - val_loss: 9.4342e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 87/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6636e-04 - mae: 0.0155\n",
      "Epoch 87: val_mae did not improve from 0.00499\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 4.6636e-04 - mae: 0.0155 - val_loss: 9.5214e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 88/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6102e-04 - mae: 0.0155\n",
      "Epoch 88: val_mae did not improve from 0.00499\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 4.6103e-04 - mae: 0.0155 - val_loss: 9.5119e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 89/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6547e-04 - mae: 0.0155\n",
      "Epoch 89: val_mae did not improve from 0.00499\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 4.6548e-04 - mae: 0.0155 - val_loss: 9.4931e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 90/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5605e-04 - mae: 0.0154\n",
      "Epoch 90: val_mae did not improve from 0.00499\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 4.5606e-04 - mae: 0.0154 - val_loss: 9.5512e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 91/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6994e-04 - mae: 0.0155\n",
      "Epoch 91: val_mae improved from 0.00499 to 0.00496, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8ms/step - loss: 4.6993e-04 - mae: 0.0155 - val_loss: 9.3891e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 92/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6611e-04 - mae: 0.0155\n",
      "Epoch 92: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 4.6612e-04 - mae: 0.0155 - val_loss: 9.3945e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 93/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6151e-04 - mae: 0.0154\n",
      "Epoch 93: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 4.6151e-04 - mae: 0.0154 - val_loss: 9.3616e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 94/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5906e-04 - mae: 0.0154\n",
      "Epoch 94: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 4.5907e-04 - mae: 0.0154 - val_loss: 9.5473e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 95/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6735e-04 - mae: 0.0155\n",
      "Epoch 95: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 7ms/step - loss: 4.6735e-04 - mae: 0.0155 - val_loss: 9.4831e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 96/200\n",
      "\u001b[1m3389/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6582e-04 - mae: 0.0155\n",
      "Epoch 96: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 4.6582e-04 - mae: 0.0155 - val_loss: 9.4996e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 97/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6168e-04 - mae: 0.0154\n",
      "Epoch 97: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 4.6168e-04 - mae: 0.0154 - val_loss: 9.4930e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 98/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6917e-04 - mae: 0.0154\n",
      "Epoch 98: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 8ms/step - loss: 4.6917e-04 - mae: 0.0154 - val_loss: 9.4008e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 99/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5993e-04 - mae: 0.0155\n",
      "Epoch 99: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 4.5993e-04 - mae: 0.0155 - val_loss: 9.4682e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 100/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6455e-04 - mae: 0.0155\n",
      "Epoch 100: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 4.6455e-04 - mae: 0.0155 - val_loss: 9.4463e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 101/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6524e-04 - mae: 0.0154\n",
      "Epoch 101: val_mae did not improve from 0.00496\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 4.6524e-04 - mae: 0.0154 - val_loss: 9.6195e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 102/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6920e-04 - mae: 0.0155\n",
      "Epoch 102: val_mae improved from 0.00496 to 0.00492, saving model to deep_model1.keras\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 4.6920e-04 - mae: 0.0155 - val_loss: 9.4170e-05 - val_mae: 0.0049 - learning_rate: 1.0000e-06\n",
      "Epoch 103/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7206e-04 - mae: 0.0155\n",
      "Epoch 103: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 16ms/step - loss: 4.7206e-04 - mae: 0.0155 - val_loss: 9.5035e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 104/200\n",
      "\u001b[1m3393/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7013e-04 - mae: 0.0155\n",
      "Epoch 104: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 13ms/step - loss: 4.7012e-04 - mae: 0.0155 - val_loss: 9.6730e-05 - val_mae: 0.0052 - learning_rate: 1.0000e-06\n",
      "Epoch 105/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.7062e-04 - mae: 0.0155\n",
      "Epoch 105: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 14ms/step - loss: 4.7061e-04 - mae: 0.0155 - val_loss: 9.4967e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 106/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.6899e-04 - mae: 0.0155\n",
      "Epoch 106: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 13ms/step - loss: 4.6899e-04 - mae: 0.0155 - val_loss: 9.5969e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 107/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.6219e-04 - mae: 0.0154\n",
      "Epoch 107: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 12ms/step - loss: 4.6219e-04 - mae: 0.0154 - val_loss: 9.5001e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 108/200\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.6519e-04 - mae: 0.0155\n",
      "Epoch 108: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 9ms/step - loss: 4.6519e-04 - mae: 0.0155 - val_loss: 9.4204e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 109/200\n",
      "\u001b[1m3394/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.6576e-04 - mae: 0.0154\n",
      "Epoch 109: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - loss: 4.6576e-04 - mae: 0.0154 - val_loss: 9.4735e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 110/200\n",
      "\u001b[1m3395/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.6688e-04 - mae: 0.0155\n",
      "Epoch 110: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 11ms/step - loss: 4.6688e-04 - mae: 0.0155 - val_loss: 9.3793e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 111/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.6625e-04 - mae: 0.0155\n",
      "Epoch 111: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 12ms/step - loss: 4.6625e-04 - mae: 0.0155 - val_loss: 9.3916e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 112/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.6490e-04 - mae: 0.0154\n",
      "Epoch 112: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 9ms/step - loss: 4.6490e-04 - mae: 0.0154 - val_loss: 9.4726e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 113/200\n",
      "\u001b[1m3391/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.6248e-04 - mae: 0.0154\n",
      "Epoch 113: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 4.6249e-04 - mae: 0.0154 - val_loss: 9.4269e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 114/200\n",
      "\u001b[1m3398/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.6746e-04 - mae: 0.0154\n",
      "Epoch 114: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 10ms/step - loss: 4.6746e-04 - mae: 0.0154 - val_loss: 9.5835e-05 - val_mae: 0.0051 - learning_rate: 1.0000e-06\n",
      "Epoch 115/200\n",
      "\u001b[1m3396/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.6230e-04 - mae: 0.0155\n",
      "Epoch 115: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 4.6231e-04 - mae: 0.0155 - val_loss: 9.4467e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 116/200\n",
      "\u001b[1m3397/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6391e-04 - mae: 0.0154\n",
      "Epoch 116: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 8ms/step - loss: 4.6391e-04 - mae: 0.0154 - val_loss: 9.5337e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 117/200\n",
      "\u001b[1m3392/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.7021e-04 - mae: 0.0155\n",
      "Epoch 117: val_mae did not improve from 0.00492\n",
      "\u001b[1m3399/3399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 4.7020e-04 - mae: 0.0155 - val_loss: 9.5297e-05 - val_mae: 0.0050 - learning_rate: 1.0000e-06\n",
      "Epoch 117: early stopping\n",
      "Restoring model weights from the end of the best epoch: 102.\n"
     ]
    }
   ],
   "source": [
    "# 1. Early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_mae',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Model checkpoint (save best model)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='deep_model1.keras',\n",
    "    monitor='val_mae',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. CSV logger (log training history)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_logger = CSVLogger(f'training_log_{timestamp}.csv')\n",
    "\n",
    "# Train phase :\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint_cb, csv_logger],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 362503 entries, 0 to 362502\n",
      "Data columns (total 30 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Tg_scat         362503 non-null  float64\n",
      " 1   Tg_abs          362503 non-null  float64\n",
      " 2   Ta_abs          362503 non-null  float64\n",
      " 3   SSA             362503 non-null  float64\n",
      " 4   GOD             362503 non-null  float64\n",
      " 5   AOD             362503 non-null  float64\n",
      " 6   AODS            362503 non-null  float64\n",
      " 7   SZA             362503 non-null  float64\n",
      " 8   Z               362503 non-null  float64\n",
      " 9   R_scence        362503 non-null  float64\n",
      " 10  g1              362503 non-null  float64\n",
      " 11  Cos(SZA)        362503 non-null  float64\n",
      " 12  mu_g            362503 non-null  float64\n",
      " 13  mu_a            362503 non-null  float64\n",
      " 14  muprime_g       362503 non-null  float64\n",
      " 15  muprime_a       362503 non-null  float64\n",
      " 16  mprime_g        362503 non-null  float64\n",
      " 17  mprime_a        362503 non-null  float64\n",
      " 18  mprime_a_bin_0  362503 non-null  float64\n",
      " 19  mprime_a_bin_1  362503 non-null  float64\n",
      " 20  mprime_a_bin_2  362503 non-null  float64\n",
      " 21  mprime_a_bin_3  362503 non-null  float64\n",
      " 22  mprime_a_bin_4  362503 non-null  float64\n",
      " 23  mprime_g_bin_0  362503 non-null  float64\n",
      " 24  mprime_g_bin_1  362503 non-null  float64\n",
      " 25  mprime_g_bin_2  362503 non-null  float64\n",
      " 26  mprime_g_bin_3  362503 non-null  float64\n",
      " 27  mprime_g_bin_4  362503 non-null  float64\n",
      " 28  BOA_RT          362502 non-null  float64\n",
      " 29  alpha           362502 non-null  float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 83.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'reg:squarederror',\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'device': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'feature_types': None,\n",
       " 'gamma': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_bin': None,\n",
       " 'max_cat_threshold': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': 9,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'multi_strategy': None,\n",
       " 'n_estimators': 300,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 271877 entries, 80661 to 121958\n",
      "Data columns (total 28 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Tg_scat         271877 non-null  float64\n",
      " 1   Tg_abs          271877 non-null  float64\n",
      " 2   Ta_abs          271877 non-null  float64\n",
      " 3   SSA             271877 non-null  float64\n",
      " 4   GOD             271877 non-null  float64\n",
      " 5   AOD             271877 non-null  float64\n",
      " 6   AODS            271877 non-null  float64\n",
      " 7   SZA             271877 non-null  float64\n",
      " 8   Z               271877 non-null  float64\n",
      " 9   R_scence        271877 non-null  float64\n",
      " 10  g1              271877 non-null  float64\n",
      " 11  Cos(SZA)        271877 non-null  float64\n",
      " 12  mu_g            271877 non-null  float64\n",
      " 13  mu_a            271877 non-null  float64\n",
      " 14  muprime_g       271877 non-null  float64\n",
      " 15  muprime_a       271877 non-null  float64\n",
      " 16  mprime_g        271877 non-null  float64\n",
      " 17  mprime_a        271877 non-null  float64\n",
      " 18  mprime_a_bin_0  271877 non-null  float64\n",
      " 19  mprime_a_bin_1  271877 non-null  float64\n",
      " 20  mprime_a_bin_2  271877 non-null  float64\n",
      " 21  mprime_a_bin_3  271877 non-null  float64\n",
      " 22  mprime_a_bin_4  271877 non-null  float64\n",
      " 23  mprime_g_bin_0  271877 non-null  float64\n",
      " 24  mprime_g_bin_1  271877 non-null  float64\n",
      " 25  mprime_g_bin_2  271877 non-null  float64\n",
      " 26  mprime_g_bin_3  271877 non-null  float64\n",
      " 27  mprime_g_bin_4  271877 non-null  float64\n",
      "dtypes: float64(28)\n",
      "memory usage: 60.2 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80661     0.723401\n",
       "248726    0.415715\n",
       "247518    0.391787\n",
       "303607    0.244650\n",
       "305103    0.475617\n",
       "            ...   \n",
       "119879    0.474890\n",
       "259178    0.342325\n",
       "131932   -0.104851\n",
       "146867    0.504175\n",
       "121958    0.103362\n",
       "Name: alpha, Length: 271877, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8401935, 1.4607283, 1.2887543, ..., 1.2130278, 1.1415201,\n",
       "       1.2756097], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=joblib.load('model.pkl')\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X_train.to_numpy()\n",
    "y_np = y_train.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('deep_model.h5', compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2833/2833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21968824],\n",
       "       [ 0.3766121 ],\n",
       "       [ 0.22941366],\n",
       "       ...,\n",
       "       [ 0.1528129 ],\n",
       "       [ 0.14012328],\n",
       "       [ 0.23602526]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159680   -0.224256\n",
       "347342    0.376680\n",
       "307069    0.238902\n",
       "126659    0.471005\n",
       "356896    0.128247\n",
       "            ...   \n",
       "326002    0.194530\n",
       "190999   -0.142424\n",
       "27703     0.149952\n",
       "76624     0.143431\n",
       "189606    0.229081\n",
       "Name: alpha, Length: 90626, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004759879721172527"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mprime_a_bin_0</th>\n",
       "      <th>mprime_a_bin_1</th>\n",
       "      <th>mprime_a_bin_2</th>\n",
       "      <th>mprime_a_bin_3</th>\n",
       "      <th>mprime_a_bin_4</th>\n",
       "      <th>mprime_g_bin_0</th>\n",
       "      <th>mprime_g_bin_1</th>\n",
       "      <th>mprime_g_bin_2</th>\n",
       "      <th>mprime_g_bin_3</th>\n",
       "      <th>mprime_g_bin_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159680</th>\n",
       "      <td>0.968688</td>\n",
       "      <td>0.690381</td>\n",
       "      <td>-1.610247</td>\n",
       "      <td>-0.842480</td>\n",
       "      <td>-0.765509</td>\n",
       "      <td>0.221345</td>\n",
       "      <td>-0.349359</td>\n",
       "      <td>-1.019351</td>\n",
       "      <td>2.036857</td>\n",
       "      <td>1.369018</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347342</th>\n",
       "      <td>-1.203300</td>\n",
       "      <td>0.908147</td>\n",
       "      <td>0.427997</td>\n",
       "      <td>-0.205544</td>\n",
       "      <td>0.900421</td>\n",
       "      <td>-0.673723</td>\n",
       "      <td>-0.603632</td>\n",
       "      <td>0.725515</td>\n",
       "      <td>-0.216203</td>\n",
       "      <td>-0.280011</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307069</th>\n",
       "      <td>-0.052629</td>\n",
       "      <td>-0.103307</td>\n",
       "      <td>1.356170</td>\n",
       "      <td>-2.028452</td>\n",
       "      <td>-0.101012</td>\n",
       "      <td>-1.229507</td>\n",
       "      <td>-0.940427</td>\n",
       "      <td>1.285479</td>\n",
       "      <td>-1.116649</td>\n",
       "      <td>0.798446</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126659</th>\n",
       "      <td>0.122052</td>\n",
       "      <td>-1.145261</td>\n",
       "      <td>-1.486535</td>\n",
       "      <td>-0.352838</td>\n",
       "      <td>-0.226280</td>\n",
       "      <td>0.457653</td>\n",
       "      <td>-0.026960</td>\n",
       "      <td>0.770059</td>\n",
       "      <td>2.211000</td>\n",
       "      <td>-0.862565</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356896</th>\n",
       "      <td>-0.314747</td>\n",
       "      <td>0.806773</td>\n",
       "      <td>-0.039354</td>\n",
       "      <td>-1.604842</td>\n",
       "      <td>0.097997</td>\n",
       "      <td>-0.740746</td>\n",
       "      <td>-0.841591</td>\n",
       "      <td>-1.634395</td>\n",
       "      <td>-0.617101</td>\n",
       "      <td>1.323553</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326002</th>\n",
       "      <td>0.570257</td>\n",
       "      <td>0.210242</td>\n",
       "      <td>0.225099</td>\n",
       "      <td>-0.963130</td>\n",
       "      <td>-0.524454</td>\n",
       "      <td>-0.741572</td>\n",
       "      <td>-0.750298</td>\n",
       "      <td>-1.102005</td>\n",
       "      <td>0.164011</td>\n",
       "      <td>0.682795</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190999</th>\n",
       "      <td>-0.122803</td>\n",
       "      <td>-1.748566</td>\n",
       "      <td>0.261369</td>\n",
       "      <td>1.172320</td>\n",
       "      <td>-0.049088</td>\n",
       "      <td>0.707174</td>\n",
       "      <td>0.919019</td>\n",
       "      <td>1.778795</td>\n",
       "      <td>0.595663</td>\n",
       "      <td>1.627898</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27703</th>\n",
       "      <td>0.718968</td>\n",
       "      <td>0.367986</td>\n",
       "      <td>0.122163</td>\n",
       "      <td>0.103691</td>\n",
       "      <td>-0.616840</td>\n",
       "      <td>-0.358504</td>\n",
       "      <td>-0.347800</td>\n",
       "      <td>-0.751896</td>\n",
       "      <td>-1.091658</td>\n",
       "      <td>0.908998</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76624</th>\n",
       "      <td>0.015910</td>\n",
       "      <td>0.770747</td>\n",
       "      <td>1.477093</td>\n",
       "      <td>-1.292452</td>\n",
       "      <td>-0.150825</td>\n",
       "      <td>-1.263014</td>\n",
       "      <td>-0.941725</td>\n",
       "      <td>-0.509560</td>\n",
       "      <td>-0.099483</td>\n",
       "      <td>0.205600</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189606</th>\n",
       "      <td>0.435172</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>1.487815</td>\n",
       "      <td>1.916318</td>\n",
       "      <td>-0.437842</td>\n",
       "      <td>2.312245</td>\n",
       "      <td>3.150395</td>\n",
       "      <td>-1.475347</td>\n",
       "      <td>-0.882587</td>\n",
       "      <td>-1.054974</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "159680  0.968688  0.690381 -1.610247 -0.842480 -0.765509  0.221345 -0.349359   \n",
       "347342 -1.203300  0.908147  0.427997 -0.205544  0.900421 -0.673723 -0.603632   \n",
       "307069 -0.052629 -0.103307  1.356170 -2.028452 -0.101012 -1.229507 -0.940427   \n",
       "126659  0.122052 -1.145261 -1.486535 -0.352838 -0.226280  0.457653 -0.026960   \n",
       "356896 -0.314747  0.806773 -0.039354 -1.604842  0.097997 -0.740746 -0.841591   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "326002  0.570257  0.210242  0.225099 -0.963130 -0.524454 -0.741572 -0.750298   \n",
       "190999 -0.122803 -1.748566  0.261369  1.172320 -0.049088  0.707174  0.919019   \n",
       "27703   0.718968  0.367986  0.122163  0.103691 -0.616840 -0.358504 -0.347800   \n",
       "76624   0.015910  0.770747  1.477093 -1.292452 -0.150825 -1.263014 -0.941725   \n",
       "189606  0.435172  0.002304  1.487815  1.916318 -0.437842  2.312245  3.150395   \n",
       "\n",
       "             SZA         Z  R_scence  ...  mprime_a_bin_0  mprime_a_bin_1  \\\n",
       "159680 -1.019351  2.036857  1.369018  ...             1.0             0.0   \n",
       "347342  0.725515 -0.216203 -0.280011  ...             1.0             0.0   \n",
       "307069  1.285479 -1.116649  0.798446  ...             1.0             0.0   \n",
       "126659  0.770059  2.211000 -0.862565  ...             1.0             0.0   \n",
       "356896 -1.634395 -0.617101  1.323553  ...             1.0             0.0   \n",
       "...          ...       ...       ...  ...             ...             ...   \n",
       "326002 -1.102005  0.164011  0.682795  ...             1.0             0.0   \n",
       "190999  1.778795  0.595663  1.627898  ...             1.0             0.0   \n",
       "27703  -0.751896 -1.091658  0.908998  ...             1.0             0.0   \n",
       "76624  -0.509560 -0.099483  0.205600  ...             1.0             0.0   \n",
       "189606 -1.475347 -0.882587 -1.054974  ...             1.0             0.0   \n",
       "\n",
       "        mprime_a_bin_2  mprime_a_bin_3  mprime_a_bin_4  mprime_g_bin_0  \\\n",
       "159680             0.0             0.0             0.0             1.0   \n",
       "347342             0.0             0.0             0.0             1.0   \n",
       "307069             0.0             0.0             0.0             1.0   \n",
       "126659             0.0             0.0             0.0             1.0   \n",
       "356896             0.0             0.0             0.0             1.0   \n",
       "...                ...             ...             ...             ...   \n",
       "326002             0.0             0.0             0.0             1.0   \n",
       "190999             0.0             0.0             0.0             0.0   \n",
       "27703              0.0             0.0             0.0             1.0   \n",
       "76624              0.0             0.0             0.0             1.0   \n",
       "189606             0.0             0.0             0.0             1.0   \n",
       "\n",
       "        mprime_g_bin_1  mprime_g_bin_2  mprime_g_bin_3  mprime_g_bin_4  \n",
       "159680             0.0             0.0             0.0             0.0  \n",
       "347342             0.0             0.0             0.0             0.0  \n",
       "307069             0.0             0.0             0.0             0.0  \n",
       "126659             0.0             0.0             0.0             0.0  \n",
       "356896             0.0             0.0             0.0             0.0  \n",
       "...                ...             ...             ...             ...  \n",
       "326002             0.0             0.0             0.0             0.0  \n",
       "190999             0.0             1.0             0.0             0.0  \n",
       "27703              0.0             0.0             0.0             0.0  \n",
       "76624              0.0             0.0             0.0             0.0  \n",
       "189606             0.0             0.0             0.0             0.0  \n",
       "\n",
       "[90626 rows x 28 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale_inv = X_test.drop(columns=['mprime_a_bin_0', 'mprime_a_bin_1',\n",
    "       'mprime_a_bin_2', 'mprime_a_bin_3', 'mprime_a_bin_4', 'mprime_g_bin_0',\n",
    "       'mprime_g_bin_1', 'mprime_g_bin_2', 'mprime_g_bin_3', 'mprime_g_bin_4'\n",
    "       ]).columns\n",
    "cols_to_encode_inv = ['mprime_a_bin_0', 'mprime_a_bin_1',\n",
    "       'mprime_a_bin_2', 'mprime_a_bin_3', 'mprime_a_bin_4', 'mprime_g_bin_0',\n",
    "       'mprime_g_bin_1', 'mprime_g_bin_2', 'mprime_g_bin_3', 'mprime_g_bin_4',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=joblib.load('scaler.pkl')\n",
    "encoder=joblib.load('encoder.pkl')\n",
    "X_scaled_part = X_test[cols_to_scale_inv].values\n",
    "X_encoded_part = X_test[cols_to_encode_inv].values\n",
    "# 2. Inverser la standardisation\n",
    "X_original_scaled = scaler.inverse_transform(X_scaled_part)\n",
    "\n",
    "# 3. Inverser l'encodage\n",
    "X_original_categoricals = encoder.inverse_transform(X_encoded_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reconstructed = pd.DataFrame(\n",
    "    X_original_scaled,\n",
    "    columns=['Tg_scat', 'Tg_abs', 'Ta_abs', 'SSA', 'GOD', 'AOD', 'AODS', 'SZA', 'Z',\n",
    "       'R_scence', 'g1', 'Cos(SZA)', 'mu_g', 'mu_a', 'muprime_g', 'muprime_a',\n",
    "       'mprime_g', 'mprime_a'],\n",
    "    index=X_test.index\n",
    ")\n",
    "df_categoricals = pd.DataFrame(\n",
    "    X_original_categoricals,\n",
    "    columns=['mprime_a_bin', 'mprime_g_bin'],\n",
    "    index=X_test.index\n",
    ")\n",
    "X_test_real = pd.concat([df_reconstructed, df_categoricals], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real['alpha_pred']=y_pred\n",
    "X_test_real['alpha']=y_test\n",
    "X_test_real['BOA_RT']=BOA_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>mu_a</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>mprime_a_bin</th>\n",
       "      <th>mprime_g_bin</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159680</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>3187.429121</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.219688</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>914.444755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347342</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.056472</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376612</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>594.575497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307069</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.507886</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.229414</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>284.591943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126659</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>3187.535216</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468541</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>373.718733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356896</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.812230</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.126366</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>832.137702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326002</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.288112</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194265</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>801.588284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190999</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.551091</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.150260</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>32.089238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27703</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.523112</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.152813</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>735.627029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>3186.127582</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140123</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>924.379368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189606</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>3185.650486</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236025</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>730.440816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "159680  0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "347342  0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "307069  0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "126659  0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "356896  0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "326002  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "190999  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "27703   0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "76624   0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "189606  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "              SZA            Z  R_scence  ...         mu_a  muprime_g  \\\n",
       "159680  17.879232  3858.242750  0.894296  ...  3187.429121   0.951776   \n",
       "347342  61.931220  1112.943981  0.419185  ...  3186.056472   0.471695   \n",
       "307069  76.068422    15.772549  0.729906  ...  3185.507886   0.243492   \n",
       "126659  63.055806  4070.432067  0.251342  ...  3187.535216   0.454355   \n",
       "356896   2.351437   624.459254  0.881197  ...  3185.812230   0.999159   \n",
       "...           ...          ...       ...  ...          ...        ...   \n",
       "326002  15.792506  1576.224940  0.696585  ...  3186.288112   0.962308   \n",
       "190999  88.522985  2102.182329  0.968884  ...  3186.551091   0.042399   \n",
       "27703   24.631558    46.223183  0.761757  ...  3185.523112   0.909141   \n",
       "76624   30.749744  1255.164902  0.559097  ...  3186.127582   0.859623   \n",
       "189606   6.366867   300.971201  0.195906  ...  3185.650486   0.993841   \n",
       "\n",
       "        muprime_a   mprime_g   mprime_a  mprime_a_bin  mprime_g_bin  \\\n",
       "159680   0.951721   0.684362   0.152645             0             0   \n",
       "347342   0.470791   1.873413   1.217586             0             0   \n",
       "307069   0.241375   4.099723   4.110381             0             0   \n",
       "126659   0.453397   1.400187   0.288163             0             0   \n",
       "356896   0.999158   0.933753   0.732430             0             0   \n",
       "...           ...        ...        ...           ...           ...   \n",
       "326002   0.962266   0.872219   0.472533             0             0   \n",
       "190999   0.030856  18.672395  11.328534             0             2   \n",
       "27703    0.909037   1.094304   1.074933             0             0   \n",
       "76624    0.859456   1.011868   0.621184             0             0   \n",
       "189606   0.993834   0.973105   0.865627             0             0   \n",
       "\n",
       "        alpha_pred     alpha      BOA_RT  \n",
       "159680   -0.219688 -0.224256  914.444755  \n",
       "347342    0.376612  0.376680  594.575497  \n",
       "307069    0.229414  0.238902  284.591943  \n",
       "126659    0.468541  0.471005  373.718733  \n",
       "356896    0.126366  0.128247  832.137702  \n",
       "...            ...       ...         ...  \n",
       "326002    0.194265  0.194530  801.588284  \n",
       "190999   -0.150260 -0.142424   32.089238  \n",
       "27703     0.152813  0.149952  735.627029  \n",
       "76624     0.140123  0.143431  924.379368  \n",
       "189606    0.236025  0.229081  730.440816  \n",
       "\n",
       "[90626 rows x 23 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_BOA_IA(Tg_abs,Mg,Ta_abs,Ma,alpha,delta_g_scat,AODS):\n",
    "    numerator= 1000*(Tg_abs**Mg)*(Ta_abs**Ma)\n",
    "    denominator=1+alpha*delta_g_scat*Mg+(alpha*(1/3)*AODS)*Ma\n",
    "    if denominator != 0 :\n",
    "        BOA_ia= numerator / denominator\n",
    "        return BOA_ia\n",
    "    else :\n",
    "        return 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real['BOA_AI'] = X_test_real.apply(lambda row: calculate_BOA_IA(row['Tg_abs'], row['mprime_g'],row['Ta_abs'],row['mprime_a'],row['alpha_pred'],row['GOD'],row['AODS']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg_scat</th>\n",
       "      <th>Tg_abs</th>\n",
       "      <th>Ta_abs</th>\n",
       "      <th>SSA</th>\n",
       "      <th>GOD</th>\n",
       "      <th>AOD</th>\n",
       "      <th>AODS</th>\n",
       "      <th>SZA</th>\n",
       "      <th>Z</th>\n",
       "      <th>R_scence</th>\n",
       "      <th>...</th>\n",
       "      <th>muprime_g</th>\n",
       "      <th>muprime_a</th>\n",
       "      <th>mprime_g</th>\n",
       "      <th>mprime_a</th>\n",
       "      <th>mprime_a_bin</th>\n",
       "      <th>mprime_g_bin</th>\n",
       "      <th>alpha_pred</th>\n",
       "      <th>alpha</th>\n",
       "      <th>BOA_RT</th>\n",
       "      <th>BOA_AI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159680</th>\n",
       "      <td>0.990261</td>\n",
       "      <td>0.940649</td>\n",
       "      <td>0.715428</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.513978</td>\n",
       "      <td>0.179103</td>\n",
       "      <td>17.879232</td>\n",
       "      <td>3858.242750</td>\n",
       "      <td>0.894296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>0.951721</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.219688</td>\n",
       "      <td>-0.224256</td>\n",
       "      <td>914.444755</td>\n",
       "      <td>914.378481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347342</th>\n",
       "      <td>0.550236</td>\n",
       "      <td>0.983172</td>\n",
       "      <td>0.902145</td>\n",
       "      <td>0.498744</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>61.931220</td>\n",
       "      <td>1112.943981</td>\n",
       "      <td>0.419185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.470791</td>\n",
       "      <td>1.873413</td>\n",
       "      <td>1.217586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376612</td>\n",
       "      <td>0.376680</td>\n",
       "      <td>594.575497</td>\n",
       "      <td>594.608170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307069</th>\n",
       "      <td>0.783352</td>\n",
       "      <td>0.785665</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.244174</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>76.068422</td>\n",
       "      <td>15.772549</td>\n",
       "      <td>0.729906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243492</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>4.099723</td>\n",
       "      <td>4.110381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.229414</td>\n",
       "      <td>0.238902</td>\n",
       "      <td>284.591943</td>\n",
       "      <td>286.792481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126659</th>\n",
       "      <td>0.818740</td>\n",
       "      <td>0.582204</td>\n",
       "      <td>0.726761</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.276276</td>\n",
       "      <td>63.055806</td>\n",
       "      <td>4070.432067</td>\n",
       "      <td>0.251342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454355</td>\n",
       "      <td>0.453397</td>\n",
       "      <td>1.400187</td>\n",
       "      <td>0.288163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468541</td>\n",
       "      <td>0.471005</td>\n",
       "      <td>373.718733</td>\n",
       "      <td>373.965582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356896</th>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.168593</td>\n",
       "      <td>0.314370</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>2.351437</td>\n",
       "      <td>624.459254</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.933753</td>\n",
       "      <td>0.732430</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.126366</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>832.137702</td>\n",
       "      <td>832.591657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326002</th>\n",
       "      <td>0.909543</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.319999</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.182056</td>\n",
       "      <td>0.058258</td>\n",
       "      <td>15.792506</td>\n",
       "      <td>1576.224940</td>\n",
       "      <td>0.696585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194265</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>801.588284</td>\n",
       "      <td>801.607407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190999</th>\n",
       "      <td>0.769135</td>\n",
       "      <td>0.464396</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.262489</td>\n",
       "      <td>0.681445</td>\n",
       "      <td>0.561401</td>\n",
       "      <td>88.522985</td>\n",
       "      <td>2102.182329</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.030856</td>\n",
       "      <td>18.672395</td>\n",
       "      <td>11.328534</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.150260</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>32.089238</td>\n",
       "      <td>-0.002811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27703</th>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.877695</td>\n",
       "      <td>0.874129</td>\n",
       "      <td>0.571705</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.314101</td>\n",
       "      <td>0.179573</td>\n",
       "      <td>24.631558</td>\n",
       "      <td>46.223183</td>\n",
       "      <td>0.761757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909141</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>1.094304</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.152813</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>735.627029</td>\n",
       "      <td>735.353831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76624</th>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.956342</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.226603</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>30.749744</td>\n",
       "      <td>1255.164902</td>\n",
       "      <td>0.559097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.859456</td>\n",
       "      <td>1.011868</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140123</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>924.379368</td>\n",
       "      <td>925.058904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189606</th>\n",
       "      <td>0.882176</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.234720</td>\n",
       "      <td>1.233952</td>\n",
       "      <td>6.366867</td>\n",
       "      <td>300.971201</td>\n",
       "      <td>0.195906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.973105</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236025</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>730.440816</td>\n",
       "      <td>728.261901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90626 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tg_scat    Tg_abs    Ta_abs       SSA       GOD       AOD      AODS  \\\n",
       "159680  0.990261  0.940649  0.715428  0.348465  0.009787  0.513978  0.179103   \n",
       "347342  0.550236  0.983172  0.902145  0.498744  0.597408  0.205444  0.102464   \n",
       "307069  0.783352  0.785665  0.987172  0.068646  0.244174  0.013862  0.000952   \n",
       "126659  0.818740  0.582204  0.726761  0.463992  0.199988  0.595434  0.276276   \n",
       "356896  0.730249  0.963376  0.859333  0.168593  0.314370  0.182341  0.030741   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "326002  0.909543  0.846892  0.883558  0.319999  0.094813  0.182056  0.058258   \n",
       "190999  0.769135  0.464396  0.886881  0.823838  0.262489  0.681445  0.561401   \n",
       "27703   0.939670  0.877695  0.874129  0.571705  0.062226  0.314101  0.179573   \n",
       "76624   0.797237  0.956342  0.998250  0.242298  0.226603  0.002312  0.000560   \n",
       "189606  0.882176  0.806288  0.999232  0.999378  0.125364  1.234720  1.233952   \n",
       "\n",
       "              SZA            Z  R_scence  ...  muprime_g  muprime_a  \\\n",
       "159680  17.879232  3858.242750  0.894296  ...   0.951776   0.951721   \n",
       "347342  61.931220  1112.943981  0.419185  ...   0.471695   0.470791   \n",
       "307069  76.068422    15.772549  0.729906  ...   0.243492   0.241375   \n",
       "126659  63.055806  4070.432067  0.251342  ...   0.454355   0.453397   \n",
       "356896   2.351437   624.459254  0.881197  ...   0.999159   0.999158   \n",
       "...           ...          ...       ...  ...        ...        ...   \n",
       "326002  15.792506  1576.224940  0.696585  ...   0.962308   0.962266   \n",
       "190999  88.522985  2102.182329  0.968884  ...   0.042399   0.030856   \n",
       "27703   24.631558    46.223183  0.761757  ...   0.909141   0.909037   \n",
       "76624   30.749744  1255.164902  0.559097  ...   0.859623   0.859456   \n",
       "189606   6.366867   300.971201  0.195906  ...   0.993841   0.993834   \n",
       "\n",
       "         mprime_g   mprime_a  mprime_a_bin  mprime_g_bin  alpha_pred  \\\n",
       "159680   0.684362   0.152645             0             0   -0.219688   \n",
       "347342   1.873413   1.217586             0             0    0.376612   \n",
       "307069   4.099723   4.110381             0             0    0.229414   \n",
       "126659   1.400187   0.288163             0             0    0.468541   \n",
       "356896   0.933753   0.732430             0             0    0.126366   \n",
       "...           ...        ...           ...           ...         ...   \n",
       "326002   0.872219   0.472533             0             0    0.194265   \n",
       "190999  18.672395  11.328534             0             2   -0.150260   \n",
       "27703    1.094304   1.074933             0             0    0.152813   \n",
       "76624    1.011868   0.621184             0             0    0.140123   \n",
       "189606   0.973105   0.865627             0             0    0.236025   \n",
       "\n",
       "           alpha      BOA_RT      BOA_AI  \n",
       "159680 -0.224256  914.444755  914.378481  \n",
       "347342  0.376680  594.575497  594.608170  \n",
       "307069  0.238902  284.591943  286.792481  \n",
       "126659  0.471005  373.718733  373.965582  \n",
       "356896  0.128247  832.137702  832.591657  \n",
       "...          ...         ...         ...  \n",
       "326002  0.194530  801.588284  801.607407  \n",
       "190999 -0.142424   32.089238   -0.002811  \n",
       "27703   0.149952  735.627029  735.353831  \n",
       "76624   0.143431  924.379368  925.058904  \n",
       "189606  0.229081  730.440816  728.261901  \n",
       "\n",
       "[90626 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_BOA_TOTAL(SZA,Z,alpha,l,E_TOA):   \n",
    "    def muprime(z,h,µ):\n",
    "        RAYON_TERRESTRE=6371\n",
    "        eta = (RAYON_TERRESTRE*1000 + z) / h\n",
    "        root = (eta*µ)**2  + 2 * eta + 1\n",
    "        sum = (root)**0.5 - eta * µ\n",
    "        if sum > 0 :\n",
    "            return 1/sum\n",
    "        return 1 \n",
    "    BOA=[]\n",
    "    for i in range(len(l)) :\n",
    "        tg_scat, tg_abs, optical_depth, albedo, a, g1 = l[i]\n",
    "        Ha=2000\n",
    "        Hg=9000\n",
    "        angle_rad = math.radians(SZA)\n",
    "        µ=math.cos(angle_rad)  \n",
    "        Y_a=muprime(Z,Ha,µ)\n",
    "        Y_g=muprime(Z,Hg,µ)\n",
    "        Ma=math.exp(-Z/Ha)/Y_a\n",
    "        Mg=math.exp(-Z/Hg)/Y_g\n",
    "        delta_a_scat=optical_depth*albedo\n",
    "        delta_g_scat=-math.log(tg_scat)\n",
    "        Ta_abs=math.exp(-optical_depth*(1-albedo))\n",
    "        numerator= E_TOA[i]*(tg_abs**Mg)*(Ta_abs**Ma)\n",
    "        denominator=1+alpha[i]*delta_g_scat*Mg+(alpha[i]*(1/3)*delta_a_scat)*Ma\n",
    "        BOA_i= numerator / denominator\n",
    "        BOA.append(BOA_i)\n",
    "    return BOA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myptd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
